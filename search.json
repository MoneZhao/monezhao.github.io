[{"title":"账户余额Docker部署命令","url":"%2F2022%2F07%2F28%2Fbalance-docker-cmd%2F","content":"\n## 环境版本\n\n***开发环境和运行环境使用同一版本***\n\n- Java 8u232\n- MySQL 8.0.18\n- Redis 5.0.9\n- emqx 4.4.5\n\n<!-- more -->\n\n## 命令\n\n### 导入镜像\n\n#### 从docker服务器拉取\n\n```sh\ndocker pull openjdk:8u232-jre\ndocker pull mysql:8.0.18\ndocker pull redis:5.0.9\ndocker pull emqx/emqx:4.4.5\n```\n\n#### 从本地文件导入\n```sh\n# docker load < emqx.tar\ndocker load -i ${image_tar_name}\nor\ndocker load < ${image_tar_name}\n```\n\n### 导出镜像\n\n```sh\n# docker save mysql:8.0.18 > mysql.tar\ndocker save ${image_name} > ${image_tar_name}\nor\ndocker save ${image_name} -o ${image_tar_name}\n```\n\n### Docker运行\n\n#### 运行单个镜像\n\n```sh\n# mysql\ndocker run --restart=always -p 3306:3306 --name mysql \\\n-e MYSQL_ROOT_PASSWORD=root -d mysql:8.0.18 \\\n--default-authentication-plugin=mysql_native_password --lower_case_table_names=1\n\n# redis\ndocker run --restart=always -d --name redis -p 6379:6379 redis:5.0.9\n\n# emqx\ndocker run --restart=always -d --name emqx -p 1883:1883 -p 8083:8083 -p 18083:18083 emqx/emqx:4.4.5\n\n# balance\ndocker run --restart=always -d --name balance \\\n--link=mysql:mysql --link=redis:redis -p 8686:8686 monezhao/balance:latest\n\n# SQL Server\n# docker run --restart=always -e \"ACCEPT_EULA=Y\" -e \"SA_PASSWORD=abcd,1234\" \\\n# -p 1433:1433 --name mssql -d mcr.microsoft.com/mssql/server:2017-latest\n```\n\n#### `docker-compose`运行\n\n`docker-compose.yml`\n\n```yml\nversion: '3.1'\nservices:\n  mysql:\n    image: mysql:8.0.18\n    restart: always\n    environment:\n        MYSQL_ROOT_PASSWORD: 'root'\n    command:\n        --default-authentication-plugin=mysql_native_password\n        --lower_case_table_names=1\n    ports:\n        - 3306:3306\n  redis:\n    image: redis:5.0.9\n    restart: always\n    ports:\n        - 6379:6379\n  emqx:\n    image: emqx/emqx:4.4.5\n    restart: always\n    ports:\n        - 1883:1883\n        - 8083:8083\n        - 18083:18083\n#   mssql:\n#     image: mcr.microsoft.com/mssql/server:2017-latest\n#     restart: always\n#     environment:\n#         ACCEPT_EULA: 'Y'\n#         SA_PASSWORD: 'abcd,1234'\n#     ports:\n#         - 1433:1433\n```\n\n`docker-compose`命令\n\n```sh\n# 创建并启动服务\ndocker-compose up -d\n# 停止服务\nocker-compose stop\n# 启动服务\ndocker-compose start\n# 停止并删除服务\ndocker-compose down\n```\n","tags":["docker"]},{"title":"账户余额项目总结","url":"%2F2021%2F11%2F25%2Fspring-mvc-mybatis-vue%2F","content":"\n## 使用`p6spy`展示sql\n\n### `pom.xml`中引入`p6spy`\n\n```xml\n<dependency>\n  <groupId>p6spy</groupId>\n  <artifactId>p6spy</artifactId>\n  <version>${p6spy.version}</version>\n</dependency>\n```\n\n### `application.yml`中数据源设置`p6spy`\n\n```yml\nspring:\n  datasource:\n    dynamic:\n      p6spy: true\n```\n\n<!-- more -->\n\n### spy.properties\n\n```properties\nmodule.log=com.p6spy.engine.logging.P6LogFactory\n# 单行日志\nlogMessageFormat=com.monezhao.config.CustomSqlFormat\n#logMessageFormat=com.baomidou.mybatisplus.extension.p6spy.P6SpyLogger\n# 使用Slf4J记录sql\nappender=com.p6spy.engine.spy.appender.Slf4JLogger\n# 是否开启慢SQL记录\noutagedetection=true\n# 慢SQL记录标准，单位秒\noutagedetectioninterval=2\n# 日期格式\ndateformat=yyyy-MM-dd HH:mm:ss\n# 过滤Quartz的SQL\nfilter=true\nexclude=QRTZ_FIRED_TRIGGERS,QRTZ_PAUSED_TRIGGER_GRPS,QRTZ_SCHEDULER_STATE,QRTZ_LOCKS,QRTZ_SIMPLE_TRIGGERS,QRTZ_SIMPROP_TRIGGERS,QRTZ_CRON_TRIGGERS,QRTZ_BLOB_TRIGGERS,QRTZ_TRIGGERS,QRTZ_JOB_DETAILS,QRTZ_CALENDARS\nexcludecategories=info,debug,result,commit,resultset\n```\n\n### 自定义输出sql格式\n\n```java\n@Slf4j\npublic class CustomSqlFormat implements MessageFormattingStrategy {\n    @Override\n    public String formatMessage(int connectionId, String now, long elapsed, String category,\n                                String prepared, String sql, String url) {\n        if (CommonUtil.isEmptyStr(sql)) {\n            return \"\";\n        } else {\n            if (elapsed > 200) {\n                log.info(\"用时超过200ms!!!\");\n            }\n            return \"\\n# \" + now + \" | 执行sql用时 \" + elapsed + \"ms\" + \"\\n==>    SQl开始:\\n\" + sql + \"\\n<==    SQL结束\";\n        }\n    }\n}\n```\n\n## 数据乐观锁\n\n### 数据库新增`version`字段\n\n```sql\nALTER TABLE ${tableName} ADD COLUMN `version` int(6) NOT NULL DEFAULT 1 COMMENT '乐观锁'\n```\n### 表对应`table entity`新增字段\n\n```java\n@Version\n@ExcelIgnore\n@ApiModelProperty(hidden = true)\nprivate int version;\n```\n\n### `MybatisPlusConfig`中设置乐观锁\n\n```java\n@Bean\npublic OptimisticLockerInterceptor optimisticLockerInterceptor() {\n    return new OptimisticLockerInterceptor();\n}\n```\n\n> `MybatisPlus`会自动处理乐观锁的冲突, 因此按照之前的方式save, update即可\n\n## Nginx前后端分离部署\n\n### 后端开启CROS\n\n`WebMvcConfigurer`中配置：\n\n```java\n@Configuration\npublic class WebMvcConfig implements WebMvcConfigurer {\n    @Bean\n    public CorsFilter corsFilter() {\n      final UrlBasedCorsConfigurationSource urlBasedCorsConfigurationSource = new UrlBasedCorsConfigurationSource();\n      final CorsConfiguration corsConfiguration = new CorsConfiguration();\n      /* 是否允许请求带有验证信息 */\n      corsConfiguration.setAllowCredentials(true);\n      /* 允许访问的客户端域名 */\n      corsConfiguration.addAllowedOrigin(\"*\");\n      /* 允许服务端访问的客户端请求头 */\n      corsConfiguration.addAllowedHeader(\"*\");\n      /* 允许访问的方法名,GET POST等 */\n      corsConfiguration.addAllowedMethod(\"*\");\n      urlBasedCorsConfigurationSource.registerCorsConfiguration(\"/**\", corsConfiguration);\n      return new CorsFilter(urlBasedCorsConfigurationSource);\n    }\n}\n```\n\n### 前端vue配置\n\n`vue.config.js`项目配置中`publicPath`设置为后端的`context-path`, 例如`/bar/`\n\n`.env`文件里面`VUE_APP_BASE_API`后缀加上`/api`, 方便`Nginx`区分前端资源还是后台API\n\n### Nginx配置\n\nnginx.conf\n\n```conf\nuser  root;\nworker_processes  1;\nevents {\n    #设置Nginx网络连接序列化\n    accept_mutex on;\n    #设置Nginx的worker进程是否可以同时接收多个请求\n    multi_accept on;\n    #设置Nginx的worker进程最大的连接数\n    worker_connections 1024;\n}\n\nhttp {\n    include       mime.types;\n    default_type  application/octet-stream;\n    sendfile        on;\n    keepalive_timeout  65;\n    gzip on; #开启压缩\n    gzip_vary on; #设置为on会在Header里增加 \"Vary: Accept-Encoding\"\n    gzip_proxied any; #代理结果数据的压缩\n    gzip_comp_level 5; #gzip压缩比（1~9），越小压缩效果越差，但是越大处理越慢，所以一般取中间值\n    gzip_buffers 4 128k; #获取多少内存用于缓存压缩结果,拿出 4 个 128K 用来缓存压缩的文件\n    gzip_http_version 1.1; #识别http协议的版本\n    gzip_min_length 1k; #设置允许压缩的页面最小字节数，超过1k的文件会被压缩\n    include /usr/local/etc/nginx/servers/*.conf;\n}\n```\n\nserver.conf\n\n```conf\n# 多台服务器\nupstream backend {\n    server localhost:8686 weight=2;\n    server localhost:18686 weight=1;\n    server localhost:28686 backup;\n}\nserver {\n    listen 8080;\n    server_name localhost;\n    add_header Cache-Control no-cache;\n    location /balance {\n        alias /usr/local/web/dist;\n        try_files $uri $uri/  /balance/index.html;\n        index index.html index.htm;\n        expires 7d;\n     }\n    location /balance/api {\n        proxy_pass http://backend/balance/;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header REMOTE-HOST $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    }\n}\n```\n\n## spring MVC接口测试\n\n```java\n@RunWith(SpringRunner.class)\n@SpringBootTest\n@Slf4j\n@Transactional //测试结束后使用事务回滚,不影响现有数据库\n@Ignore //maven打包时需要忽略与数据库操作相关的测试\npublic class XXControllerTest {\n}\n```\n","tags":["Vue"]},{"title":"账户余额","url":"%2F2021%2F04%2F24%2Faccount-balance%2F","content":"\n## 目标\n\n做一个简单的账户余额管理系统, 前端使用 `Vue` + `ElementUI` + `Echarts`, 后端一边学习一边使用 `Spring Boot 2` 开发, 并记录开发的各种坑。\n\n主要有以下的功能：\n\n1. 注册\n2. 登录\n3. 账户余额记录的CRUD\n4. 搜索\n5. Swagger自动生成API文档\n6. 账户余额对比\n7. 账户余额详情对比\n8. 账户余额复制\n9. 账户余额报表\n10. 账户余额详情报表\n11. 账户余额导入导出\n\n<!-- more -->\n\n## 前端进度\n\n- [x] 注册\n- [x] 登录\n- [x] 账户余额记录的CRUD\n- [x] 搜索\n- [x] 账户余额对比\n- [x] 账户余额详情对比\n- [x] 账户余额复制\n- [x] 账户余额报表\n- [x] 账户余额详情报表\n- [x] 账户余额导入导出\n\n## 后端进度\n\n- [x] 注册\n- [x] 登录\n- [x] 账户余额记录的CRUD\n- [x] 搜索\n- [x] Swagger整合进项目(使用Knife4j实现更美观)\n- [x] 账户余额对比\n- [x] 账户余额详情对比\n- [x] 账户余额复制\n- [x] 账户余额报表\n- [x] 账户余额详情报表\n- [x] 账户余额导入导出\n","tags":["Vue"]},{"title":"ArrayList 遍历删除","url":"%2F2019%2F08%2F02%2FArrayList-loop-remove%2F","content":"\nlist的遍历分为3种:\n\n1. 普通遍历, `for(int i=0;i<list.size();i++)`\n2. 增强for循环, `for(Object x:list)`\n3. iterator遍历, `Iterator<String> it = list.iterator();\nwhile(it.hasNext()){}`\n\n同时ArrayList和线程安全的CopyOnWriteArrayList不同遍历下变现也不同，下面分类展示各种情况。\n\n<!-- more -->\n\n## ArrayList\n\n### 普通for循环遍历\n\n```java\n// list {0, 1, 2, 3, 4}\nfor (int i = 0; i < list.size(); i++) {\n  // index and number\n  System.out.print(i + \" \" + list.get(i));\n  if (list.get(i) % 2 == 0) {\n    list.remove(list.get(i));\n    System.out.print(\" delete\");\n    i--; // 索引改变!\n  }\n}\n\n/*\n0 0 delete\n0 1\n1 2 delete\n1 3\n2 4 delete\n*/\n```\n\n删除某个元素后，list的大小发生了变化，而你的索引也在变化，平时编程不注意可能会在遍历时导致一些访问越界的问题，因此不是特别推荐。\n\n### 增强型for循环遍历\n\n```java\n// list {0, 1, 2, 3, 4}\nfor (Integer num : list) {\n  // number\n  System.out.print(num);\n  if (num % 2 == 0) {\n    list.remove(num);\n    System.out.print(\" delete\");\n  }\n}\n```\n\n删除第一个元素时是没有问题的，但删除后继续执行遍历过程的话就会抛出ConcurrentModificationException的异常。\n\n### 使用iterator遍历\n\n```java\n// list {0, 1, 2, 3, 4}\nIterator<Integer> it = list.iterator();\nwhile (it.hasNext()) {\n    // index and number\n    int num = it.next();\n    System.out.print(num);\n    if (num % 2 == 0) {\n      it.remove();\n      System.out.print(\" delete\");\n    }\n}\n```\n\n可以看到顺利的执行了遍历并删除的操作，因此最推荐的做法是使用iterator执行遍历删除操作。但要注意的是，**使用iterator的remove方法**，如果用list的remove方法同样会报上面提到的ConcurrentModificationException错误。\n\n### 问题产生原因\n\n#### fail-fast 机制\n\n在JDK的集合类中有这样一段描述：\n\n***注意，迭代器的快速失败行为无法得到保证，因为一般来说，不可能对是否出现不同步并发修改做出任何硬性保证。快速失败迭代器会尽最大努力抛出 ConcurrentModificationException。因此，为提高这类迭代器的正确性而编写一个依赖于此异常的程序是错误的做法：迭代器的快速失败行为应该仅用于检测 bug。***\n\n“快速失败”也就是 fail-fast，它是 Java 集合的一种错误检测机制。当多个线程对集合进行结构上的改变的操作时，有可能会产生 fail-fast 机制。记住是有可能，而不是一定。例如：假设存在两个线程（线程 1、线程 2），线程 1 通过 Iterator 在遍历集合 A 中的元素，在某个时候线程 2 修改了集合 A 的结构（是结构上面的修改，而不是简单的修改集合元素的内容），那么这个时候程序就会抛出 ConcurrentModificationException 异常，从而产生 fail-fast 机制。\n\n首先我们看看：\n\n```java\nprivate class Itr implements Iterator<E> {\n    int cursor;\n    int lastRet = -1;\n    int expectedModCount = ArrayList.this.modCount;\n    public boolean hasNext() {\n        return (this.cursor != ArrayList.this.size);\n    }\n    public E next() {\n        checkForComodification();\n        /** 省略此处代码 */\n    }\n    public void remove() {\n        if (this.lastRet < 0)\n            throw new IllegalStateException();\n        checkForComodification();\n        /** 省略此处代码 */\n    }\n    final void checkForComodification() {\n        if (ArrayList.this.modCount == this.expectedModCount)\n            return;\n        throw new ConcurrentModificationException();\n    }\n}\n```\n\n迭代器在调用 next()、remove() 方法时都是调用 checkForComodification() 方法，该方法主要就是检测 modCount == expectedModCount ? 若不等则抛出 ConcurrentModificationException 异常，从而产生 fail-fast 机制。\n\nexpectedModCount 是在 Itr 中定义的：int expectedModCount = ArrayList.this.modCount;所以他的值是不可能会修改的，所以会变的就是 modCount。modCount 是在 AbstractList 中定义的，为全局变量：\n\n```java\nprotected transient int modCount = 0;\n```\n\nArrayList 中无论 add、remove、clear 方法只要是涉及了改变 ArrayList 元素的个数的方法都会导致 modCount 的改变。\n\n有两个线程（线程 A，线程 B），其中线程 A 负责遍历 list、线程B修改 list。线程 A 在遍历 list 过程的某个时候（此时 expectedModCount = modCount=N），线程启动，同时线程B增加一个元素，这是 modCount 的值发生改变（modCount = N + 1）。线程 A 继续遍历执行 next 方法时，通告 checkForComodification 方法发现 expectedModCount = N ，而 modCount = N + 1，两者不等，这时就抛出ConcurrentModificationException 异常，从而产生 fail-fast 机制。\n\n## CopyOnWriteArrayList\n\nCopyOnWriteArrayList 是 ArrayList 的一个线程安全的变体，其中所有可变操作（add、set 等等）都是通过对底层数组进行一次新的复制来实现的。 该类产生的开销比较大，但是在两种情况下，它非常适合使用。1：在不能或不想进行同步遍历，但又需要从并发线程中排除冲突时。2：当遍历操作的数量大大超过可变操作的数量时。遇到这两种情况使用 CopyOnWriteArrayList 来替代 ArrayList 再适合不过了。\n\n### 普通for循环\n\n和ArrayList相同，list的大小发生了变化，索引也在变化，在遍历时也有可能导致一些访问越界的问题，因此不是特别推荐。\n\n### 增强型for循环\n\nCopyOnWriterArrayList 的方法没有像 ArrayList 中使用 checkForComodification 方法来判断 expectedModCount 与 modCount 是否相等, 既CopyOnWriterArrayList 不会产生 ConcurrentModificationException 异常，它使用迭代器不会产生 fail-fast 机制。\n\n为什么呢？我们以 add 方法为例：\n\n```java\n    public boolean add(E paramE) {\n        ReentrantLock localReentrantLock = this.lock;\n        localReentrantLock.lock();\n        try {\n            Object[] arrayOfObject1 = getArray();\n            int i = arrayOfObject1.length;\n\n            //CopyOnWriterArrayList 的 add 方法与 ArrayList 的 add 方法\n            Object[] arrayOfObject2 = Arrays.copyOf(arrayOfObject1, i + 1);\n            arrayOfObject2[i] = paramE;\n            setArray(arrayOfObject2);\n            //最大的不同之处\n\n            int j = 1;\n            return j;\n        } finally {\n            localReentrantLock.unlock();\n        }\n    }\n\n    final void setArray(Object[] paramArrayOfObject) {\n        this.array = paramArrayOfObject;\n    }\n```\n\n***CopyOnWriterArrayList 所代表的核心概念就是：任何对 array 在结构上有所改变的操作（add、remove、clear 等），CopyOnWriterArrayList 都会 copy 现有的数据，再在 copy 的数据上修改，这样就不会影响 COWIterator 中的数据了，修改完成之后改变原有数据的引用即可。同时这样造成的代价就是产生大量的对象，同时数组的 copy 也是相当有损耗的。***\n\n### iterator遍历\n\n和 ArrayList 不同, CopyOnWriteArrayList不支持iterator遍历, 创建迭代器时复制一份数组拷贝(快照)，在迭代期间数组不会被改变，调用remove、set、add方法抛出UnsupportedOperationException异常。\n","tags":["Java"]},{"title":"tcp三次握手四次挥手","url":"%2F2019%2F07%2F26%2Ftcp-handshake%2F","content":"\n建立TCP需要三次握手才能建立，而断开连接则需要四次挥手。整个过程如下图所示：\n\n![图示](/images/tcp/tcp.png)\n\n<!-- more -->\n\n## 三次握手\n\n假定主机A运行的是客户端，B运行服务端。最初两端的TCP进程都处于CLOESD状态。连接建立的握手环节:\n\n1. 第一次握手：A将SYN置1，选择一个seq=x（序号，起始发送位）。SYN报文段不能携带数据，但要消耗掉一个序号。这时TCP进程进入SYN-SENT（同步已发送）状态。\n\n2. 第二次握手：B收到请求报文段后，若同意则向A发送确认。在确认报文段中把SYN和ACK都置1，确认号是ack = x+1，同时也为自己选择一个初始序号seq=y。这个报文段也不能携带数据，但同样要消耗掉一个序号。TCP服务器进程进入SYN-RCVD（同步收到）状态。\n\n3. 第三次握手：TCP客户进程收到B的确认后，还要向B给出确认。确认报文段的ACK置1（连接建立，两边ACK必须全部置1），确认号ack = y+1，而自己的序号seq = x+1。TCP的标准规定，ACK报文段可以携带数据。但如果不携带数据则不消耗序号。在这种情况下，下一个数据报文段的序号仍是seq=x+1.这时TCP连接已经建立，A进入ESABLISHED（已建立连接）状态。\n\n### 为什么需要三次握手\n\n防止失效的连接请求报文段被服务端接收，从而产生错误。\n\nPS：失效的连接请求：若客户端向服务端发送的连接请求丢失，客户端等待应答超时后就会再次发送连接请求，此时，上一个连接请求就是『失效的』。\n\n若建立连接只需两次握手，客户端并没有太大的变化，仍然需要获得服务端的应答后才进入ESTABLISHED状态，而服务端在收到连接请求后就进入ESTABLISHED状态。此时如果网络拥塞，客户端发送的连接请求迟迟到不了服务端，客户端便超时重发请求，如果服务端正确接收并确认应答，双方便开始通信，通信结束后释放连接。此时，如果那个失效的连接请求抵达了服务端，由于只有两次握手，服务端收到请求就会进入ESTABLISHED状态，等待发送数据或主动发送数据。但此时的客户端早已进入CLOSED状态，服务端将会一直等待下去，这样浪费服务端连接资源。\n\n## 四次挥手\n\nTCP连接的释放一共需要四步，因此称为『四次挥手』。\n\n我们知道，TCP连接是双向的，因此在四次挥手中，前两次挥手用于断开一个方向的连接，后两次挥手用于断开另一方向的连接。\n\n1. 第一次挥手 若A认为数据发送完成，则它需要向B发送连接释放请求。该请求只有报文头，头中携带的主要参数为： FIN=1，seq=u。此时，A将进入FIN-WAIT-1状态。PS1：FIN=1表示该报文段是一个连接释放请求。PS2：seq=u，u-1是A向B发送的最后一个字节的序号。\n\n2. 第二次挥手 B收到连接释放请求后，会通知相应的应用程序，告诉它A向B这个方向的连接已经释放。此时B进入CLOSE-WAIT状态，并向A发送连接释放的应答，其报文头包含： ACK=1，seq=v，ack=u+1。PS1：ACK=1：除TCP连接请求报文段以外，TCP通信过程中所有数据报的ACK都为1，表示应答。PS2：seq=v，v-1是B向A发送的最后一个字节的序号。PS3：ack=u+1表示希望收到从第u+1个字节开始的报文段，并且已经成功接收了前u个字节。A收到该应答，进入FIN-WAIT-2状态，等待B发送连接释放请求。第二次挥手完成后，A到B方向的连接已经释放，B不会再接收数据，A也不会再发送数据。但B到A方向的连接仍然存在，B可以继续向A发送数据。\n\n3. 第三次挥手 当B向A发完所有数据后，向A发送连接释放请求，请求头：FIN=1，ACK=1，seq=w，ack=u+1。B便进入LAST-ACK状态。\n\n4. 第四次挥手 A收到释放请求后，向B发送确认应答，此时A进入TIME-WAIT状态。该状态会持续2MSL时间，若该时间段内没有B的重发请求的话，就进入CLOSED状态，撤销TCB。当B收到确认应答后，也便进入CLOSED状态，撤销TCB。\n\n### 为什么A在TIME-WAIT状态必须等待2MSL的时间呢\n\n有两个理由：\n\n1. 为了保证A发送的最后一个ACK报文段能够到达B。这个ACK报文段有可能丢失，因而使处在LAST-ACK状态的收不到对己发送的FIN+ACK报文段的确认。B会超时重传这个FIN+ACK报文段，而A就能在2MSL时间内收到这个重传的FIN+ACK报文段。接着A重传一次确认，重新启动2MSL计时器。\n\n2. 防止“已失效的连接请求报文段”出现在本连接中。A在发送完最后一个ACK报文段后，再经过时间2MSL，就可以使得本连接持续的时间所产生的的所有报文段都从网络中消失。这样下一个新的连接不会出现旧的失效的报文段。\n","tags":["tcp/ip"]},{"title":"JDK8 新特性","url":"%2F2019%2F07%2F02%2FJDK8-new-things%2F","content":"\n## 简介\n\nJDK 8 是自JDK 5以来，Oracle对JDK做出的最重大的更新，这个版本中包含了语言、编译器、库、工具、JVM等多种新特性。针对我们平时常用的一些场景，接下来将介绍JDK 8中的新特性。\n\n## JAVA 语言的新特性\n\n### Lambda表达式和函数式接口\n\nLambda的设计者们为了让现有的功能与Lambda表达式良好兼容，考虑了很多方法，于是产生了函数接口这个概念，所以我们先讲函数式接口。\n\n#### 函数式接口\n\n函数式接口(Functional Interface)就是一个有且仅有一个抽象方法，但是可以有多个非抽象方法的接口。\n\n```java\n@FunctionalInterface\ninterface GreetingService {\n    void sayMessage(String message);\n}\n```\n\n<!-- more -->\n\nJava 8为函数式接口引入了一个新注解@FunctionalInterface，主要用于编译级错误检查，加上该注解，当你写的接口不符合函数式接口定义的时候，编译器会报错。\n\n> 加不加 **@FunctionalInterface** 对于接口是不是函数式接口没有影响，该注解只是提醒编译器去检查该接口是否仅包含一个抽象方法。\n\n**函数式接口里允许定义默认方法**\n\n```java\n@FunctionalInterface\ninterface GreetingService{\n    void sayMessage(String message);\n    default void doSomeMoreWork1(){\n        // Method body\n    }\n    default void doSomeMoreWork2(){\n        // Method body\n    }\n}\n```\n\n**函数式接口里允许定义静态方法**\n\n```java\n@FunctionalInterface\ninterface GreetingService {\n    void sayMessage(String message);\n    static void printHello(){\n        System.out.println(\"Hello\");\n    }\n}\n```\n\n**函数式接口里允许定义 java.lang.Object 里的 public 方法**\n\n```java\n@FunctionalInterface\ninterface GreetingService {\n    void sayMessage(String message);\n    @Override\n    boolean equals(Object obj);\n}\n```\n\n>默认方法和静态方法也是JDK 8的新特性，之后会详细的介绍。\n\nJDK 1.8 之前已有的函数式接口:\n\n- java.lang.Runnable\n- java.util.concurrent.Callable\n- java.security.PrivilegedAction\n- java.util.Comparator\n- java.io.FileFilter\n- java.nio.file.PathMatcher\n- java.lang.reflect.InvocationHandler\n- java.beans.PropertyChangeListener\n- java.awt.event.ActionListener\n- javax.swing.event.ChangeListener\n\nJDK 1.8 新增加的函数接口：\n\n- java.util.function\n\n#### Lambda表达式\n\nLambda表达式（也称为闭包）允许我们将函数当成参数传递给某个方法，或者把代码本身当作数据处理。之前的JDK中只能使用匿名内部类代替Lambda表达式。\n\n之前的写法：\n\n```java\ninterface Person {\n    void eat();\n}\n\npublic class Demo {\n    public static void main(String[] args) {\n        Person p = new Person() {\n            public void eat(String s) {\n                System.out.println(\"eat something\" + s);\n            }\n        };\n        p.eat();\n    }\n}\n```\n\nJDK 8的写法：\n\n```java\ninterface Person {\n    void eat();\n}\n\npublic class Demo {\n    public static void main(String[] args) {\n        Person p = (s) -> System.out.println(\"eat something\" + s);\n        p.eat();\n    }\n}\n```\n\n这个操作就像是将`(s) -> System.out.println(\"eat something\" + s)`这段代码赋值给了p这个实例。\n\n我们对比一下：\n\n```java\npublic void eat(String s) {\n  System.out.println(\"eat something\" + s);\n}\n\np = (s) -> System.out.println(\"eat something\" + s);\n```\n\nLambda表达式移除了多余的 `public void eat` , 移除了传入参数的类型 String，一行代码所以可以移除大括号，在参数和函数之间加入了 `->`符号。传统的Java 7必须要求你定义一个“污染环境”的接口实现InterfaceImpl或者使用匿名内部类，而相较之下Java 8的Lambda, 就显得干净很多。\n\n**Lambda表达式并非匿名内部类的语法糖**\n\n实际上，匿名内部类存在着影响应用性能的问题。\n\n首先，编译器会为每一个匿名内部类创建一个类文件。创建出来的类文件的名称通常按照这样的规则 ClassName 符合和数字。生成如此多的文件就会带来问题，因为类在使用之前需要加载类文件并进行验证，这个过程则会影响应用的启动性能。类文件的加载很有可能是一个耗时的操作，这其中包含了磁盘 IO 和解压 JAR 文件。\n\n假设 Lambda 表达式翻译成匿名内部类，那么每一个 Lambda 表达式都会有一个对应的类文件。随着匿名内部类进行加载，其必然要占用 JVM 中的元空间（从 Java 8 开始永久代的一种替代实现）。如果匿名内部类的方法被 JIT 编译成机器代码，则会存储到代码缓存中。同时，匿名内部类都需要实例化成独立的对象。以上关于匿名内部类的种种会使得应用的内存占用增加。因此我们有必要引入新的缓存机制减少过多的内存占用，这也就意味着我们需要引入某种抽象层。\n\n总的来说，lambda的大致思路如下：\n\n1. lamdba表达式被编译生成当前类的一个私有静态方法\n2. 在原调用Lamdba方法的地方编译成了一个invokedynamic指令（java7 JVM中增加了一个新的指令）调用，同时呢也生成了一个对应的BootstrapMethod\n3. 当lamdba表达式被JVM执行，也就是碰到2中说到的invokedynamic指令，该指令引导调用LambdaMetafactory.metafactory方法，该方法返回一个CallSite实例\n4. 这个CallSite实例中的target对象，也就是直接引用到一个MethodHandle实例，而这个MethodHandle实例会调用到1中生成的静态方法，在上面的例子就是lambda$main$0这个方法，完成整个lamdba表达式的使用\n\n**查看原文：**[Java 8 Lambdas - A Peek Under the Hood](http://www.infoq.com/articles/Java-8-Lambdas-A-Peek-Under-the-Hood)\n\n### 接口的默认方法和静态方法\n\n默认方法和抽象方法之间的区别在于抽象方法需要实现，而默认方法不需要。接口提供的默认方法会被接口的实现类继承或者覆写，例子代码如下：\n\n```java\nprivate interface Defaulable {\n    default String notRequired() {\n        return \"Default implementation\";\n    }\n    static Defaulable create( Supplier< Defaulable > supplier ) {\n        return supplier.get();\n    }\n}\n\nprivate static class DefaultableImpl implements Defaulable {\n}\n\nprivate static class OverridableImpl implements Defaulable {\n    @Override\n    public String notRequired() {\n        return \"Overridden implementation\";\n    }\n}\n```\n\n### 方法引用\n\n方法引用使得开发者可以直接引用现存的方法、Java类的构造方法或者实例对象，总的来说，一共有以下几种形式：\n\n- 静态方法引用：ClassName::methodName;\n- 实例上的实例方法引用：instanceName::methodName;\n- 超类上的实例方法引用：supper::methodName;\n- 类的实例方法引用：ClassName:methodName;\n- 构造方法引用Class:new;\n- 数组构造方法引用::TypeName[]::new\n\n构造器引用，语法是**Class::new**，或者更一般的形式：**Class<T>::new**。\n\n```java\nfinal Test test = Test::new;\n```\n\n静态方法引用，语法是**Class::static_method**\n\n```java\nlist.forEach(System.out::println);\n```\n\n### 重复注解\n\n在Java 8中使用**@Repeatable**注解定义重复注解\n\n```java\n    @Target( ElementType.TYPE )\n    @Retention( RetentionPolicy.RUNTIME )\n    public @interface Filters {\n        Filter[] value();\n    }\n\n    @Target( ElementType.TYPE )\n    @Retention( RetentionPolicy.RUNTIME )\n    @Repeatable( Filters.class )\n    public @interface Filter {\n        String value();\n    };\n\n    @Filter( \"filter1\" )\n    @Filter( \"filter2\" )\n    public interface Filterable {  \n    }\n```\n\n## Java官方库的新特性\n\n### Optional\n\nJava应用中最常见的bug就是**NullPointerException**，JDK参考了Guava的**Optionals**类来解决**NullPointerException**，从而避免源码被各种**null**检查污染。\n\n看一个简单样例：\n\n```java\nOptional< String > firstName = Optional.of( \"Tom\" );\nSystem.out.println( \"First Name is set? \" + firstName.isPresent() );\nSystem.out.println( \"First Name: \" + firstName.orElseGet( () -> \"[none]\" ) );\nSystem.out.println( firstName.map( s -> \"Hey \" + s + \"!\" ).orElse( \"Hey Stranger!\" ) );\nSystem.out.println();\n```\n\n更直接的例子，使用JPA访问数据库（最新的Mybatis已经支持，但是代码生成工具还没有支持）：\n\n```java\n@Repository\npublic interface UserInfoRepository extends JpaRepository<UserInfo, String> {\n    UserInfo findByName(String name);\n}\n\npublic void business() {\n    UserInfo user = userInfoRepository.findByName(\"dumdum\");\n    if (Objects.isNull(user)) {\n        // throw exception\n    } else {\n        // do something\n    }\n}\n\n//----------------------------------------------------------------------------------\n//使用Optional\n@Repository\npublic interface UserInfoRepository extends JpaRepository<UserInfo, String> {\n    Optional<Userinfo> findByName(String name);\n}\n\npublic void business() {\n    Optional<Userinfo> userOpt = userInfoRepository.findByName(\"dumdum\");\n    UserInfo user = userOpt.orElseThrow(() ->\n            new Exception(\"查無 userInfo(dumdum) 資訊.\"));\n    // UserInfo user = userOpt.orElse(new UserInfo(\"Rocko\")));\n    // do something\n}\n\n```\n\n### Streams\n\n有了函数式编程之后，配合StreamAPI，极大得简化了集合操作（后面我们会看到不止是集合）。\n\n```java\nprivate enum Status {\n  OPEN, CLOSED\n};\n\nCollection< Task > tasks = Arrays.asList(\n    new Task( Status.OPEN, 5 ),\n    new Task( Status.OPEN, 13 ),\n    new Task( Status.CLOSED, 8 )\n);\n\n//如何计算集合中每个任务的点数在集合中所占的比重\n Collection< String > result = tasks\n    .stream()                                        // Stream< String >\n    .mapToInt( Task::getPoints )                     // IntStream\n    .asLongStream()                                  // LongStream\n    .mapToDouble( points -> points / totalPoints )   // DoubleStream\n    .boxed()                                         // Stream< Double >\n    .mapToLong( weigth -> ( long )( weigth * 100 ) ) // LongStream\n    .mapToObj( percentage -> percentage + \"%\" )      // Stream< String>\n    .collect( Collectors.toList() );                 // List< String >\n\nSystem.out.println( result );\n```\n\n传统的IO操作（从文件或者网络一行一行得读取数据）可以受益于steam处理, 配合try-with-resource写法\n\n```java\nPath path = new File( filename ).toPath();\ntry( Stream< String > lines = Files.lines( path, StandardCharsets.UTF_8 ) ) {\n    lines.onClose( () -> System.out.println(\"Done!\") ).forEach( System.out::println );\n}\n```\n\n### Date/Time API(JSR 310)\n\n**java.util.Date**和后来的**java.util.Calendar**一直存在着诸多的问题，比如：\n\n1. 线程安全问题：java.util.Date是非线程安全的，所有的日期类都是可变的；\n\n2. 设计很差：在java.util和java.sql的包中都有日期类，此外，用于格式化和解析的类在java.text包中也有定义。而每个包将其合并在一起，也是不合理的；\n\n3. 时区处理麻烦：日期类不提供国际化，没有时区支持，因此Java中引入了java.util.Calendar和Java.util.TimeZone类；\n\n从而催生了第三方库Joda-Time，Java 8中新的时间和日期管理API深受Joda-Time影响，并吸收了很多Joda-Time的精华。\n\n在java.util.time包中常用的几个类有：\n\n- 它通过指定一个时区，然后就可以获取到当前的时刻，日期与时间。Clock可以替换System.currentTimeMillis()与TimeZone.getDefault()\n- Instant:一个instant对象表示时间轴上的一个时间点，Instant.now()方法会返回当前的瞬时点（格林威治时间）；\n- Duration:用于表示两个瞬时点相差的时间量；\n- LocalDate:一个带有年份，月份和天数的日期，可以使用静态方法now或者of方法进行创建；\n- LocalTime:表示一天中的某个时间，同样可以使用now和of进行创建；\n- LocalDateTime：兼有日期和时间；\n- ZonedDateTime：通过设置时间的id来创建一个带时区的时间；\n- DateTimeFormatter：日期格式化类，提供了多种预定义的标准格式；\n\n#### Spring 中 LocalDateTime格式处理\n\n##### Controller接收LocalDateTime参数\n\n@RequestParam @DateTimeFormat(pattern = \"yyyy-MM-dd HH:mm:ss\") LocalDateTime date\n\n##### ResponseBody格式化LocalDateTime\n\nSpring默认使用jackson来进行json格式转换，我们只需要使用@Bean注解创建一个ObjectMapperbean，并将JavaTimeModule注册到ObjectMapper中即可，spring会使用该bean创建MappingJackson2HttpMessageConverter进行json格式转换。这里需要加入`jackson`的`jsr310`扩展包。\n\n```java\n<dependency>\n    <groupId>com.fasterxml.jackson.datatype</groupId>\n    <artifactId>jackson-datatype-jsr310</artifactId>\n    <version>2.8.9</version>\n</dependency>\n```\n\n```java\n@Bean(name = \"mapperObject\")\npublic ObjectMapper getObjectMapper() {\n    ObjectMapper om = new ObjectMapper();\n    JavaTimeModule javaTimeModule = new JavaTimeModule();\n    javaTimeModule.addSerializer(LocalDateTime.class, new LocalDateTimeSerializer(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\")));\n    javaTimeModule.addSerializer(LocalDate.class, new LocalDateSerializer(DateTimeFormatter.ofPattern(\"yyyy-MM-dd\")));\n    javaTimeModule.addSerializer(LocalTime.class, new LocalTimeSerializer(DateTimeFormatter.ofPattern(\"HH:mm:ss\")));\n    om.registerModule(javaTimeModule);\n    return om;\n}\n```\n\n或者使用（缺点是每一个变量都需要加这个注解）\n\n```java\n@JsonSerialize(using = LocalDateTimeSerializer.class)\n```\n\n>另外，如果持久层框架使用mybatis，同样需要加入`mybatis`的`jsr310` 扩展包。\n\n```java\n<dependency>\n    <groupId>org.mybatis</groupId>\n    <artifactId>mybatis-typehandlers-jsr310</artifactId>\n    <version>1.0.2</version>\n</dependency>\n```\n\n### Base64\n\n对Base64编码的支持已经被加入到Java 8官方库中，这样不需要使用第三方库就可以进行Base64编码，例子代码如下：\n\n```java\nString text = \"Base64 finally in Java 8!\";\nString encoded = Base64.getEncoder()\n  .encodeToString( text.getBytes( StandardCharsets.UTF_8 ) );\nSystem.out.println( encoded );\nString decoded = new String(\n  Base64.getDecoder()\n  .decode( encoded ),StandardCharsets.UTF_8\n);\nSystem.out.println( decoded );\n```\n","tags":["Java"]},{"title":"Maven入门知识","url":"%2F2019%2F06%2F06%2Fmaven-knowledge%2F","content":"\n> Maven 是 Apache 下的一个纯 Java 开发的项目管理工具，可以对 Java 项目进行构建、依赖管理。\n\n## 约定配置\n\nMaven 提倡使用一个共同的标准目录结构，Maven 使用约定优于配置的原则，大家尽可能的遵守这样的目录结构。如下所示：\n\n|                目录                |                             作用                             |\n| :--------------------------------: | :----------------------------------------------------------: |\n|             ${basedir}             |                  存放pom.xml和所有的子目录                   |\n|      ${basedir}/src/main/java      |                       项目的java源代码                       |\n|   ${basedir}/src/main/resources    |        项目的资源，比如说property文件，springmvc.xml         |\n|      ${basedir}/src/test/java      |                项目的测试类，比如说Junit代码                 |\n|   ${basedir}/src/test/resources    |                         测试用的资源                         |\n| ${basedir}/src/main/webapp/WEB-INF | web应用文件目录，web项目的信息，比如存放web.xml、本地图片、jsp视图页面 |\n|         ${basedir}/target          |                         打包输出目录                         |\n|     ${basedir}/target/classes      |                         编译输出目录                         |\n|   ${basedir}/target/test-classes   |                       测试编译输出目录                       |\n\n<!-- more -->\n\n## Maven POM\n\nPOM( Project Object Model，项目对象模型 ) 是 Maven 工程的基本工作单元，是一个XML文件，包含了项目的基本信息，用于描述项目如何构建，声明项目依赖，等等。\n\n执行任务或目标时，Maven 会在当前目录中查找 POM。它读取 POM，获取所需的配置信息，然后执行目标。\n\n所有 POM 文件都需要 project 元素和三个必需字段：groupId，artifactId，version。\n\n样例\n\n```xml\n<project xmlns = \"http://maven.apache.org/POM/4.0.0\"\n    xmlns:xsi = \"http://www.w3.org/2001/XMLSchema-instance\"\n    xsi:schemaLocation = \"http://maven.apache.org/POM/4.0.0\n    http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n\n    <!-- 模型版本 -->\n    <modelVersion>4.0.0</modelVersion>\n    <!-- 公司或者组织的唯一标志，并且配置时生成的路径也是由此生成 -->\n    <groupId>com.58suo.mes</groupId>\n\n    <!-- 项目的唯一ID，一个groupId下面可能多个项目，就是靠artifactId来区分的 -->\n    <artifactId>718mes</artifactId>\n\n    <!-- 版本号 -->\n    <version>1.0</version>\n</project>\n```\n\n\n\n|     节点     |                             描述                             |\n| :----------: | :----------------------------------------------------------: |\n|   project    |                         工程的根标签                         |\n| modelVersion |                    模型版本需要设置为 4.0                    |\n|   groupId    | 这是工程组的标识。它在一个组织或者项目中通常是唯一的<br />一般是网站域名的反写 |\n|  artifactId  |                        这是工程的标识                        |\n|   version    |                         工程的版本号                         |\n\n## Maven 构建生命周期\n\n不同的生命周期一般还有不同的阶段，当我们执行 mvn post-clean 命令时，Maven 调用 clean 生命周期，它包含以下阶段：\n\n- pre-clean：执行一些需要在clean之前完成的工作\n- clean：移除所有上一次构建生成的文件\n- post-clean：执行一些需要在clean之后立刻完成的工作\n\nmvn clean 中的 clean 就是上面的 clean，在一个生命周期中，运行某个阶段的时候，它之前的所有阶段都会被运行，但不包含之后的周期。\n\n重要的周期如下：\n\n|       生命周期        |                             描述                             |\n| :-------------------: | :----------------------------------------------------------: |\n|       validate        | 检查工程配置是否正确，完成构建过程的所有必要信息是否能够获取到。 |\n|      initialize       |                初始化构建状态，例如设置属性。                |\n|   generate-sources    |             生成编译阶段需要包含的任何源码文件。             |\n|    process-sources    |      处理源代码，例如，过滤任何值（filter any value）。      |\n|  generate-resources   |               生成工程包中需要包含的资源文件。               |\n|   process-resources   |      拷贝和处理资源文件到目的目录中，为打包阶段做准备。      |\n|        compile        |                        编译工程源码。                        |\n|    process-classes    |   处理编译生成的文件，例如 Java Class 字节码的加强和优化。   |\n| generate-test-sources |            生成编译阶段需要包含的任何测试源代码。            |\n| process-test-sources  |    处理测试源代码，例如，过滤任何值（filter any values)。    |\n|     test-compile      |                编译测试源代码到测试目的目录。                |\n| process-test-classes  |              处理测试代码文件编译后生成的文件。              |\n|         test          |        使用适当的单元测试框架（例如JUnit）运行测试。         |\n|    prepare-package    |        在真正打包之前，为准备打包执行任何必要的操作。        |\n|        package        | 获取编译后的代码，并按照可发布的格式进行打包，例如 JAR、WAR 或者 EAR 文件。 |\n| pre-integration-test  | 在集成测试执行之前，执行所需的操作。例如，设置所需的环境变量。 |\n|   integration-test    |      处理和部署必须的工程包到集成测试能够运行的环境中。      |\n| post-integration-test |      在集成测试被执行后执行必要的操作。例如，清理环境。      |\n|        verify         |      运行检查操作来验证工程包是有效的，并满足质量要求。      |\n|        install        |  安装工程包到本地仓库中，该仓库可以作为本地其他工程的依赖。  |\n|        deploy         |  拷贝最终的工程包到远程仓库中，以共享给其他开发人员和工程。  |\n\n## Maven 不同环境配置\n\nmaven pom文件中包含了`<profiles>`节点，以此来为不同的环境设置不同的配置文件，构建过程等，例如开发环境，测试环境，生产环境等。\n\nprofiles 节点样例：\n\n```xml\n <profiles>\n        <!--根据环境参数或命令行参数激活某个构建处理 -->\n        <profile>\n            <!--构建配置的唯一标识符。即用于命令行激活，也用于在继承时合并具有相同标识符的profile。 -->\n            <id />\n            <!--自动触发profile的条件逻辑。Activation是profile的开启钥匙。profile的力量来自于它 能够在某些特定的环境中自动使用某些特定的值；这些环境通过activation元素指定。activation元素并不是激活profile的唯一方式。 -->\n            <activation>\n                <!--profile默认是否激活的标志 -->\n                <activeByDefault />\n                <!--当匹配的jdk被检测到，profile被激活。例如，1.4激活JDK1.4，1.4.0_2，而!1.4激活所有版本不是以1.4开头的JDK。 -->\n                <jdk />\n                <!--当匹配的操作系统属性被检测到，profile被激活。os元素可以定义一些操作系统相关的属性。 -->\n                <os>\n                    <!--激活profile的操作系统的名字 -->\n                    <name>Windows XP</name>\n                    <!--激活profile的操作系统所属家族(如 'windows') -->\n                    <family>Windows</family>\n                    <!--激活profile的操作系统体系结构 -->\n                    <arch>x86</arch>\n                    <!--激活profile的操作系统版本 -->\n                    <version>5.1.2600</version>\n                </os>\n                <!--如果Maven检测到某一个属性（其值可以在POM中通过${名称}引用），其拥有对应的名称和值，Profile就会被激活。如果值 字段是空的，那么存在属性名称字段就会激活profile，否则按区分大小写方式匹配属性值字段 -->\n                <property>\n                    <!--激活profile的属性的名称 -->\n                    <name>mavenVersion</name>\n                    <!--激活profile的属性的值 -->\n                    <value>2.0.3</value>\n                </property>\n                <!--提供一个文件名，通过检测该文件的存在或不存在来激活profile。missing检查文件是否存在，如果不存在则激活 profile。另一方面，exists则会检查文件是否存在，如果存在则激活profile。 -->\n                <file>\n                    <!--如果指定的文件存在，则激活profile。 -->\n                    <exists>/usr/local/hudson/hudson-home/jobs/maven-guide-zh-to-production/workspace/\n                    </exists>\n                    <!--如果指定的文件不存在，则激活profile。 -->\n                    <missing>/usr/local/hudson/hudson-home/jobs/maven-guide-zh-to-production/workspace/\n                    </missing>\n                </file>\n            </activation>\n            <build/><!--构建项目所需要的信息。参见Maven构建一节。-->\n```\n\n## Maven 构建\n\n**pom.xml中的两种build**\n\n- Project Build, `<project>`的直接子元素`<build>`\n- Profile Build, `<profile>` 的子元素`<build>`\n\nProfile Build包含了基本的build元素，而Project Build还包含两个特殊的元素，即各种<...Directory>和<…extensions>。\n\n### 共有节点\n\n#### 直接子节点\n\n```xml\n<build>\n    <defaultGoal></defaultGoal>\n    <directory></directory>\n    <finalName></finalName>\n</build>\n```\n\n说明：\n\n- defaultGoal，执行构建时默认的goal或phase，如jar:jar或者package等\n- directory，构建的结果所在的路径，默认为${basedir}/target目录\n- finalName，构建的最终结果的名字，该名字可能在其他plugin中被改变\n\n#### resources节点\n\n资源往往不是代码，无需编译，而是一些properties或XML配置文件，构建过程中会往往会将资源文件从源路径复制到指定的目标路径。\n\n```xml\n<build>\n    <filters>\n        <filter></filter>\n    </filters>\n    <resources>\n        <resource>\n            <targetPath></targetPath>\n            <filtering></filtering>\n            <directory></directory>\n            <includes>\n                <include></include>\n            </includes>\n            <excludes>\n                <exclude></exclude>\n            </excludes>\n        </resource>\n    </resources>\n    <testResources>\n    </testResources>\n</build>\n```\n\n说明：\n\n- resources，build过程中涉及的资源文件\n\n- targetPath，资源文件的目标路径\n- filtering，构建过程中是否对资源进行过滤，默认false\n- directory，资源文件的路径，默认位于${basedir}/src/main/resources/目录下\n- includes，一组文件名的匹配模式，被匹配的资源文件将被构建过程处理\n- excludes，一组文件名的匹配模式，被匹配的资源文件将被构建过程忽略。同时被includes和excludes匹配的资源文件，将被忽略。\n\n- filters，给出对资源文件进行过滤的属性文件的路径，默认位于${basedir}/src/main/filters/目录下。属性文件中定义若干键值对。在构建过程中，对于资源文件中出现的变量（键），将使用属性文件中该键对应的值替换。\n- testResources，test过程中涉及的资源文件，默认位于${basedir}/src/test/resources/目录下。这里的资源文件不会被构建到目标构件中\n\n#### plugins节点\n\n`<plugins>`给出构建过程中所用到的插件，例如maven-assembly这个插件, 可以打包成zip、tar.gz等格式：\n\n```xml\n\n<build>\n <plugins>\n  <plugin>\n   <groupId>org.apache.maven.plugins</groupId>\n   <artifactId>maven-assembly-plugin</artifactId>\n   <version>2.4</version>\n   <configuration>\n    <encoding>UTF-8</encoding>\n    <descriptors>\n     <descriptor>src/main/assembly/package.xml</descriptor>\n    </descriptors>\n    <outputDirectory>c:\\\\</outputDirectory>\n   </configuration>\n   <executions>\n    <execution>\n     <id>make-assembly</id>\n     <phase>package</phase>\n     <goals>\n      <goal>single</goal>\n     </goals>\n    </execution>\n   </executions>\n  </plugin>\n </plugins>\n</build>\n```\n\n## Project Build特有的<...Directory>\n\n```xml\n<build>\n    <sourceDirectory>${basedir}/src/main/java</sourceDirectory>\n    <scriptSourceDirectory></scriptSourceDirectory>\n    <testSourceDirectory>src/test/java</testSourceDirectory>\n    <outputDirectory></outputDirectory>\n    <testOutputDirectory></testOutputDirectory>\n</build>\n```\n\n目录可以使用绝对路径；如果使用相对路径，则所有的相对路径都是在${basedir}目录下。\n\n## Project Build特有的`<extensions>`\n\n`<extensions>`是执行构建过程中可能用到的其他工具，在执行构建的过程中被加入到classpath中。\n\n也可以通过`<extensions>`激活构建插件，从而改变构建的过程。\n\n通常，通过`<extensions>`给出通用插件的一个具体实现，用于构建过程。\n\n## Maven仓库\n\nMaven 仓库有三种类型：\n\n- 本地（local）\n- 中央（central）\n- 远程（remote）\n\n### 本地仓库\n\n运行 Maven 的时候，Maven 所需要的任何构件都是直接从本地仓库获取的。如果本地仓库没有，它会首先尝试从远程仓库下载构件至本地仓库，然后再使用本地仓库的构件。\n\n默认情况下，不管Linux还是 Windows，每个用户在自己的用户目录下都有一个路径名为 .m2/respository/ 的仓库目录。\n\nMaven 本地仓库默认被创建在 %USER_HOME% 目录下。要修改默认位置，在 %M2_HOME%\\conf 目录中的 Maven 的 settings.xml 文件中定义另一个路径。\n\n```xml\n<settings xmlns=\"http://maven.apache.org/SETTINGS/1.0.0\"\n   xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n   xsi:schemaLocation=\"http://maven.apache.org/SETTINGS/1.0.0\n   http://maven.apache.org/xsd/settings-1.0.0.xsd\">\n      <localRepository>C:/MyLocalRepository</localRepository>\n</settings>\n```\n\n### 中央仓库\n\nMaven 中央仓库是由 Maven 社区提供的仓库，其中包含了大量常用的库。\n\n中央仓库包含了绝大多数流行的开源Java构件，以及源码、作者信息、SCM、信息、许可证信息等。一般来说，简单的Java项目依赖的构件都可以在这里下载到。\n\n中央仓库的关键概念：\n\n- 这个仓库由 Maven 社区管理。\n- 不需要配置。\n- 需要通过网络才能访问。\n\n要浏览中央仓库的内容，maven 社区提供了一个 URL：<http://search.maven.org/#browse>。使用这个仓库，开发人员可以搜索所有可以获取的代码库。\n\n### 远程仓库\n\n如果 Maven 在中央仓库中也找不到依赖的文件，它会停止构建过程并输出错误信息到控制台。为避免这种情况，Maven 提供了远程仓库的概念，它是开发人员自己定制仓库，包含了所需要的代码库或者其他工程中用到的 jar 文件。\n\n举例说明，使用下面的 pom.xml，Maven 将从远程仓库中下载该 pom.xml 中声明的所依赖的（在中央仓库中获取不到的）文件。\n\n```xml\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n   xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n   xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0\n   http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n   <modelVersion>4.0.0</modelVersion>\n   <groupId>com.companyname.projectgroup</groupId>\n   <artifactId>project</artifactId>\n   <version>1.0</version>\n   <dependencies>\n      <dependency>\n         <groupId>com.companyname.common-lib</groupId>\n         <artifactId>common-lib</artifactId>\n         <version>1.0.0</version>\n      </dependency>\n   <dependencies>\n   <repositories>\n      <repository>\n         <id>companyname.lib1</id>\n         <url>http://download.companyname.org/maven2/lib1</url>\n      </repository>\n   </repositories>\n</project>\n```\n\n### Maven 依赖搜索顺序\n\n当我们执行 Maven 构建命令时，Maven 开始按照以下顺序查找依赖的库：\n\n- **步骤 1** － 在本地仓库中搜索，如果找不到，执行步骤 2，如果找到了则执行其他操作。\n- **步骤 2** － 在中央仓库中搜索，如果找不到，并且有一个或多个远程仓库已经设置，则执行步骤 4，如果找到了则下载到本地仓库中以备将来引用。\n- **步骤 3** － 如果远程仓库没有被设置，Maven 将简单的停滞处理并抛出错误（无法找到依赖的文件）。\n- **步骤 4** － 在一个或多个远程仓库中搜索依赖的文件，如果找到则下载到本地仓库以备将来引用，否则 Maven 将停止处理并抛出错误（无法找到依赖的文件）。\n\n在我们的maven配置里面访问的maven私服就属于中心仓库的一个镜像。如果联网一般会使用阿里的maven仓库：\n\n```xml\n<mirrors>\n    <mirror>\n      <id>alimaven</id>\n      <name>aliyun maven</name>\n      <url>http://maven.aliyun.com/nexus/content/groups/public/</url>\n      <mirrorOf>central</mirrorOf>\n    </mirror>\n</mirrors>\n```\n","tags":["maven"]},{"title":"Docker 多容器部署 LNMP 环境","url":"%2F2018%2F05%2F08%2Fdocker-lnmp%2F","content":"\n## 整个流程\n\n1. 客户端 http 请求服务器 80 端口，该端口被映射到 Nginx 容器 80 端口，进入 Nginx 处理。\n2. Nginx 分析请求，如果是静态资源，直接服务器读取内容；如果是 PHP 脚本，通过 PHP 容器调用服务器获取脚本，然后 FastCGI 处理。\n3. FastCGI 解析 PHP 脚本，必要时访问 MySQL 容器读写数据。\n\n<!-- more -->\n\n### MySQL\n\n```sh\ndocker pull mysql:8.0.16\ndocker run --restart=always -p 3306:3306 --name mysql -e MYSQL_ROOT_PASSWORD=${ADMIN_PASSWORD}  -d mysql:8.0.16 --default-authentication-plugin=mysql_native_password\n```\n\n### Redis\n\n```sh\ndocker pull redis:5.0.4\ndocker run --restart=always -d --name redis -p 6379:6379 redis:5.0.4\n```\n\n或者镜像备份恢复\n\n```sh\ndocker save redis:5.0.4 > redis.tar # 备份镜像\ndocker load < redis.tar # 还原镜像\n```\n\n### PHP\n\n```sh\nmkdir -p  ~/nginx/html\nmkdir -p  ~/php/\ndocker pull php:5.6.36-fpm\ndocker run --restart=always --name php-fpm --link mysql:mysql -d -p 9000:9000 -v ~/nginx/html:/usr/share/nginx/html -v ~/php/www.conf:/usr/local/etc/php-fpm.d/www.conf -v ~/php/php.ini:/usr/local/etc/php/php.ini php:5.6.36-fpm\n```\n\n目录结构:\n\n~/php/\n-> [www.conf](/txt/php/www.conf)\n-> [php.ini](/txt/php/php.ini)\n\n### Nginx\n\n```sh\nmkdir -p  ~/nginx/html\nmkdir -p  ~/nginx/log\ntouch ~/nginx/log/access.log ~/nginx/log/error.log\nmkdir -p  ~/nginx/conf.d\ndocker pull nginx:1.13.12\ndocker run --restart=always --name nginx --link php-fpm:php-fpm -d -p 80:80 -v ~/nginx/html:/usr/share/nginx/html -v ~/nginx/nginx.conf:/etc/nginx/nginx.conf:ro -v ~/nginx/conf.d:/etc/nginx/conf.d -v ~/nginx/log:/var/log/nginx/ nginx:1.13.12\n```\n\n目录结构:\n\n~/nginx/\n->conf.d/\n->->[default.conf](/txt/nginx/conf.d/default.conf)\n->html/\n->->[50x.html](/txt/nginx/html/50x.txt)\n->->[index.html](/txt/nginx/html/index.txt)\n->->[phpinfo.php](/txt/nginx/html/phpinfo.txt)\n->[nginx.conf](/txt/nginx/nginx.conf)\n","tags":["docker"]},{"title":"SpringMVC执行流程及工作原理","url":"%2F2018%2F04%2F26%2Fspring-mvc-execution-process%2F","content":"\n图解SpringMVC执行流程:\n\n![SpringMVC执行流程](/images/spring/spring-mvc-process.png)\n\nSpringMVC执行流程:\n\n1. 用户发送请求至前端控制器DispatcherServlet\n2. DispatcherServlet收到请求调用处理器映射器HandlerMapping。\n3. 处理器映射器根据请求url找到具体的处理器，生成处理器执行链HandlerExecutionChain(包括处理器对象和处理器拦截器)一并返回给DispatcherServlet。\n4. DispatcherServlet根据处理器Handler获取处理器适配器HandlerAdapter执行HandlerAdapter处理一系列的操作，如：参数封装，数据格式转换，数据验证等操作\n5. 执行处理器Handler(Controller，也叫页面控制器)。\n6. Handler执行完成返回ModelAndView\n7. HandlerAdapter将Handler执行结果ModelAndView返回到DispatcherServlet\n8. DispatcherServlet将ModelAndView传给ViewReslover视图解析器\n9. ViewReslover解析后返回具体View\n10. DispatcherServlet对View进行渲染视图（即将模型数据model填充至视图中）。\n11. DispatcherServlet响应用户。\n","tags":["Java"]},{"title":"Java 单例模式的两种高效写法","url":"%2F2018%2F03%2F22%2Fsingleton%2F","content":"\n## 前言\n\n在Java对象的创建时，单例模式使用尤其多，同时也是个面试必问的基础题。很多时候面试官想问的无非是懒汉式的双重检验锁。但是其实还有两种更加直观高效的写法，也是《Effective Java》中所推荐的写法。\n\n## 静态内部类(static nested class)\n\n```java\npublic class Singleton {\n\n  private static class SingletonHolder {\n    private static final Singleton INSTANCE = new Singleton();\n  }\n\n  private Singleton() {}\n\n  public static final Singleton getInstance() {\n    return SingletonHolder.INSTANCE;\n  }\n\n}\n```\n\n这种写法仍然使用JVM本身机制保证了线程安全问题；由于 SingletonHolder 是私有的，除了 getInstance() 之外没有办法访问它，因此它是懒汉式的；同时读取实例的时候不会进行同步，没有性能缺陷；也不依赖 JDK 版本。\n\n<!-- more -->\n\n## 枚举 Enum\n\n在《Effective Java》最后推荐了这样一个写法，简直有点颠覆，不仅超级简单，而且保证了现场安全。这里引用一下，此方法无偿提供了序列化机制，绝对防止多次实例化，及时面对复杂的序列化或者反射攻击。单元素枚举类型已经成为实现Singleton的最佳方法。\n\n```java\npublic enum Singleton {\n  INSTANCE;\n}\n```\n\n对于一个标准的enum单例模式，最优秀的写法还是实现接口的形式:\n\n```java\n// 定义单例模式中需要完成的代码逻辑\npublic interface MySingleton {\n  void doSomething();\n}\n\npublic enum Singleton implements MySingleton {\n  INSTANCE {\n    @Override\n    public void doSomething() {\n        System.out.println(\"complete singleton\");\n    }\n  };\n\n  public static MySingleton getInstance() {\n    return Singleton.INSTANCE;\n  }\n}\n```\n","tags":["Java"]},{"title":"Semaphore -- 信号量","url":"%2F2018%2F03%2F21%2Fsemaphore%2F","content":"\n## 作用\n\n1. Semaphore就是一个信号量，它的作用是限制某段代码块的并发数。\n2. Semaphore有一个构造函数，可以传入一个int型整数n，表示某段代码最多只有n个线程可以访问\n3. 如果超出了n，那么请等待，等到某个线程执行完毕这段代码块，下一个线程再进入。\n4. 由此可以看出如果Semaphore构造函数中传入的int型整数n=1，相当于变成了一个synchronized了。\n\nSemaphore类位于java.util.concurrent包下，它提供了2个构造器：\n\n```java\n//参数permits表示许可数目，即同时可以允许多少线程进行访问\npublic Semaphore(int permits) {\n  sync = new NonfairSync(permits);\n}\n//这个多了一个参数fair表示是否是公平的，即等待时间越久的越先获取许可\npublic Semaphore(int permits, boolean fair) {\n  sync = (fair)? new FairSync(permits) : new NonfairSync(permits);\n}\n```\n\nSemaphore类中比较重要的几个方法，首先是acquire()、release()方法：\n\n1. acquire()用来获取一个许可，若无许可能够获得，则会一直等待，直到获得许可。\n2. release()用来释放许可。注意，在释放许可之前，必须先获获得许可。\n\nacquire()方法会被阻塞，如果想立即得到执行结果，可以使用下面几个方法：\n\n```java\n//尝试获取一个许可，若获取成功，则立即返回true，若获取失败，则立即返回false\npublic boolean tryAcquire() { };\n//尝试获取一个许可，若在指定的时间内获取成功，则立即返回true，否则则立即返回false\npublic boolean tryAcquire(long timeout, TimeUnit unit) throws InterruptedException { };\n//尝试获取permits个许可，若获取成功，则立即返回true，若获取失败，则立即返回false\npublic boolean tryAcquire(int permits) { };\n//尝试获取permits个许可，若在指定的时间内获取成功，则立即返回true\npublic boolean tryAcquire(int permits, long timeout, TimeUnit unit) throws InterruptedException { };\n//得到当前可用的许可数目\npublic int availablePermits();\n```\n\n<!-- more -->\n\n## 示例\n\n假若一个工厂有5台机器，但是有8个工人，一台机器同时只能被一个工人使用，只有使用完了，其他工人才能继续使用。那么我们就可以通过Semaphore来实现：\n\n```java\npublic class Test {\n    public static void main(String[] args) {\n        int N = 8; //工人数\n        Semaphore semaphore = new Semaphore(5); //机器数目\n        for(int i=0;i<N;i++)\n            new Worker(i,semaphore).start();\n    }\n    static class Worker extends Thread{\n        private int num;\n        private Semaphore semaphore;\n        public Worker(int num,Semaphore semaphore){\n            this.num = num;\n            this.semaphore = semaphore;\n        }\n        @Override\n        public void run() {\n            try {\n                semaphore.acquire();\n                System.out.println(\"工人\"+this.num+\"占用一个机器在生产...\");\n                Thread.sleep(2000);\n                System.out.println(\"工人\"+this.num+\"释放出机器\");\n                semaphore.release();\n            } catch (InterruptedException e) {\n                e.printStackTrace();\n            }\n        }\n    }\n}\n```\n\n运行结果：\n\n```java\n工人0占用一个机器在生产...\n工人1占用一个机器在生产...\n工人2占用一个机器在生产...\n工人4占用一个机器在生产...\n工人5占用一个机器在生产...\n工人0释放出机器\n工人2释放出机器\n工人3占用一个机器在生产...\n工人7占用一个机器在生产...\n工人4释放出机器\n工人5释放出机器\n工人1释放出机器\n工人6占用一个机器在生产...\n工人3释放出机器\n工人7释放出机器\n工人6释放出机器\n```\n","tags":["Concurrency"]},{"title":"Java线程池类","url":"%2F2018%2F03%2F20%2FThreadPool%2F","content":"\n## 线程池的优点\n\n1. 线程是稀缺资源，使用线程池可以减少创建和销毁线程的次数，每个工作线程都可以重复使用。\n\n2. 可以根据系统的承受能力，调整线程池中工作线程的数量，防止因为消耗过多内存导致服务器崩溃。\n\n## 线程池的原理\n\n当我们把一个Runnable交给线程池去执行的时候，这个线程池处理的流程是这样的：\n\n1. 当线程数小于corePoolSize时，创建线程执行任务。\n2. 当线程数大于等于corePoolSize并且workQueue没有满时，放入workQueue中\n3. 线程数大于等于corePoolSize并且当workQueue满时，新任务新建线程运行，线程总数要小于maximumPoolSize\n4. 当线程总数等于maximumPoolSize并且workQueue满了的时候执行handler的rejectedExecution。也就是拒绝策略。\n\n## 任务拒接策略(RejectedExecutionHandler)\n\n当队列和线程池都满了的时候，再有新的任务到达，就必须要有一种办法来处理新来的任务。Java线程池中提供了以下四种策略：\n\n1. AbortPolicy: 直接抛异常\n2. CallerRunsPolicy：让调用者帮着跑这个任务\n3. DiscardOldestPolicy：会抛弃任务队列中最旧的任务，再把这个新任务添加进去。\n4. DiscardPolicy：不处理，直接扔掉\n\n<!-- more -->\n\n## ExecutorService\n\nJava中的线程池类有两个，分别是：ThreadPoolExecutor和ScheduledThreadPoolExecutor，这两个类都继承自ExecutorService。利用这两个类，可以创建各种不同的Java线程池，为了方便我们创建线程池，Java API提供了Executors工厂类来帮助我们创建各种各样的线程池。\n\nJava线程池ExecutorService继承树：\n\n![ExecutorService继承树](/images/threadpool/ExecutorService继承树.jpg)\n\n<!-- more -->\n\n### ExecutorService的方法\n\n#### execute(Runnable)\n\n这个方法接收一个Runnable实例，并且异步的执行，例子如下：\n\n```java\nExecutorService executorService = Executors.newSingleThreadExecutor();\n\n//java 7\nexecutorService.execute(new Runnable() {\npublic void run() {\n    System.out.println(\"Asynchronous task\");\n}\n});\n\n//java 8 lambda\nexecutorService.execute(() -> {\n  System.out.println(\"Asynchronous task\");\n});\n\nexecutorService.shutdown();\n```\n\n这个方法有个问题，就是没有办法获知task的执行结果。如果我们想获得task的执行结果，我们可以传入一个Callable的实例。\n\n#### submit(Runnable)\n\nsubmit(Runnable)可以返回一个Future对象，通过返回的Future对象，我们可以检查提交的任务是否执行完毕，例子如下：\n\n```java\n//java 7\nFuture future = executorService.submit(new Runnable() {\npublic void run() {\n    System.out.println(\"Asynchronous task\");\n}\n});\n\n//java 8 lambda\nFuture future = executorService.execute(() -> {\n  System.out.println(\"Asynchronous task\");\n});\n\nfuture.get();  //returns null if the task has finished correctly.\n```\n\n如果任务执行完成，future.get()方法会返回一个null。注意，future.get()方法会产生阻塞。\n\n#### submit(Callable)\n\nsubmit(Callable)和submit(Runnable)类似，也会返回一个Future对象，但是除此之外，submit(Callable)接收的是一个Callable的实现，Callable接口中的call()方法有一个返回值，可以返回任务的执行结果，而Runnable接口中的run()方法是void的，没有返回值。例子如下：\n\n```java\n//java 7\nFuture future = executorService.submit(new Callable(){\npublic Object call() throws Exception {\n    System.out.println(\"Asynchronous Callable\");\n    return \"Callable Result\";\n}\n});\n\n//java 8 lambda\nFuture<String> future = executorService.submit(() -> {\n  System.out.println(\"Asynchronous Callable\");\n  return \"Callable Result\";\n});\n\n\nSystem.out.println(\"future done? \" + future.isDone());\n\nString result = future.get();\n\nSystem.out.println(\"future done? \" + future.isDone());\nSystem.out.print(\"result: \" + result);\n```\n\n如果任务执行完成，future.get()方法会返回Callable任务的执行结果。注意，future.get()方法会产生阻塞。\n\n#### Timeouts\n\n任何future.get()调用都会阻塞，然后等待直到callable中止。在最糟糕的情况下，一个callable持续运行——因此使你的程序将没有响应。我们可以简单的传入一个时长来避免这种情况。\n\n```java\nExecutorService executor = Executors.newFixedThreadPool(1);\n\nFuture<Integer> future = executor.submit(() -> {\n  try {\n    TimeUnit.SECONDS.sleep(2);\n    return 123;\n  } catch (InterruptedException e) {\n    throw new IllegalStateException(\"task interrupted\", e);\n  }\n});\n\nfuture.get(1, TimeUnit.SECONDS);\n```\n\n运行上面的代码将会产生一个TimeoutException：\n\n```java\nException in thread \"main\" java.util.concurrent.TimeoutException\n    at java.util.concurrent.FutureTask.get(FutureTask.java:205)\n```\n\n我们指定的最长等待时间为1分钟，而这个callable在返回结果之前实际需要两分钟。\n\n#### invokeAny(…)\n\ninvokeAny(...)方法接收的是一个Callable的集合，执行这个方法不会返回Future，但是会返回所有Callable任务中任意一个成功的结果(如果没有异常发生)，如果有正常或者异常的返回，那么那些没有完成的任务将会取消。例子如下：\n\n```java\nExecutorService executorService = Executors.newSingleThreadExecutor();\n\n//java 7\nSet<Callable<String>> callables = new HashSet<Callable<String>>();\n\ncallables.add(new Callable<String>() {\npublic String call() throws Exception {\n    return \"Task 1\";\n}\n});\ncallables.add(new Callable<String>() {\npublic String call() throws Exception {\n    return \"Task 2\";\n}\n});\ncallables.add(new Callable<String>() {\n    public String call() throws Exception {\n    return \"Task 3\";\n}\n});\n\n//java 8\nList<Callable<String>> callables = Arrays.asList(\n        () -> \"task2\",\n        () -> \"task1\",\n        () -> \"task3\"\n);\n\nString result = executorService.invokeAny(callables);\nSystem.out.println(\"result = \" + result);\nexecutorService.shutdown();\n```\n\n执行上面代码，每次执行都会返回一个结果，并且返回的结果是变化的。\n\n#### invokeAll(…)\n\ninvokeAll(...)与 invokeAny(...)类似也是接收一个Callable集合，但是前者执行之后会返回一个Future的List，其中对应着每个Callable任务执行后的Future对象。例子如下：\n\n```java\nExecutorService executorService = Executors.newSingleThreadExecutor();\n\nSet<Callable<String>> callables = new HashSet<Callable<String>>();\n\ncallables.add(new Callable<String>() {\npublic String call() throws Exception {\n    return \"Task 1\";\n}\n});\ncallables.add(new Callable<String>() {\n    public String call() throws Exception {\n    return \"Task 2\";\n}\n});\ncallables.add(new Callable<String>() {\npublic String call() throws Exception {\n    return \"Task 3\";\n}\n});\n\nList<Future<String>> futures = executorService.invokeAll(callables);\n\nfor(Future<String> future : futures){\nSystem.out.println(\"future.get = \" + future.get());\n}\n\nexecutorService.shutdown();\n```\n\n### ExecutorService的关闭\n\n当我们使用完成ExecutorService之后应该关闭它，否则它里面的线程会一直处于运行状态。\n\n举个例子，如果的应用程序是通过main()方法启动的，在这个main()退出之后，如果应用程序中的ExecutorService没有关闭，这个应用将一直运行。之所以会出现这种情况，是因为ExecutorService中运行的线程会阻止JVM关闭。\n\n如果要关闭ExecutorService中执行的线程，我们可以调用 `ExecutorService.shutdown()` 方法。在调用shutdown()方法之后，ExecutorService不会立即关闭，但是它不再接收新的任务，直到当前所有线程执行完成才会关闭，所有在shutdown()执行之前提交的任务都会被执行。\n\n如果我们想立即关闭ExecutorService，我们可以调用 `ExecutorService.shutdownNow()方法` 。这个动作将跳过所有正在执行的任务和被提交还没有执行的任务。但是它并不对正在执行的任务做任何保证，有可能它们都会停止，也有可能执行完成。\n\n通常关闭executors的方式：\n\n```java\ntry {\n  System.out.println(\"attempt to shutdown executor\");\n  executor.shutdown();\n  executor.awaitTermination(5, TimeUnit.SECONDS);\n  } catch (InterruptedException e) {\n  System.err.println(\"tasks interrupted\");\n  } finally {\n  if (!executor.isTerminated()) {\n      System.err.println(\"cancel non-finished tasks\");\n  }\n  executor.shutdownNow();\n  System.out.println(\"shutdown finished\");\n}\n```\n\nexecutor通过等待指定的时间让当前执行的任务终止来“温柔的”关闭executor。在等待最长N分钟的时间后，execuote最终会通过中断所有的正在执行的任务关闭。\n\n## Executors\n\nExecutors的工厂方法提供的5种不同的线程池。Executors只是一个工厂类，它所有的方法返回的都是ThreadPoolExecutor、ScheduledThreadPoolExecutor这两个类的实例。\n\n重载后的版本，需要多传入实现了ThreadFactory接口的对象。\n\n``` markdown\n1. newFixedThreadPool(int nThreads)\n\n创建可重用固定大小(nThreads,大小不能超过int的最大值)的线程池，以共享的无界队列方式来运行这些线程，缓冲任务的队列为LinkedBlockingQueue，大小为整型的最大数。\n\n当使用此线程池时，在同执行的任务数量超过传入的线程池大小值后，将会放入LinkedBlockingQueue，在LinkedBlockingQueue中的任务需要等待线程空闲后再执行，如果放入LinkedBlockingQueue中的任务超过整型的最大数时，抛出RejectedExecutionException。\n\n2. newCachedThreadPool()\n\n缓存线程池大小是不定值，可以需要创建不同数量的线程。\n\n在使用缓存型池时，先查看池中有没有空闲的线程，如果有，就复用。如果没有，就新建新的线程加入池中。\n\n缓存型池子通常用于执行一些生存期很短的异步型任务。\n\n3. newSingleThreadExecutor()\n\n创建大小为1的固定线程池，同时执行任务(task)的只有一个,其它的任务(task)都放在LinkedBlockingQueue中排队等待执行。\n\n4. newScheduledThreadPool(int corePoolSize)\n\n该方法返回一个可以控制线程池内线程延时或周期性执行某任务的线程池。\n\n5. newSingleThreadScheduledExecutor()\n\n该方法返回一个可以控制线程池内线程定时或周期性执行某任务的线程池。只不过和上面的区别是该线程池大小为1，而上面的可以指定线程池的大小。\n```\n\n## ThreadPoolExecutor\n\n先来通过ThreadPoolExecutor的构造方法了解一下这个类:\n\n```java\npublic ThreadPoolExecutor(int corePoolSize,\n                             int maximumPoolSize,\n                             long keepAliveTime,\n                             TimeUnit unit,\n                             BlockingQueue<Runnable> workQueue,\n                             ThreadFactory threadFactory,\n                             RejectedExecutionHandler handler)\n```\n\n主要参数有:\n\ncorePoolSize 核心线程的数量，不超过这个参数数量的线程会被保留在线程池内，即使它们是空闲的，如果设置了allowCoreThreadTimeOut为true除外。\n\nmaximumPoolSize 线程池所允许拥有线程的最大数量，当任务队列的任务已满，线程数已达到最大数量，任务会被拒绝。\n\nkeepAliveTime 当线程池的线程数量超过核心线程的数量，这些非核心线程会尝试在keepAliveTime内获取队列内的任务，如果获取失败则被线程池移除并终止。\n\nunit 超时时间的单位。\n\nworkQueue 任务的阻塞队列，缓存将要执行的Runnable任务，由各线程轮询该任务队列获取任务执行。\n\nthreadFactory 线程创建的工厂。\n\nhandler 当任务由于线程数量或者任务队列达到上限，会执行该接口的方法处理任务的拒绝。\n\n### ThreadPoolExecutor的状态变量\n\n```java\n    private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));\n    private static final int COUNT_BITS = Integer.SIZE - 3;\n    private static final int CAPACITY   = (1 << COUNT_BITS) - 1;\n\n    // runState is stored in the high-order bits\n    private static final int RUNNING    = -1 << COUNT_BITS;\n    private static final int SHUTDOWN   =  0 << COUNT_BITS;\n    private static final int STOP       =  1 << COUNT_BITS;\n    private static final int TIDYING    =  2 << COUNT_BITS;\n    private static final int TERMINATED =  3 << COUNT_BITS;\n\n    // Packing and unpacking ctl\n    private static int runStateOf(int c)     { return c & ~CAPACITY; }\n    private static int workerCountOf(int c)  { return c & CAPACITY; }\n    private static int ctlOf(int rs, int wc) { return rs | wc; }\n```\n\nctl是ThreadPoolExecutor的同步状态变量。\n\nworkerCountOf()方法取得当前线程池的线程数量，算法是将ctl的值取低29位。\n\nrunStateOf()方法取得线程池的状态，算法是将ctl的值取高3位:\n\n1. RUNNING 111 表示正在运行\n2. SHUTDOWN 000 表示拒绝接收新的任务\n3. STOP 001 表示拒绝接收新的任务并且不再处理任务队列中剩余的任务，并且中断正在执行的任务。\n4. TIDYING 010 表示所有线程已停止，准备执行terminated()方法。\n5. TERMINATED 011 表示已执行完terminated()方法。\n\n### Executor.execute(Runnable command)\n\n该方法将使用线程池执行Runnable对象的run()方法:\n\n```java\npublic void execute(Runnable command) {\n    if (command == null)\n        throw new NullPointerException();\n    /*\n     * Proceed in 3 steps:\n     *\n     * 1. If fewer than corePoolSize threads are running, try to\n     * start a new thread with the given command as its first\n     * task.  The call to addWorker atomically checks runState and  * workerCount, and so prevents false alarms that would add\n     * threads when it shouldn't, by returning false.\n     *\n     * 2. If a task can be successfully queued, then we still\n     * to double-check whether we should have added a thread\n     * (because existing ones died since last checking) or that\n     * the pool shut down since entry into this method. So we\n     * recheck state and if necessary roll back the enqueuing if\n     * stopped, or start a new thread if there are none.\n     *\n     * 3. If we cannot queue task, then we try to add a new\n     * thread.  If it fails, we know we are shut down or rated\n     * and so reject the task.\n     */\n    int c = ctl.get();\n    if (workerCountOf(c) < corePoolSize) {\n        if (addWorker(command, true))\n            return;\n        c = ctl.get();\n    }\n    if (isRunning(c) && workQueue.offer(command)) {\n        int recheck = ctl.get();\n        if (! isRunning(recheck) && remove(command))\n            reject(command);\n        else if (workerCountOf(recheck) == 0)\n            addWorker(null, false);\n    }\n    else if (!addWorker(command, false))\n        reject(command);\n}\n```\n\n以上代码对应了三种情况:\n\n1. 线程池的线程数量小于核心线程数量上限，开启核心线程执行任务。\n2. 线程池的线程数量不小于核心线程数量上限，或者开启核心线程失败，尝试将任务以非阻塞的方式添加到任务队列。\n3. 任务队列以满导致添加任务失败，开启新的非核心线程执行任务。\n\n回顾FixedThreadPool，因为它配置的corePoolSize与maximumPoolSize相等，所以不会执行到情况3，并且因为workQueue为默认的LinkedBlockingQueue，其长度为Integer.MAX_VALUE，几乎不可能出现任务无法被添加到workQueue的情况，所以FixedThreadPool的所有任务执行在核心线程中。\n\n而CachedThreadPool的corePoolSize为0，表示它不会执行到情况1，因为它的maximumPoolSize为Integer.MAX_VALUE，所以几乎没有线程数量上限，因为它的workQueue为SynchronousQueue，所以当线程池里没有闲置的线程SynchronousQueue就会添加任务失败，因此会执行到情况3添加新的线程执行任务。\n\n### addWorker方法\n\n```java\nprivate boolean addWorker(Runnable firstTask, boolean core) {\n    retry:\n    for (;;) {\n        int c = ctl.get();\n        int rs = runStateOf(c);\n        // Check if queue empty only if necessary.\n        if (rs >= SHUTDOWN &&\n            ! (rs == SHUTDOWN &&\n               firstTask == null &&\n               ! workQueue.isEmpty()))\n            return false;\n        for (;;) {\n            int wc = workerCountOf(c);\n            if (wc >= CAPACITY ||\n                wc >= (core ? corePoolSize : maximumPoolSize))\n                return false;\n            if (compareAndIncrementWorkerCount(c))\n                break retry;\n            c = ctl.get();  // Re-read ctl\n            if (runStateOf(c) != rs)\n                continue retry;\n            // else CAS failed due to workerCount change; retry inner loop\n        }\n    }\n    boolean workerStarted = false;\n    boolean workerAdded = false;\n    Worker w = null;\n    try {\n        w = new Worker(firstTask);\n        final Thread t = w.thread;\n        if (t != null) {\n            final ReentrantLock mainLock = this.mainLock;\n            mainLock.lock();\n            try {\n                // Recheck while holding lock.\n                // Back out on ThreadFactory failure or if\n                // shut down before lock acquired.\n                int rs = runStateOf(ctl.get());\n                if (rs < SHUTDOWN ||\n                    (rs == SHUTDOWN && firstTask == null)) {\n                    if (t.isAlive()) // precheck that t is table\n                        throw new IllegalThreadStateException();\n                    workers.add(w);\n                    int s = workers.size();\n                    if (s > largestPoolSize)\n                        largestPoolSize = s;\n                    workerAdded = true;\n                }\n            } finally {\n                mainLock.unlock();\n            }\n            if (workerAdded) {\n                t.start();\n                workerStarted = true;\n            }\n        }\n    } finally {\n        if (! workerStarted)\n            addWorkerFailed(w);\n    }\n    return workerStarted;\n}\n```\n\naddWorker这个方法先尝试在线程池运行状态为RUNNING并且线程数量未达上限的情况下通过CAS操作将线程池数量+1,接着在ReentrantLock同步锁的同步保证下判断线程池为运行状态，然后把Worker添加到HashSet workers中。如果添加成功则执行Worker的内部线程。\n\n### Worker构造方法\n\n```java\nWorker(Runnable firstTask) {\n    setState(-1); // inhibit interrupts until runWorker\n    this.firstTask = firstTask;\n    this.thread = getThreadFactory().newThread(this);\n}\n```\n\n这里指定了第一个要执行的任务，并通过线程池的线程工厂创建线程。可以发现这个线程的参数为this，即Worker对象，因为Worker实现了Runnable因此可以被当成任务执行，执行的即Worker实现的run方法:\n\n```java\npublic void run() {\n    runWorker(this);\n}\n```\n\n### runWorker方法\n\n因为Worker为ThreadPoolExecutor的内部类，因此runWorker方法实际是ThreadPoolExecutor定义的:\n\n```java\nfinal void runWorker(Worker w) {\n    Thread wt = Thread.currentThread();\n    Runnable task = w.firstTask;\n    w.firstTask = null;\n    w.unlock(); // allow interrupts\n    boolean completedAbruptly = true;\n    try {\n        while (task != null || (task = getTask()) != null) {\n            w.lock();\n            // If pool is stopping, ensure thread is interrupted;\n            // if not, ensure thread is not interrupted.  This\n            // requires a recheck in second case to deal with\n            // shutdownNow race while clearing interrupt\n            if ((runStateAtLeast(ctl.get(), STOP) ||\n                 (Thread.interrupted() &&\n                  runStateAtLeast(ctl.get(), STOP))) &&\n                !wt.isInterrupted())\n                wt.interrupt();\n            try {\n                beforeExecute(wt, task);\n                Throwable thrown = null;\n                try {\n                    task.run();\n                } catch (RuntimeException x) {\n                    thrown = x; throw x;\n                } catch (Error x) {\n                    thrown = x; throw x;\n                } catch (Throwable x) {\n                    thrown = x; throw new Error(x);\n                } finally {\n                    afterExecute(task, thrown);\n                }\n            } finally {\n                task = null;\n                w.completedTasks++;\n                w.unlock();\n            }\n        }\n        completedAbruptly = false;\n    } finally {\n        processWorkerExit(w, completedAbruptly);\n    }\n}\n```\n\n这个方法是线程池复用线程的核心代码，注意Worker继承了AbstractQueuedSynchronizer，在执行每个任务前通过lock方法加锁，执行完后通过unlock方法解锁，这种机制用来防止运行中的任务被中断。在执行任务时先尝试获取firstTask，即构造方法传入的Runnable对象，然后尝试从getTask方法中获取任务队列中的任务。在任务执行前还要再次判断线程池是否已经处于STOP状态或者线程被中断。\n\n注意这里w.lock方法是在获取到任务后才执行的，也就是如果线程获取到任务前都未加锁，这样能保证showDown方法尝试获取该锁中断空闲的线程，详见后面的解析。\n\n当线程被中断、抛出异常、不能及时得到任务，processWorkerExit方法用于最后将线程回收。\n\n### getTask方法\n\n```java\nprivate Runnable getTask() {\n    boolean timedOut = false; // Did the last poll() time out?\n\n    for (;;) {\n        int c = ctl.get();\n        int rs = runStateOf(c);\n\n        // Check if queue empty only if necessary.\n        if (rs >= SHUTDOWN && (rs >= STOP || workQueue.isEmpty())) {\n            decrementWorkerCount();\n            return null;\n        }\n\n        int wc = workerCountOf(c);\n\n        // Are workers subject to culling?\n        boolean timed = allowCoreThreadTimeOut || wc > corePoolSize;\n\n        if ((wc > maximumPoolSize || (timed && timedOut))\n            && (wc > 1 || workQueue.isEmpty())) {\n            if (compareAndDecrementWorkerCount(c))\n                return null;\n            continue;\n        }\n\n        try {\n            Runnable r = timed ?\n                workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) :\n                workQueue.take();\n            if (r != null)\n                return r;\n            timedOut = true;\n        } catch (InterruptedException retry) {\n            timedOut = false;\n        }\n    }\n}\n```\n\n还记得Executor.execute方法的情况是将任务添加到任务队列，getTask方法就是从任务队列中同步地取出任务。\n\n这个方法通过一个循环不断轮询任务队列有没有任务到来，首先判断线程池是否处于正常运行状态，通过超时配置有两种方法取出任务：\n\n1. BlockingQueue.poll 阻塞指定的时间尝试获取任务，如果超过指定的时间还未获取到任务就返回null。\n2. BlockingQueue.take 这种方法会在取到任务前一直阻塞。\n\nFixedThreadPool使用的是take方法，所以会线程会一直阻塞等待任务。CachedThreadPool使用的是poll方法，也就是说CachedThreadPool中的线程如果在60秒内未获取到队列中的任务就会被终止。\n\n到此ThreadPoolExecutor是怎么执行Runnable任务的分析结束。\n\n### ExecutorService.shutdown()\n\n```java\npublic void shutdown() {\n    final ReentrantLock mainLock = this.mainLock;\n    mainLock.lock();\n    try {\n        checkShutdownAccess();\n        advanceRunState(SHUTDOWN);\n        interruptIdleWorkers();\n        onShutdown(); // hook for ScheduledThreadPoolExecutor\n    } finally {\n        mainLock.unlock();\n    }\n    tryTerminate();\n}\n\nprivate void interruptIdleWorkers(boolean onlyOne) {\n    final ReentrantLock mainLock = this.mainLock;\n    mainLock.lock();\n    try {\n        for (Worker w : workers) {\n            Thread t = w.thread;\n            if (!t.isInterrupted() && w.tryLock()) {\n                try {\n                    t.interrupt();\n                } catch (SecurityException ignore) {\n                } finally {\n                    w.unlock();\n                }\n            }\n            if (onlyOne)\n                break;\n        }\n    } finally {\n        mainLock.unlock();\n    }\n}\n```\n\nExecutorService是Executor的子类，也是ThreadPoolExecutor的基类。首先通过mainLock加锁同步修改线程池状态为SHUTDOWN，然后通过interruptIdleWorkers方法中断空闲线程，OnShowDown方法是留给子类去实现的。\n\ninterruptIdleWorkers(boolean onlyOne)方法也是先用mainLock加锁同步，然后循环找出所有Worker中Thread未中断的，通过tryLock方法尝试获取锁。还记得上文的runWorker方法Worker的锁是在获取任务时才加的，interruptIdleWorkers方法通过竞争该锁抢先中断线程，这样就导致未执行任务的线程被中断了，而正在执行任务的线程不受影响，并且可以继续执行任务队列中的任务。\n\n### ExecutorService.shutdownNow()\n\n与ExecutorService.shutdown()不同的是，shutdownNow方法除了让线程池拒绝接收新的任务，并且不再执行任务队列里未执行的任务。\n\n```java\npublic List<Runnable> shutdownNow() {\n    List<Runnable> tasks;\n    final ReentrantLock mainLock = this.mainLock;\n    mainLock.lock();\n    try {\n        checkShutdownAccess();\n        advanceRunState(STOP);\n        interruptWorkers();\n        tasks = drainQueue();\n    } finally {\n        mainLock.unlock();\n    }\n    tryTerminate();\n    return tasks;\n}\n```\n\n首先mainLock同步将状态改为STOP,然后中断所有线程。\n\n```java\nprivate void interruptWorkers() {\n    final ReentrantLock mainLock = this.mainLock;\n    mainLock.lock();\n    try {\n        for (Worker w : workers)\n            w.interruptIfStarted();\n    } finally {\n        mainLock.unlock();\n    }\n}\n\n```\n\ninterruptWorkers方法将对所有Worker执行interruptIfStarted，即将所有运行中的线程中断:\n\n```java\nvoid interruptIfStarted() {\n    Thread t;\n    if (getState() >= 0 && (t = thread) != null && !t.isInterrupted()) {\n        try {\n            t.interrupt();\n        } catch (SecurityException ignore) {\n        }\n    }\n}\n```\n\n还记得Worker的构造函数中执行了setState(-1)，而在runWorker方法中通过unlock将state改为0，因此可以被interruptWorkers方法中断。\n\n这里注意的是中断并不意味着线程就一定停止工作，除非在任务中捕获InterruptedException退出任务。\n\n## ScheduledThreadPoolExecutor\n\nScheduledThreadPoolExecutor同ThreadPoolExecutor一样也可以从 Executors线程池工厂创建，所不同的是它具有定时执行，以周期或间隔循环执行任务等功能。\n\nScheduledThreadPoolExecutor继承自ThreadPoolExecutor，因此它具有ThreadPoolExecutor的所有能力。\n通过super方法的参数可知，核心线程的数量即传入的参数，而线程池的线程数为Integer.MAX_VALUE，几乎为无上限。\n这里采用了DelayedWorkQueue任务队列，也是定时任务的核心，留在后面分析。\n\nScheduledThreadPoolExecutor实现了ScheduledExecutorService 中的接口:\n\n```java\npublic <V> ScheduledFuture<V> schedule(Callable<V> callable, long delay, TimeUnit unit);\n```\n\n延时执行Callable任务的功能。\n\n```java\npublic ScheduledFuture<?> schedule(Runnable command, long delay, TimeUnit unit);\n```\n\n延时执行Runnable任务的功能。\n\n```java\npublic ScheduledFuture<?> scheduleAtFixedRate(Runnable command,\n                                                long initialDelay,\n                                                long period,\n                                                TimeUnit unit);\n```\n\n可以延时循环执行周期性任务。\n\n假设任务执行时间固定2s,period为1s，因为任务的执行时间大于规定的period，所以任务会每隔2s（任务执行时间）开始执行一次。如果任务执行时间固定为0.5s,period为1s，因为任务执行时间小于period，所以任务会每隔1s（period）开始执行一次。实际任务的执行时间即可能是大于period的，也可能小于period，scheduleAtFixedRate的好处就是每次任务的开始时间间隔必然大于等于period。\n\n### 四种执行定时任务的方法\n\n```java\nschedule(Runnable command,long delay, TimeUnit unit)\n\nschedule(Callable<V> callable, long delay, TimeUnit unit)\n\nscheduleAtFixedRate(Runnable command,long initialDelay,long period,TimeUnit unit)\n\nscheduleWithFixedDelay(Runnable command,long initialDelay,long delay,TimeUnit unit)\n```\n","tags":["Concurrency"]},{"title":"Java NIO详解","url":"%2F2018%2F03%2F16%2FNIO%2F","content":"\n## 概述\n\nBIO是面向字节流和字符流的，数据从流中顺序获取\n\nNIO是面向通道和缓冲区的，数据总是从通道中读到buffer缓冲区内，或者从buffer缓冲区内写入通道\n\nChannel通道和Buffer缓冲区是NIO的核心，几乎在每一个IO操作中使用它们，Selector选择器则允许单个线程操作多个通道，对于高并发多连接很有帮助。\n\n> 操作系统的IO一般分为两个阶段，等待和就绪操作，比如读可以分为等待系统可读和真正的读，写可以分为等待系统可写和真正的写，在传统的BIO中是这两个阶段都会阻塞，在NIO中第一个阶段不是阻塞的，第二个阶段是阻塞的，如下图，BIO是阻塞IO，NIO是非阻塞IO\n\n<!-- more -->\n\n## Buffer（缓冲区）\n\n### 常用Buffer类型\n\nByteBuffer, MappedByteBuffer, CharBuffer, DoubleBuffer, FloatBuffer, IntBuffer, LongBuffer, ShortBuffer。对应了几大基本数据类型。\n\n> ByteBuffer可以选择实例化为DirectByteBuffer和HeapByteBuffer，如果数据量比较小的中小应用情况下，可以考虑使用heapBuffer；反之可以用directBuffer。一般来说DirectByteBuffer可以减少一次系统空间到用户空间的拷贝。但Buffer创建和销毁的成本更高，更不宜维护，通常会用内存池来提高性能。其他buffer类似。\n\n### 利用Buffer读写数据步骤\n\n1. 将数据写入buffer\n2. 调用flip将写模式改为读模式\n3. 从buffer中读取数据，进行操作\n4. 调用clear()清除整个buffer数据或者调用compact()清空已读数据\n\n### buffer的属性\n\n- capacity容量，该buffer最多存储的字节数\n- position位置，写模式下，position从0到capacity-1，变更为读模式后，position归零，边读边移动\n- limit限制，写模式下，代表我们能写的最大量为capacity，读模式下，变更为原position位置，即有数据的位置\n\n![buffer_modes](/images/nio/buffer_modes.png)\n\n### buffer api\n\n- 通过allocate()方法为buffer分配内存大小，如开辟一个48字节的ByteBuffer buffer：ByteBuffer.allocate(48)\n- 写数据可以通过通道写，如：FileChannel.read(buffer)；也可以通过put方法来写数据，如：buffer.put(127)\n- flip()翻转方法，将写模式切换到读模式，position归零，设置limit为之前的position位置\n- 读数据可以读到通道，如：FileChannel.write(buffer)；也可以调用get()方法读取，byte aByte=buffer.get()\n- buffer.rewind()将position置0，limit不变，这样我们就可以重复读取数据啦\n- buffer.clear()将position置为0，limit设置为capacity，这里并没有删除buffer里面的数据，只是把标记位置改了；\n- buffer.compact()清除已读数据，这里也没有删除数据，将position设置为未读的个数，将后面几个未读的字节顺序的复制到前面的几个字节，limit设置为capacity，比如buffer容量3个字节，读取hello，在读取了2个字节后我就调用了compact()方法，那么此时position为1，limit为3，buffer内部存储的数据buff[0]='l',buff[1]='e',buff[2]='l'，因为有一个'l'没有读完，将'l'提取到最前面供下次读取\n- mark()可以标记当前的position位置，通过reset来恢复mark位置，可以用来实现重复读取满足条件的数据块\n- equals()两个buffer相等需满足，类型相同，buffer剩余（未读）字节数相同，所有剩余字节数相同\n- compareTo()比较buffer中的剩余元素，只不过此方法适合排序\n\n## Channel（通道）\n\n### Channel的重要实现\n\n- FileChannel用于文件数据的读写，transferTo()方法可以将通道的数据传送至另外一个通道，完成数据的复制\n- DatagramChannel用于UDP数据的读写\n- SocketChannel用于TCP的数据读写，通常我们所说的客户端套接字通道\n- ServerSocketChannel允许我们监听TCP链接请求，通常我们所说的服务端套接字通道，每一个请求都会创建一个SocketChannel\n\n### Scatter和Gather\n\nJava nio在Channel实现类也实现了Scatter和Gather相关类\n\n- Scatter.read()是从通道读取的操作能把数据写入多个buffer，即一个通道向多个buffer写数据的过程，但是read必须写满一个buffer后才会向后移动到下一个buffer，因此read不适合大小会动态改变的数据。代码如下：\n\n```java\nByteBuffer header = ByteBuffer.allocate(128);\nByteBuffer body = ByteBuffer.allocate(1024);\nByteBuffer[] bufferArray = { header, body };\n```\n\nchannel.read(bufferArray);\nGather.write()是从可以把多个buffer的数据写入通道，write是只会写position到limit之间的数据，因此写是可以适应大小动态改变的数据。代码如下：\n\n```java\nByteBuffer header = ByteBuffer.allocate(128);\nByteBuffer body = ByteBuffer.allocate(1024);\n//write data into buffers\nByteBuffer[] bufferArray = { header, body };\nchannel.write(bufferArray);\n```\n\n在有些场景会非常有用，比如处理多份需要分开传输的数据，举例来说，假设一个消息包含了header和body，我们可能会把header和body分别放在不同的buffer\n\n## Selector（选择器）\n\nSelector用于检查一个或多个NIO Channel的状态是否可读可写，这样就可以实现单线程管理多个Channels，也就是可以管理多个网络连接，NIO非阻塞主要就是通过Selector注册事件监听，监听通道将数据就绪后，就进行实际的读写操作，因此前面说的IO两个阶段，一阶段NIO仅仅是异步监听，二阶段就是同步实际操作数据。\n\n### Selector监听事件类别\n\n- SelectionKey.OP_CONNECT是Channel和server连接成功后，连接就绪\n- SelectionKey.OP_ACCEPT是server Channel接收请求连接就绪\n- SelectionKey.OP_READ是Channel有数据可读时，处于读就绪\n- SelectionKey.OP_WRITE是Channel可以进行数据写入是，写就绪\n\n### 使用Selector的步骤\n\n- 创建一个Selector，Selector selector = Selector.open();\n- 注册Channel到Selector上面，将Channel切换为非阻塞的：channel.configureBlocking(false)，然后绑定Selector：SelectionKey key = channel.register(selector, SelectionKey.OP_READ);\n- SelectionKey.OP_READ为监听的事件类别，如果要监听多个事件，可利用位的或运算结合多个常量如：int interestSet = SelectionKey.OP_READ | SelectionKey.OP_WRITE;\n\n代码如下：\n\n```java\nSelector selector = Selector.open();\nSocketChannel clientChannel = SocketChannel.open();\nclientChannel.configureBlocking(false);\nclientChannel.connect(new InetSocketAddress(port));\nclientChannel.register(selector, SelectionKey.OP_CONNECT);\n```\n\n> 需要使用Selector的Channel必须是非阻塞的，FileChannel不能切换为非租塞的，因此FileChannel不适用于Selector\n\n### API\n\n- Selector.select()方法是阻塞的，因此可以放心的将它写在while(true)中，不用担心cpu会空转\n- Selector.wakeup()唤醒select()造成的阻塞，可能是有新的事件注册，优先级更高的事件触发（如定时器事件），希望及时处理。其原理是向通道或者连接中写入一个字节，阻塞的select因为有IO事件就绪，立即返回\n","tags":["Java"]},{"title":"ConcurrentHashMap 从 Java 7 到 Java 8","url":"%2F2018%2F03%2F13%2Fconcurrenthashmap%2F","content":"\n## Happens-before 关系\n\nhappens-before 关系保证：如果线程 A 与线程 B 满足 happens-before 关系，则线程 A 执行动作的结果对于线程 B 是可见的。如果两个操作未按 happens-before 排序，JVM 将可以对他们任意重排序。\n\n下面介绍几个与理解 ConcurrentHashMap 有关的 happens-before 关系法则：\n\n1. 程序次序法则：如果在程序中，所有动作 A 出现在动作 B 之前，则线程中的每动作 A 都 happens-before 于该线程中的每一个动作 B。\n2. 监视器锁法则：对一个监视器的解锁 happens-before 于每个后续对同一监视器的加锁。\n3. Volatile 变量法则：对 Volatile 域的写入操作 happens-before 于每个后续对同一 Volatile 的读操作。\n4. 传递性：如果 A happens-before 于 B，且 B happens-before C，则 A happens-before C。\n\n## Java 7基于分段锁的ConcurrentHashMap\n\n### 数据结构\n\nJava 7中的ConcurrentHashMap 类中包含两个静态内部类 HashEntry 和 Segment。HashEntry 用来封装映射表的键 / 值对；Segment 用来充当锁的角色，每个 Segment 对象守护整个散列映射表的若干个桶。每个桶是由若干个 HashEntry 对象链接起来的链表。一个 ConcurrentHashMap 实例中包含由若干个 Segment 对象组成的数组。\n\n1. 最大的分段（segment）数为2的16次方，每一个segment的HashEntry[]的最大容量为2的30次方。\n2. 默认的分段数和每个segment的HashEntry[]的初始容量均为16。segment的默认加载因子为0.75。\n3. 定位segment段需要用的两个参数：segmentMask，segmentShift。\n\n整体数据结构如下图所示：\n\n![java7数据结构](/images/concurrenthashmap/java7数据结构.png)\n\n<!-- more -->\n\n#### HashEntry 类\n\nHashEntry 用来封装散列映射表中的键值对。在 HashEntry 类中，key，hash 和 next 域都被声明为 final 型，value 域被声明为 volatile 型。\n\n```java\n//HashEntry 类的定义\nstatic final class HashEntry<K,V> {\n       final K key;                       // 声明 key 为 final 型\n       final int hash;                   // 声明 hash 值为 final 型\n       volatile V value;                 // 声明 value 为 volatile 型\n       final HashEntry<K,V> next;      // 声明 next 为 final 型\n\n       HashEntry(K key, int hash, HashEntry<K,V> next, V value) {\n           this.key = key;\n           this.hash = hash;\n           this.next = next;\n           this.value = value;\n       }\n}\n```\n\n在 ConcurrentHashMap 中，在散列时如果产生“碰撞”，将采用“分离链接法”来处理“碰撞”：把“碰撞”的 HashEntry 对象链接成一个链表。由于 HashEntry 的 next 域为 final 型，所以新节点只能在链表的表头处插入。 下图是在一个空桶中依次插入 A，B，C 三个 HashEntry 对象后的结构图：\n\n![插入三个节点后桶的结构示意图](/images/concurrenthashmap/插入三个节点后桶的结构示意图.jpg)\n\n> 由于只能在表头插入，所以链表中节点的顺序和插入的顺序相反。\n\n#### Segment 类\n\nSegment 类继承于 ReentrantLock 类，从而使得 Segment 对象能充当锁的角色。每个 Segment 对象用来守护其（成员对象 table 中）包含的若干个桶。\n\ntable 是一个由 HashEntry 对象组成的数组。table 数组的每一个数组成员就是散列映射表的一个桶。\n\ncount 变量是一个计数器，它表示每个 Segment 对象管理的 table 数组（若干个 HashEntry 组成的链表）包含的 HashEntry 对象的个数。每一个 Segment 对象都有一个 count 对象来表示本 Segment 中包含的 HashEntry 对象的总数。注意，之所以在每个 Segment 对象中包含一个计数器，而不是在 ConcurrentHashMap 中使用全局的计数器，是为了避免出现“热点域”而影响 ConcurrentHashMap 的并发性。\n\n```java\n//Segment 类的定义\nstatic final class Segment<K,V> extends ReentrantLock implements Serializable {\n       /**\n        * 在本 segment 范围内，包含的 HashEntry 元素的个数\n        * 该变量被声明为 volatile 型\n        */\n       transient volatile int count;\n\n       /**\n        * table 被更新的次数\n        */\n       transient int modCount;\n\n       /**\n        * 当 table 中包含的 HashEntry 元素的个数超过本变量值时，触发 table 的再散列\n        */\n       transient int threshold;\n\n       /**\n        * table 是由 HashEntry 对象组成的数组\n        * 如果散列时发生碰撞，碰撞的 HashEntry 对象就以链表的形式链接成一个链表\n        * table 数组的数组成员代表散列映射表的一个桶\n        * 每个 table 守护整个 ConcurrentHashMap 包含桶总数的一部分\n        * 如果并发级别为 16，table 则守护 ConcurrentHashMap 包含的桶总数的 1/16\n        */\n       transient volatile HashEntry<K,V>[] table;\n\n       /**\n        * 装载因子\n        */\n       final float loadFactor;\n\n       Segment(int initialCapacity, float lf) {\n           loadFactor = lf;\n           setTable(HashEntry.<K,V>newArray(initialCapacity));\n       }\n\n       /**\n        * 设置 table 引用到这个新生成的 HashEntry 数组\n        * 只能在持有锁或构造函数中调用本方法\n        */\n       void setTable(HashEntry<K,V>[] newTable) {\n           // 计算临界阀值为新数组的长度与装载因子的乘积\n           threshold = (int)(newTable.length * loadFactor);\n           table = newTable;\n       }\n\n       /**\n        * 根据 key 的散列值，找到 table 中对应的那个桶（table 数组的某个数组成员）\n        */\n       HashEntry<K,V> getFirst(int hash) {\n           HashEntry<K,V>[] tab = table;\n           // 把散列值与 table 数组长度减 1 的值相“与”，\n// 得到散列值对应的 table 数组的下标\n           // 然后返回 table 数组中此下标对应的 HashEntry 元素\n           return tab[hash & (tab.length - 1)];\n       }\n}\n```\n\n下图是依次插入 ABC 三个 HashEntry 节点后，Segment 的结构示意图。\n\n![插入三个节点后Segment的结构示意图](/images/concurrenthashmap/插入三个节点后Segment的结构示意图.jpg)\n\n#### ConcurrentHashMap 类\n\nConcurrentHashMap 在默认并发级别会创建包含 16 个 Segment 对象的数组。每个 Segment 的成员对象 table 包含若干个散列表的桶。每个桶是由 HashEntry 链接起来的一个链表。如果键能均匀散列，每个 Segment 大约守护整个散列表中桶总数的 1/16。\n\n```java\n//ConcurrentHashMap 类的定义\npublic class ConcurrentHashMap<K, V> extends AbstractMap<K, V>\n       implements ConcurrentMap<K, V>, Serializable {\n\n   /**\n    * 散列映射表的默认初始容量为 16，即初始默认为 16 个桶\n    * 在构造函数中没有指定这个参数时，使用本参数\n    */\n   static final int DEFAULT_INITIAL_CAPACITY= 16;\n\n   /**\n    * 散列映射表的默认装载因子为 0.75，该值是 table 中包含的 HashEntry 元素的个数与\n* table 数组长度的比值\n    * 当 table 中包含的 HashEntry 元素的个数超过了 table 数组的长度与装载因子的乘积时，\n* 将触发 再散列\n    * 在构造函数中没有指定这个参数时，使用本参数\n    */\n   static final float DEFAULT_LOAD_FACTOR= 0.75f;\n\n   /**\n    * 散列表的默认并发级别为 16。该值表示当前更新线程的估计数\n    * 在构造函数中没有指定这个参数时，使用本参数\n    */\n   static final int DEFAULT_CONCURRENCY_LEVEL= 16;\n\n   /**\n    * segments 的掩码值\n    * key 的散列码的高位用来选择具体的 segment\n    */\n   final int segmentMask;\n\n   /**\n    * 偏移量\n    */\n   final int segmentShift;\n\n   /**\n    * 由 Segment 对象组成的数组\n    */\n   final Segment<K,V>[] segments;\n\n   /**\n     * The maximum number of segments to allow; used to bound\n     * constructor arguments.\n   */\n   static final int MAX_SEGMENTS = 1 << 16;\n\n   /**\n    * 创建一个带有指定初始容量、加载因子和并发级别的新的空映射。\n    */\n   public ConcurrentHashMap(int initialCapacity,\n                            float loadFactor, int concurrencyLevel) {\n       if(!(loadFactor > 0) || initialCapacity < 0 ||\nconcurrencyLevel <= 0)\n           throw new IllegalArgumentException();\n\n       if(concurrencyLevel > MAX_SEGMENTS)\n           concurrencyLevel = MAX_SEGMENTS;\n\n       // 寻找最佳匹配参数（不小于给定参数的最接近的 2 次幂）\n       int sshift = 0;\n       int ssize = 1;\n       while(ssize < concurrencyLevel) {\n           ++sshift;\n           ssize <<= 1;\n       }\n       segmentShift = 32 - sshift;       // 偏移量值\n       segmentMask = ssize - 1;           // 掩码值\n       this.segments = Segment.newArray(ssize);   // 创建数组\n\n       if (initialCapacity > MAXIMUM_CAPACITY)\n           initialCapacity = MAXIMUM_CAPACITY;\n       int c = initialCapacity / ssize;\n       if(c * ssize < initialCapacity)\n           ++c;\n       int cap = 1;\n       while(cap < c)\n           cap <<= 1;\n\n       // 依次遍历每个数组元素\n       for(int i = 0; i < this.segments.length; ++i)\n           // 初始化每个数组元素引用的 Segment 对象\nthis.segments[i] = new Segment<K,V>(cap, loadFactor);\n   }\n\n   /**\n    * 创建一个带有默认初始容量 (16)、默认加载因子 (0.75) 和 默认并发级别 (16)\n * 的空散列映射表。\n    */\n   public ConcurrentHashMap() {\n       // 使用三个默认参数，调用上面重载的构造函数来创建空散列映射表\nthis(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR, DEFAULT_CONCURRENCY_LEVEL);\n}\n```\n\n下面是 ConcurrentHashMap 的结构示意图。\n\n![ConcurrentHashMap的结构示意图](/images/concurrenthashmap/ConcurrentHashMap的结构示意图.jpg)\n\n### 寻址方式\n\n在读写某个Key时，先取该Key的哈希值。并将哈希值的高N位对Segment个数取模从而得到该Key应该属于哪个Segment，接着如同操作HashMap一样操作这个Segment。为了保证不同的值均匀分布到不同的Segment，需要通过如下方法计算哈希值。\n\n```java\nprivate int hash(Object k) {\n  int h = hashSeed;\n  if ((0 != h) && (k instanceof String)) {\n    return sun.misc.Hashing.stringHash32((String) k);\n  }\n  h ^= k.hashCode();\n  h += (h <<  15) ^ 0xffffcd7d;\n  h ^= (h >>> 10);\n  h += (h <<   3);\n  h ^= (h >>>  6);\n  h += (h <<   2) + (h << 14);\n  return h ^ (h >>> 16);\n}\n```\n\n同样为了提高取模运算效率，通过如下计算，ssize即为大于concurrencyLevel的最小的2的N次方，同时segmentMask为2^N-1。这一点跟上文中计算数组长度的方法一致。对于某一个Key的哈希值，只需要向右移segmentShift位以取高sshift位，再与segmentMask取与操作即可得到它在Segment数组上的索引。\n\n```java\nint sshift = 0;\nint ssize = 1;\nwhile (ssize < concurrencyLevel) {\n  ++sshift;\n  ssize <<= 1;\n}\nthis.segmentShift = 32 - sshift;\nthis.segmentMask = ssize - 1;\nSegment<K,V>[] ss = (Segment<K,V>[])new Segment[ssize];\n```\n\n### size操作\n\nput、remove和get操作只需要关心一个Segment，而size操作需要遍历所有的Segment才能算出整个Map的大小。一个简单的方案是，先锁住所有Sgment，计算完后再解锁。但这样做，在做size操作时，不仅无法对Map进行写操作，同时也无法进行读操作，不利于对Map的并行操作。\n\n为更好支持并发操作，ConcurrentHashMap会在不上锁的前提逐个Segment计算3次size，如果某相邻两次计算获取的所有Segment的更新次数（每个Segment都与HashMap一样通过modCount跟踪自己的修改次数，Segment每修改一次其modCount加一）相等，说明这两次计算过程中无更新操作，则这两次计算出的总size相等，可直接作为最终结果返回。如果这三次计算过程中Map有更新，则对所有Segment加锁重新计算Size。该计算方法代码如下:\n\n```java\npublic int size() {\n  final Segment<K,V>[] segments = this.segments;\n  int size;\n  boolean overflow; // true if size overflows 32 bits\n  long sum;         // sum of modCounts\n  long last = 0L;   // previous sum\n  int retries = -1; // first iteration isn't retry\n  try {\n    for (;;) {\n      if (retries++ == RETRIES_BEFORE_LOCK) {\n        for (int j = 0; j < segments.length; ++j)\n          ensureSegment(j).lock(); // force creation\n      }\n      sum = 0L;\n      size = 0;\n      overflow = false;\n      for (int j = 0; j < segments.length; ++j) {\n        Segment<K,V> seg = segmentAt(segments, j);\n        if (seg != null) {\n          sum += seg.modCount;\n          int c = seg.count;\n          if (c < 0 || (size += c) < 0)\n            overflow = true;\n        }\n      }\n      if (sum == last)\n        break;\n      last = sum;\n    }\n  } finally {\n    if (retries > RETRIES_BEFORE_LOCK) {\n      for (int j = 0; j < segments.length; ++j)\n        segmentAt(segments, j).unlock();\n    }\n  }\n  return overflow ? Integer.MAX_VALUE : size;\n}\n```\n\n### 用分离锁实现多个线程间的并发写操作\n\n在 ConcurrentHashMap 中，线程对映射表做读操作时，一般情况下不需要加锁就可以完成，对容器做结构性修改的操作才需要加锁。下面以 put 操作为例说明对 ConcurrentHashMap 做结构性修改的过程。\n\n首先，根据 key 计算出对应的 hash 值：\n\n```java\n//Put 方法的实现\npublic V put(K key, V value) {\n       if (value == null)          //ConcurrentHashMap 中不允许用 null 作为映射值\n           throw new NullPointerException();\n       int hash = hash(key.hashCode());        // 计算键对应的散列码\n       // 根据散列码找到对应的 Segment\n       return segmentFor(hash).put(key, hash, value, false);\n}\n```\n\n然后，根据 hash 值找到对应的Segment 对象：\n\n```java\n//根据 hash 值找到对应的 Segment\n  /**\n    * 使用 key 的散列码来得到 segments 数组中对应的 Segment\n    */\nfinal Segment<K,V> segmentFor(int hash) {\n   // 将散列值右移 segmentShift 个位，并在高位填充 0\n   // 然后把得到的值与 segmentMask 相“与”\n// 从而得到 hash 值对应的 segments 数组的下标值\n// 最后根据下标值返回散列码对应的 Segment 对象\n       return segments[(hash >>> segmentShift) & segmentMask];\n}\n```\n\n最后，在这个 Segment 中执行具体的 put 操作：\n\n```java\n//在 Segment 中执行具体的 put 操作\nV put(K key, int hash, V value, boolean onlyIfAbsent) {\n    lock();  // 加锁，这里是锁定某个 Segment 对象而非整个 ConcurrentHashMap\n    try {\n        int c = count;\n        if (c++ > threshold)     // 如果超过再散列的阈值\n            rehash();              // 执行再散列，table 数组的长度将扩充一倍\n        HashEntry<K,V>[] tab = table;\n        // 把散列码值与 table 数组的长度减 1 的值相“与”\n        // 得到该散列码对应的 table 数组的下标值\n        int index = hash & (tab.length - 1);\n        // 找到散列码对应的具体的那个桶\n        HashEntry<K,V> first = tab[index];\n        HashEntry<K,V> e = first;\n        while (e != null && (e.hash != hash || !key.equals(e.key)))\n            e = e.next;\n        V oldValue;\n        if (e != null) {            // 如果键 / 值对以经存在\n            oldValue = e.value;\n            if (!onlyIfAbsent)\n                e.value = value;    // 设置 value 值\n        }\n        else {                        // 键 / 值对不存在\n            oldValue = null;\n            ++modCount;         // 要添加新节点到链表中，所以 modCont 要加 1\n            // 创建新节点，并添加到链表的头部\n            tab[index] = new HashEntry<K,V>(key, hash, first, value);\n            count = c;               // 写 count 变量\n        }\n        return oldValue;\n    } finally {\n        unlock();                     // 解锁\n    }\n}\n```\n\n注意：这里的加锁操作是针对（键的 hash 值对应的）某个具体的 Segment，锁定的是该 Segment 而不是整个 ConcurrentHashMap。因为插入键/值对操作只是在这个 Segment 包含的某个桶中完成，不需要锁定整个ConcurrentHashMap。此时，其他写线程对另外 15 个Segment 的加锁并不会因为当前线程对这个 Segment 的加锁而阻塞。同时，所有读线程几乎不会因本线程的加锁而阻塞（除非读线程刚好读到这个 Segment 中某个 HashEntry 的 value 域的值为 null，此时需要加锁后重新读取该值）。\n\n相比较于 HashTable 和由同步包装器包装的 HashMap每次只能有一个线程执行读或写操作，ConcurrentHashMap 在并发访问性能上有了质的提高。在理想状态下，ConcurrentHashMap 可以支持 16 个线程执行并发写操作（如果并发级别设置为 16），及任意数量线程的读操作。\n\n获取锁时，并不直接使用lock来获取，因为该方法获取锁失败时会挂起（参考可重入锁）。事实上，它使用了自旋锁，如果tryLock获取锁失败，说明锁被其它线程占用，此时通过循环再次以tryLock的方式申请锁。如果在循环过程中该Key所对应的链表头被修改，则重置retry次数。如果retry次数超过一定值，则使用lock方法申请锁。\n\n这里使用自旋锁是因为自旋锁的效率比较高，但是它消耗CPU资源比较多，因此在自旋次数超过阈值时切换为互斥锁。\n\n### 用 HashEntery 对象的不变性来降低读操作对加锁的需求\n\n在代码清单“HashEntry 类的定义”中我们可以看到，HashEntry 中的 key，hash，next 都声明为 final 型。这意味着，不能把节点添加到链接的中间和尾部，也不能在链接的中间和尾部删除节点。这个特性可以保证：在访问某个节点时，这个节点之后的链接不会被改变。这个特性可以大大降低处理链表时的复杂性。\n\n同时，HashEntry 类的 value 域被声明为 Volatile 型，Java 的内存模型可以保证：某个写线程对 value 域的写入马上可以被后续的某个读线程“看”到。在 ConcurrentHashMap 中，不允许用 unll 作为键和值，当读线程读到某个 HashEntry 的 value 域的值为 null 时，便知道产生了冲突——发生了重排序现象，需要加锁后重新读入这个 value 值。这些特性互相配合，使得读线程即使在不加锁状态下，也能正确访问 ConcurrentHashMap。\n\n下面我们分别来分析线程写入的两种情形：对散列表做非结构性修改的操作和对散列表做结构性修改的操作。\n\n非结构性修改操作只是更改某个 HashEntry 的 value 域的值。由于对 Volatile 变量的写入操作将与随后对这个变量的读操作进行同步。当一个写线程修改了某个 HashEntry 的 value 域后，另一个读线程读这个值域，Java 内存模型能够保证读线程读取的一定是更新后的值。所以，写线程对链表的非结构性修改能够被后续不加锁的读线程“看到”。\n\n对 ConcurrentHashMap 做结构性修改，实质上是对某个桶指向的链表做结构性修改。如果能够确保：在读线程遍历一个链表期间，写线程对这个链表所做的结构性修改不影响读线程继续正常遍历这个链表。那么读 / 写线程之间就可以安全并发访问这个 ConcurrentHashMap。\n\n结构性修改操作包括 put，remove，clear。下面我们分别分析这三个操作。\n\nclear 操作只是把 ConcurrentHashMap 中所有的桶“置空”，每个桶之前引用的链表依然存在，只是桶不再引用到这些链表（所有链表的结构并没有被修改）。正在遍历某个链表的读线程依然可以正常执行对该链表的遍历。\n\n从上面的代码清单“在 Segment 中执行具体的 put 操作”中，我们可以看出：put 操作如果需要插入一个新节点到链表中时 , 会在链表头部插入这个新节点。此时，链表中的原有节点的链接并没有被修改。也就是说：插入新健 / 值对到链表中的操作不会影响读线程正常遍历这个链表。\n\n下面来分析 remove 操作，先让我们来看看 remove 操作的源代码实现。\n\n```java\n//remove 操作\nV remove(Object key, int hash, Object value) {\n    lock();         // 加锁\n    try{\n        int c = count - 1;\n        HashEntry<K,V>[] tab = table;\n        // 根据散列码找到 table 的下标值\n        int index = hash & (tab.length - 1);\n        // 找到散列码对应的那个桶\n        HashEntry<K,V> first = tab[index];\n        HashEntry<K,V> e = first;\n        while(e != null&& (e.hash != hash || !key.equals(e.key)))\n            e = e.next;\n        V oldValue = null;\n        if(e != null) {\n            V v = e.value;\n            if(value == null|| value.equals(v)) { // 找到要删除的节点\n                oldValue = v;\n                ++modCount;\n                // 所有处于待删除节点之后的节点原样保留在链表中\n                // 所有处于待删除节点之前的节点被克隆到新链表中\n                HashEntry<K,V> newFirst = e.next;// 待删节点的后继结点\n                for(HashEntry<K,V> p = first; p != e; p = p.next)\n                    newFirst = new HashEntry<K,V>(p.key, p.hash,\n                                                  newFirst, p.value);\n                // 把桶链接到新的头结点\n                // 新的头结点是原链表中，删除节点之前的那个节点\n                tab[index] = newFirst;\n                count = c;      // 写 count 变量\n            }\n        }\n        return oldValue;\n    } finally{\n        unlock();               // 解锁\n    }\n}\n```\n\n和 get 操作一样，首先根据散列码找到具体的链表；然后遍历这个链表找到要删除的节点；最后把待删除节点之后的所有节点原样保留在新链表中，把待删除节点之前的每个节点克隆到新链表中。下面通过图例来说明 remove 操作。假设写线程执行 remove 操作，要删除链表的 `C 节点` ，另一个读线程同时正在遍历这个链表。\n\n执行删除之前的原链表:\n\n![执行删除之前的原链表](/images/concurrenthashmap/执行删除之前的原链表.jpg)\n\n执行删除之后的新链表:\n\n![执行删除之后的新链表](/images/concurrenthashmap/执行删除之后的新链表.jpg)\n\n从上图可以看出，删除节点 C 之后的所有节点原样保留到新链表中；删除节点 C 之前的每个节点被克隆到新链表中，注意：它们在新链表中的链接顺序被反转了。\n\n在执行 remove 操作时，原始链表并没有被修改，也就是说：读线程不会受同时执行 remove 操作的并发写线程的干扰。\n\n综合上面的分析我们可以看出，写线程对某个链表的结构性修改不会影响其他的并发读线程对这个链表的遍历访问。\n\n### 用 Volatile 变量协调读写线程间的内存可见性\n\n由于内存可见性问题，未正确同步的情况下，写线程写入的值可能并不为后续的读线程可见。\n\n下面以写线程 M 和读线程 N 来说明 ConcurrentHashMap 如何协调读 / 写线程间的内存可见性问题。\n\n![协调读-写线程间的内存可见性的示意图](/images/concurrenthashmap/协调读-写线程间的内存可见性的示意图.jpg)\n\n假设线程 M 在写入了 volatile 型变量 count 后，线程 N 读取了这个 volatile 型变量 count。\n\n根据 happens-before 关系法则中的程序次序法则，A appens-before 于 B，C happens-before D。\n\n根据 Volatile 变量法则，B happens-before C。\n\n根据传递性，连接上面三个 happens-before 关系得到：A appens-before 于 B； B appens-before C；C happens-before D。也就是说：写线程 M 对链表做的结构性修改，在读线程 N 读取了同一个 volatile 变量后，对线程 N 也是可见的了。\n\n虽然线程 N 是在未加锁的情况下访问链表。Java 的内存模型可以保证：只要之前对链表做结构性修改操作的写线程 M 在退出写方法前写 volatile 型变量 count，读线程 N 在读取这个 volatile 型变量 count 后，就一定能“看到”这些修改。\n\nConcurrentHashMap 中，每个 Segment 都有一个变量 count。它用来统计 Segment 中的 HashEntry 的个数。这个变量被声明为 volatile。\n\n```java\n//Count 变量的声明\ntransient volatile int count;\n```\n\n所有不加锁读方法，在进入读方法时，首先都会去读这个 count 变量。比如下面的 get 方法：\n\n```java\n//get 操作\nV get(Object key, int hash) {\n    if(count != 0) {       // 首先读 count 变量\n        HashEntry<K,V> e = getFirst(hash);\n        while(e != null) {\n            if(e.hash == hash && key.equals(e.key)) {\n                V v = e.value;\n                if(v != null)\n                    return v;\n                // 如果读到 value 域为 null，说明发生了重排序，加锁后重新读取\n                return readValueUnderLock(e);\n            }\n            e = e.next;\n        }\n    }\n    return null;\n}\n```\n\n在 ConcurrentHashMap 中，所有执行写操作的方法（put, remove, clear），在对链表做结构性修改之后，在退出写方法前都会去写这个 count 变量。所有未加锁的读操作（get, contains, containsKey）在读方法中，都会首先去读取这个 count 变量。\n\n根据 Java 内存模型，对 同一个 volatile 变量的写 / 读操作可以确保：写线程写入的值，能够被之后未加锁的读线程“看到”。\n\n这个特性和前面介绍的 HashEntry 对象的不变性相结合，使得在 ConcurrentHashMap 中，读线程在读取散列表时，基本不需要加锁就能成功获得需要的值。这两个特性相配合，不仅减少了请求同一个锁的频率（读操作一般不需要加锁就能够成功获得值），也减少了持有同一个锁的时间（只有读到 value 域的值为 null 时 , 读线程才需要加锁后重读）。\n\n## Java 8基于CAS的ConcurrentHashMap\n\n### Java 8 的数据结构\n\nJava 7为实现并行访问，引入了Segment这一结构，实现了分段锁，理论上最大并发度与Segment个数相等。Java 8为进一步提高并发性，摒弃了分段锁的方案，而是直接使用一个大的数组。同时为了提高哈希碰撞下的寻址性能，Java 8在链表长度超过一定阈值（8）时将链表（寻址时间复杂度为O(N)）转换为红黑树（寻址时间复杂度为O(long(N))）。其数据结构如下图所示\n\n整体数据结构如下图所示：\n\n![java8数据结构](/images/concurrenthashmap/java8数据结构.png)\n\n### Java 8 的同步方式\n\n对于put操作，如果Key对应的数组元素为null，则通过 `CAS操作` 将其设置为当前值。如果Key对应的数组元素（也即链表表头或者树的根元素）不为null，则对该元素使用synchronized关键字申请锁，然后进行操作。如果该put操作使得当前链表长度超过一定阈值，则将该链表转换为树，从而提高寻址效率。\n\n```java\nstatic class Node<K,V> implements Map.Entry<K,V> {\n  final int hash;\n  final K key;\n  volatile V val;\n  volatile Node<K,V> next;\n}\n```\n\n对于读操作，由于数组被volatile关键字修饰，因此不用担心数组的可见性问题。同时每个元素是一个Node实例（Java 7中每个元素是一个HashEntry），它的Key值和hash值都由final修饰，不可变更，无须关心它们被修改后的可见性问题。而其Value及对下一个元素的引用由volatile修饰，可见性也有保障。\n\n```java\nstatic final <K,V> Node<K,V> tabAt(Node<K,V>[] tab, int i) {\n  return (Node<K,V>)U.getObjectVolatile(tab, ((long)i << ASHIFT) + ABASE);\n}\n```\n\n### 初始化\n\n```java\npublic ConcurrentHashMap() {\n}\npublic ConcurrentHashMap(int initialCapacity) {\n    if (initialCapacity < 0)\n        throw new IllegalArgumentException();\n    int cap = ((initialCapacity >= (MAXIMUM_CAPACITY >>> 1)) ?\n               MAXIMUM_CAPACITY :\n               tableSizeFor(initialCapacity + (initialCapacity >>> 1) + 1));\n    this.sizeCtl = cap;\n}\n```\n\n通过提供初始容量，计算了 sizeCtl，sizeCtl = [(1.5 * initialCapacity + 1)，然后向上取最近的 2 的 n 次方]。如 initialCapacity 为 10，那么得到 sizeCtl 为 16，如果 initialCapacity 为 11，得到 sizeCtl 为 32。\n\n### put 过程\n\nput 的主流程，第一个是初始化，第二个是扩容，第三个是帮助数据迁移。\n\n```java\npublic V put(K key, V value) {\n    return putVal(key, value, false);\n}\nfinal V putVal(K key, V value, boolean onlyIfAbsent) {\n    if (key == null || value == null) throw new NullPointerException();\n    // 得到 hash 值\n    int hash = spread(key.hashCode());\n    // 用于记录相应链表的长度\n    int binCount = 0;\n    for (Node<K,V>[] tab = table;;) {\n        Node<K,V> f; int n, i, fh;\n        // 如果数组\"空\"，进行数组初始化\n        if (tab == null || (n = tab.length) == 0)\n            // 初始化数组，后面会详细介绍\n            tab = initTable();\n\n        // 找该 hash 值对应的数组下标，得到第一个节点 f\n        else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) {\n            // 如果数组该位置为空，\n            //    用一次 CAS 操作将这个新值放入其中即可，这个 put 操作差不多就结束了，可以拉到最后面了\n            //          如果 CAS 失败，那就是有并发操作，进到下一个循环就好了\n            if (casTabAt(tab, i, null,\n                         new Node<K,V>(hash, key, value, null)))\n                break;                   // no lock when adding to empty bin\n        }\n        // hash 居然可以等于 MOVED，这个需要到后面才能看明白，不过从名字上也能猜到，肯定是因为在扩容\n        else if ((fh = f.hash) == MOVED)\n            // 帮助数据迁移，这个等到看完数据迁移部分的介绍后，再理解这个就很简单了\n            tab = helpTransfer(tab, f);\n\n        else { // 到这里就是说，f 是该位置的头结点，而且不为空\n\n            V oldVal = null;\n            // 获取数组该位置的头结点的监视器锁\n            synchronized (f) {\n                if (tabAt(tab, i) == f) {\n                    if (fh >= 0) { // 头结点的 hash 值大于 0，说明是链表\n                        // 用于累加，记录链表的长度\n                        binCount = 1;\n                        // 遍历链表\n                        for (Node<K,V> e = f;; ++binCount) {\n                            K ek;\n                            // 如果发现了\"相等\"的 key，判断是否要进行值覆盖，然后也就可以 break 了\n                            if (e.hash == hash &&\n                                ((ek = e.key) == key ||\n                                 (ek != null && key.equals(ek)))) {\n                                oldVal = e.val;\n                                if (!onlyIfAbsent)\n                                    e.val = value;\n                                break;\n                            }\n                            // 到了链表的最末端，将这个新值放到链表的最后面\n                            Node<K,V> pred = e;\n                            if ((e = e.next) == null) {\n                                pred.next = new Node<K,V>(hash, key,\n                                                          value, null);\n                                break;\n                            }\n                        }\n                    }\n                    else if (f instanceof TreeBin) { // 红黑树\n                        Node<K,V> p;\n                        binCount = 2;\n                        // 调用红黑树的插值方法插入新节点\n                        if ((p = ((TreeBin<K,V>)f).putTreeVal(hash, key,\n                                                       value)) != null) {\n                            oldVal = p.val;\n                            if (!onlyIfAbsent)\n                                p.val = value;\n                        }\n                    }\n                }\n            }\n            // binCount != 0 说明上面在做链表操作\n            if (binCount != 0) {\n                // 判断是否要将链表转换为红黑树，临界值和 HashMap 一样，也是 8\n                if (binCount >= TREEIFY_THRESHOLD)\n                    // 这个方法和 HashMap 中稍微有一点点不同，那就是它不是一定会进行红黑树转换，\n                    // 如果当前数组的长度小于 64，那么会选择进行数组扩容，而不是转换为红黑树\n                    //    具体源码我们就不看了，扩容部分后面说\n                    treeifyBin(tab, i);\n                if (oldVal != null)\n                    return oldVal;\n                break;\n            }\n        }\n    }\n\n    addCount(1L, binCount);\n    return null;\n}\n```\n\n现在对put(putVal)方法做一个总结:\n\n1. 如果待插入的键值对中key或value为null, 抛出异常, 结束. 否则执行2\n2. 如果table为null, 则进行初始化操作initTable(), 否则执行3\n3. 如果table[i]为空, 则用CAS在table[i]头结点直接插入, 如果CAS执行成功, 退出插入操作; 执行步骤7; 如果CAS失败, 则说明有其他节点已经插入, 执行4\n4. 此时判断, hash值是否为MOVED(-1), 如果是则说明其他有其他线程在执行扩容操作, 帮助他们一起扩容, 来提高性能. 如果没有在扩容, 那么执行5\n5. 判断hash的值, 如果fh(table[i])的hash>=0, 则在链表合适的位置插入, 否则, 查看table[i]是否是红黑树结构, 如果是, 则在红黑树适当位置插入. 到此时, 键值对已经顺利插入. 接下来执行6\n6. 如果table[i]节点数binCount不为0, 判断它此时的状态, 是否需要转变为红黑树\n7. 执行addcount(1L, binCount)\n\n#### 初始化数组(initTable)\n\n初始化方法中的并发问题是通过对 sizeCtl 进行一个 CAS 操作来控制的。\n\n```java\nprivate final Node<K,V>[] initTable() {\n    Node<K,V>[] tab; int sc;\n    while ((tab = table) == null || tab.length == 0) {\n        // 初始化的\"功劳\"被其他线程\"抢去\"了\n        if ((sc = sizeCtl) < 0)\n            Thread.yield(); // lost initialization race; just spin\n        // CAS 一下，将 sizeCtl 设置为 -1，代表抢到了锁\n        else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) {\n            try {\n                if ((tab = table) == null || tab.length == 0) {\n                    // DEFAULT_CAPACITY 默认初始容量是 16\n                    int n = (sc > 0) ? sc : DEFAULT_CAPACITY;\n                    // 初始化数组，长度为 16 或初始化时提供的长度\n                    Node<K,V>[] nt = (Node<K,V>[])new Node<?,?>[n];\n                    // 将这个数组赋值给 table，table 是 volatile 的\n                    table = tab = nt;\n                    // 如果 n 为 16 的话，那么这里 sc = 12\n                    // 其实就是 0.75 * n\n                    sc = n - (n >>> 2);\n                }\n            } finally {\n                // 设置 sizeCtl 为 sc，我们就当是 12 吧\n                sizeCtl = sc;\n            }\n            break;\n        }\n    }\n    return tab;\n}\n```\n\n如果sizeCtl<0, 则说明已经有线程在执行初始化, 则其他执行初始化方式的线程应当交出CPU时间片退出; 否则, 用CAS把sizeCtl设置为-1, 告诉其他线程, 自己正在执行初始化, 此时段其他进入初始化方法的线程将交出时间片.\n\n#### 链表转红黑树(treeifyBin)\n\ntreeifyBin 不一定就会进行红黑树转换，也可能是仅仅做数组扩容。\n\n```java\nprivate final void treeifyBin(Node<K,V>[] tab, int index) {\n    Node<K,V> b; int n, sc;\n    if (tab != null) {\n        // MIN_TREEIFY_CAPACITY 为 64\n        // 所以，如果数组长度小于 64 的时候，其实也就是 32 或者 16 或者更小的时候，会进行数组扩容\n        if ((n = tab.length) < MIN_TREEIFY_CAPACITY)\n            // 后面我们再详细分析这个方法\n            tryPresize(n << 1);\n        // b 是头结点\n        else if ((b = tabAt(tab, index)) != null && b.hash >= 0) {\n            // 加锁\n            synchronized (b) {\n\n                if (tabAt(tab, index) == b) {\n                    // 下面就是遍历链表，建立一颗红黑树\n                    TreeNode<K,V> hd = null, tl = null;\n                    for (Node<K,V> e = b; e != null; e = e.next) {\n                        TreeNode<K,V> p =\n                            new TreeNode<K,V>(e.hash, e.key, e.val,\n                                              null, null);\n                        if ((p.prev = tl) == null)\n                            hd = p;\n                        else\n                            tl.next = p;\n                        tl = p;\n                    }\n                    // 将红黑树设置到数组相应位置中\n                    setTabAt(tab, index, new TreeBin<K,V>(hd));\n                }\n            }\n        }\n    }\n}\n```\n\n#### 扩容(tryPresize)\n\n扩容是做翻倍扩容的，扩容后数组容量为原来的 2 倍。\n\n```java\n// 首先要说明的是，方法参数 size 传进来的时候就已经翻了倍了\nprivate final void tryPresize(int size) {\n    // c：size 的 1.5 倍，再加 1，再往上取最近的 2 的 n 次方。\n    int c = (size >= (MAXIMUM_CAPACITY >>> 1)) ? MAXIMUM_CAPACITY :\n        tableSizeFor(size + (size >>> 1) + 1);\n    int sc;\n    while ((sc = sizeCtl) >= 0) {\n        Node<K,V>[] tab = table; int n;\n\n        // 这个 if 分支和之前说的初始化数组的代码基本上是一样的，在这里，我们可以不用管这块代码\n        if (tab == null || (n = tab.length) == 0) {\n            n = (sc > c) ? sc : c;\n            if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) {\n                try {\n                    if (table == tab) {\n                        @SuppressWarnings(\"unchecked\")\n                        Node<K,V>[] nt = (Node<K,V>[])new Node<?,?>[n];\n                        table = nt;\n                        sc = n - (n >>> 2); // 0.75 * n\n                    }\n                } finally {\n                    sizeCtl = sc;\n                }\n            }\n        }\n        else if (c <= sc || n >= MAXIMUM_CAPACITY)\n            break;\n        else if (tab == table) {\n            // 我没看懂 rs 的真正含义是什么，不过也关系不大\n            int rs = resizeStamp(n);\n\n            if (sc < 0) {\n                Node<K,V>[] nt;\n                if ((sc >>> RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 ||\n                    sc == rs + MAX_RESIZERS || (nt = nextTable) == null ||\n                    transferIndex <= 0)\n                    break;\n                // 2. 用 CAS 将 sizeCtl 加 1，然后执行 transfer 方法\n                //    此时 nextTab 不为 null\n                if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1))\n                    transfer(tab, nt);\n            }\n            // 1. 将 sizeCtl 设置为 (rs << RESIZE_STAMP_SHIFT) + 2)\n            //     我是没看懂这个值真正的意义是什么？不过可以计算出来的是，结果是一个比较大的负数\n            //  调用 transfer 方法，此时 nextTab 参数为 null\n            else if (U.compareAndSwapInt(this, SIZECTL, sc,\n                                         (rs << RESIZE_STAMP_SHIFT) + 2))\n                transfer(tab, null);\n        }\n    }\n}\n```\n\n这个方法的核心在于 sizeCtl 值的操作，首先将其设置为一个负数，然后执行 transfer(tab, null)，再下一个循环将 sizeCtl 加 1，并执行 transfer(tab, nt)，之后可能是继续 sizeCtl 加 1，并执行 transfer(tab, nt)。\n\n所以，可能的操作就是执行 1 次 transfer(tab, null) + 多次 transfer(tab, nt)，这里怎么结束循环的需要看完 transfer 源码才清楚。\n\n#### 数据迁移(transfer)\n\n将原来的 tab 数组的元素迁移到新的 nextTab 数组中。\n\n虽然我们之前说的 tryPresize 方法中多次调用 transfer 不涉及多线程，但是这个 transfer 方法可以在其他地方被调用，典型地，我们之前在说 put 方法的时候就说过了，请往上看 put 方法，是不是有个地方调用了 helpTransfer 方法，helpTransfer 方法会调用 transfer 方法的。\n\n此方法支持多线程执行，外围调用此方法的时候，会保证第一个发起数据迁移的线程，nextTab 参数为 null，之后再调用此方法的时候，nextTab 不会为 null。\n\n阅读源码之前，先要理解并发操作的机制。原数组长度为 n，所以我们有 n 个迁移任务，让每个线程每次负责一个小任务是最简单的，每做完一个任务再检测是否有其他没做完的任务，帮助迁移就可以了，而 Doug Lea 使用了一个 stride，简单理解就是步长，每个线程每次负责迁移其中的一部分，如每次迁移 16 个小任务。所以，我们就需要一个全局的调度者来安排哪个线程执行哪几个任务，这个就是属性 transferIndex 的作用。\n\n第一个发起数据迁移的线程会将 transferIndex 指向原数组最后的位置，然后从后往前的 stride 个任务属于第一个线程，然后将 transferIndex 指向新的位置，再往前的 stride 个任务属于第二个线程，依此类推。当然，这里说的第二个线程不是真的一定指代了第二个线程，也可以是同一个线程，这个读者应该能理解吧。其实就是将一个大的迁移任务分为了一个个任务包。\n\n```java\nprivate final void transfer(Node<K,V>[] tab, Node<K,V>[] nextTab) {\n    int n = tab.length, stride;\n\n    // stride 在单核下直接等于 n，多核模式下为 (n>>>3)/NCPU，最小值是 16\n    // stride 可以理解为”步长“，有 n 个位置是需要进行迁移的，\n    //   将这 n 个任务分为多个任务包，每个任务包有 stride 个任务\n    if ((stride = (NCPU > 1) ? (n >>> 3) / NCPU : n) < MIN_TRANSFER_STRIDE)\n        stride = MIN_TRANSFER_STRIDE; // subdivide range\n\n    // 如果 nextTab 为 null，先进行一次初始化\n    //    前面我们说了，外围会保证第一个发起迁移的线程调用此方法时，参数 nextTab 为 null\n    //       之后参与迁移的线程调用此方法时，nextTab 不会为 null\n    if (nextTab == null) {\n        try {\n            // 容量翻倍\n            Node<K,V>[] nt = (Node<K,V>[])new Node<?,?>[n << 1];\n            nextTab = nt;\n        } catch (Throwable ex) {      // try to cope with OOME\n            sizeCtl = Integer.MAX_VALUE;\n            return;\n        }\n        // nextTable 是 ConcurrentHashMap 中的属性\n        nextTable = nextTab;\n        // transferIndex 也是 ConcurrentHashMap 的属性，用于控制迁移的位置\n        transferIndex = n;\n    }\n\n    int nextn = nextTab.length;\n\n    // ForwardingNode 翻译过来就是正在被迁移的 Node\n    // 这个构造方法会生成一个Node，key、value 和 next 都为 null，关键是 hash 为 MOVED\n    // 后面我们会看到，原数组中位置 i 处的节点完成迁移工作后，\n    //    就会将位置 i 处设置为这个 ForwardingNode，用来告诉其他线程该位置已经处理过了\n    //    所以它其实相当于是一个标志。\n    ForwardingNode<K,V> fwd = new ForwardingNode<K,V>(nextTab);\n\n\n    // advance 指的是做完了一个位置的迁移工作，可以准备做下一个位置的了\n    boolean advance = true;\n    boolean finishing = false; // to ensure sweep before committing nextTab\n\n    /*\n     * 下面这个 for 循环，最难理解的在前面，而要看懂它们，应该先看懂后面的，然后再倒回来看\n     *\n     */\n\n    // i 是位置索引，bound 是边界，注意是从后往前\n    for (int i = 0, bound = 0;;) {\n        Node<K,V> f; int fh;\n\n        // 下面这个 while 真的是不好理解\n        // advance 为 true 表示可以进行下一个位置的迁移了\n        //   简单理解结局：i 指向了 transferIndex，bound 指向了 transferIndex-stride\n        while (advance) {\n            int nextIndex, nextBound;\n            if (--i >= bound || finishing)\n                advance = false;\n\n            // 将 transferIndex 值赋给 nextIndex\n            // 这里 transferIndex 一旦小于等于 0，说明原数组的所有位置都有相应的线程去处理了\n            else if ((nextIndex = transferIndex) <= 0) {\n                i = -1;\n                advance = false;\n            }\n            else if (U.compareAndSwapInt\n                     (this, TRANSFERINDEX, nextIndex,\n                      nextBound = (nextIndex > stride ?\n                                   nextIndex - stride : 0))) {\n                // 看括号中的代码，nextBound 是这次迁移任务的边界，注意，是从后往前\n                bound = nextBound;\n                i = nextIndex - 1;\n                advance = false;\n            }\n        }\n        if (i < 0 || i >= n || i + n >= nextn) {\n            int sc;\n            if (finishing) {\n                // 所有的迁移操作已经完成\n                nextTable = null;\n                // 将新的 nextTab 赋值给 table 属性，完成迁移\n                table = nextTab;\n                // 重新计算 sizeCtl：n 是原数组长度，所以 sizeCtl 得出的值将是新数组长度的 0.75 倍\n                sizeCtl = (n << 1) - (n >>> 1);\n                return;\n            }\n\n            // 之前我们说过，sizeCtl 在迁移前会设置为 (rs << RESIZE_STAMP_SHIFT) + 2\n            // 然后，每有一个线程参与迁移就会将 sizeCtl 加 1，\n            // 这里使用 CAS 操作对 sizeCtl 进行减 1，代表做完了属于自己的任务\n            if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) {\n                // 任务结束，方法退出\n                if ((sc - 2) != resizeStamp(n) << RESIZE_STAMP_SHIFT)\n                    return;\n\n                // 到这里，说明 (sc - 2) == resizeStamp(n) << RESIZE_STAMP_SHIFT，\n                // 也就是说，所有的迁移任务都做完了，也就会进入到上面的 if(finishing){} 分支了\n                finishing = advance = true;\n                i = n; // recheck before commit\n            }\n        }\n        // 如果位置 i 处是空的，没有任何节点，那么放入刚刚初始化的 ForwardingNode ”空节点“\n        else if ((f = tabAt(tab, i)) == null)\n            advance = casTabAt(tab, i, null, fwd);\n        // 该位置处是一个 ForwardingNode，代表该位置已经迁移过了\n        else if ((fh = f.hash) == MOVED)\n            advance = true; // already processed\n        else {\n            // 对数组该位置处的结点加锁，开始处理数组该位置处的迁移工作\n            synchronized (f) {\n                if (tabAt(tab, i) == f) {\n                    Node<K,V> ln, hn;\n                    // 头结点的 hash 大于 0，说明是链表的 Node 节点\n                    if (fh >= 0) {\n                        // 下面这一块和 Java7 中的 ConcurrentHashMap 迁移是差不多的，\n                        // 需要将链表一分为二，\n                        //   找到原链表中的 lastRun，然后 lastRun 及其之后的节点是一起进行迁移的\n                        //   lastRun 之前的节点需要进行克隆，然后分到两个链表中\n                        int runBit = fh & n;\n                        Node<K,V> lastRun = f;\n                        for (Node<K,V> p = f.next; p != null; p = p.next) {\n                            int b = p.hash & n;\n                            if (b != runBit) {\n                                runBit = b;\n                                lastRun = p;\n                            }\n                        }\n                        if (runBit == 0) {\n                            ln = lastRun;\n                            hn = null;\n                        }\n                        else {\n                            hn = lastRun;\n                            ln = null;\n                        }\n                        for (Node<K,V> p = f; p != lastRun; p = p.next) {\n                            int ph = p.hash; K pk = p.key; V pv = p.val;\n                            if ((ph & n) == 0)\n                                ln = new Node<K,V>(ph, pk, pv, ln);\n                            else\n                                hn = new Node<K,V>(ph, pk, pv, hn);\n                        }\n                        // 其中的一个链表放在新数组的位置 i\n                        setTabAt(nextTab, i, ln);\n                        // 另一个链表放在新数组的位置 i+n\n                        setTabAt(nextTab, i + n, hn);\n                        // 将原数组该位置处设置为 fwd，代表该位置已经处理完毕，\n                        //    其他线程一旦看到该位置的 hash 值为 MOVED，就不会进行迁移了\n                        setTabAt(tab, i, fwd);\n                        // advance 设置为 true，代表该位置已经迁移完毕\n                        advance = true;\n                    }\n                    else if (f instanceof TreeBin) {\n                        // 红黑树的迁移\n                        TreeBin<K,V> t = (TreeBin<K,V>)f;\n                        TreeNode<K,V> lo = null, loTail = null;\n                        TreeNode<K,V> hi = null, hiTail = null;\n                        int lc = 0, hc = 0;\n                        for (Node<K,V> e = t.first; e != null; e = e.next) {\n                            int h = e.hash;\n                            TreeNode<K,V> p = new TreeNode<K,V>\n                                (h, e.key, e.val, null, null);\n                            if ((h & n) == 0) {\n                                if ((p.prev = loTail) == null)\n                                    lo = p;\n                                else\n                                    loTail.next = p;\n                                loTail = p;\n                                ++lc;\n                            }\n                            else {\n                                if ((p.prev = hiTail) == null)\n                                    hi = p;\n                                else\n                                    hiTail.next = p;\n                                hiTail = p;\n                                ++hc;\n                            }\n                        }\n                        // 如果一分为二后，节点数少于 8，那么将红黑树转换回链表\n                        ln = (lc <= UNTREEIFY_THRESHOLD) ? untreeify(lo) :\n                            (hc != 0) ? new TreeBin<K,V>(lo) : t;\n                        hn = (hc <= UNTREEIFY_THRESHOLD) ? untreeify(hi) :\n                            (lc != 0) ? new TreeBin<K,V>(hi) : t;\n\n                        // 将 ln 放置在新数组的位置 i\n                        setTabAt(nextTab, i, ln);\n                        // 将 hn 放置在新数组的位置 i+n\n                        setTabAt(nextTab, i + n, hn);\n                        // 将原数组该位置处设置为 fwd，代表该位置已经处理完毕，\n                        //    其他线程一旦看到该位置的 hash 值为 MOVED，就不会进行迁移了\n                        setTabAt(tab, i, fwd);\n                        // advance 设置为 true，代表该位置已经迁移完毕\n                        advance = true;\n                    }\n                }\n            }\n        }\n    }\n}\n```\n\n说到底，transfer 这个方法并没有实现所有的迁移任务，每次调用这个方法只实现了 transferIndex 往前 stride 个位置的迁移工作，其他的需要由外围来控制。\n\n## ConcurrentHashMap 实现高并发的总结\n\nConcurrentHashMap 是一个并发散列映射表的实现，它允许完全并发的读取，并且支持给定数量的并发更新。相比于 HashTable 和用同步包装器包装的 HashMap（Collections.synchronizedMap(new HashMap())），ConcurrentHashMap 拥有更高的并发性。在 HashTable 和由同步包装器包装的 HashMap 中，使用一个全局的锁来同步不同线程间的并发访问。同一时间点，只能有一个线程持有锁，也就是说在同一时间点，只能有一个线程能访问容器。这虽然保证多线程间的安全并发访问，但同时也导致对容器的访问变成串行化的了。\n\n在使用锁来协调多线程间并发访问的模式下，减小对锁的竞争可以有效提高并发性。有两种方式可以减小对锁的竞争：\n\n1. 减小请求 同一个锁的频率。\n2. 减少持有锁的时间。\n\nConcurrentHashMap 的高并发性主要来自于三个方面：\n\n1. 用分离锁实现多个线程间的更深层次的共享访问。\n2. 用 HashEntery 对象的不变性来降低执行读操作的线程在遍历链表期间对加锁的需求。\n3. 通过对同一个 Volatile 变量的写 / 读访问，协调不同线程间读 / 写操作的内存可见性。\n\n使用分离锁，减小了请求 同一个锁的频率。\n\n通过 HashEntery 对象的不变性及对同一个 Volatile 变量的读 / 写来协调内存可见性，使得 读操作大多数时候不需要加锁就能成功获取到需要的值。由于散列映射表在实际应用中大多数操作都是成功的 读操作，所以 2 和 3 既可以减少请求同一个锁的频率，也可以有效减少持有锁的时间。\n\n通过减小请求同一个锁的频率和尽量减少持有锁的时间 ，使得 ConcurrentHashMap 的并发性相对于 HashTable 和用同步包装器包装的 HashMap有了质的提高。\n","tags":["Concurrency"]},{"title":"线程的基本状态以及状态之间的关系","url":"%2F2018%2F03%2F12%2Fthread%2F","content":"\n## 什么是线程\n\n一个线程是进程的一个顺序执行流。同类的多个线程共享一块内存空间和一组系统资源，线程本身有一个供程序执行时的堆栈。线程在切换时负荷小，因此，线程也被称为轻负荷进程。一个进程中可以包含多个线程。\n\n## 进程与线程的区别\n\n一个进程至少有一个线程。线程的划分尺度小于进程，使得多线程程序的并发性高。另外，进程在执行过程中拥有独立的内存单元，而多个线程共享内存，从而极大地提高了程序的运行效率。\n\n线程在执行过程中与进程的区别在于每个独立的线程有一个程序运行的入口、顺序执行序列和程序的出口。但是线程不能够独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。\n\n从逻辑角度来看，多线程的意义在于一个应用程序中，有多个执行部分可以同时执行。但操作系统并没有将多个线程看做多个独立的应用来实现进程的调度和管理以及资源分配。\n\n## 并发原理\n\n多个线程或进程”同时”运行只是我们感官上的一种表现。事实上进程和线程是并发运行的，OS的线程调度机制将时间划分为很多时间片段（时间片），尽可能均匀分配给正在运行的程序，获取CPU时间片的线程或进程得以被执行，其他则等待。而CPU则在这些进程或线程上来回切换运行。微观上所有进程和线程是走走停停的，宏观上都在运行，这种都运行的现象叫并发，但是不是绝对意义上的“同时发生。\n\n<!-- more -->\n\n## 线程状态\n\n![线程状态](/images/thread/thread.png)\n\n### 1. 新建(NEW)\n\n用new语句 `new Thread(r)` 创建的线程对象处于新建状态，此时它和其他java对象一样，仅被分配了内存。\n\n### 2. 就绪(RUNNABLE)\n\n当start()方法被调用时，线程就进入RUNNABLE状态。处于这个状态的线程位于Java虚拟机的可运行池中，等待cpu的使用权。\n\n### 3. 运行(RUNNING)\n\n处于这个状态的线程占用CPU，执行程序代码。在并发运行环境中，如果计算机只有一个CPU，那么任何时刻只会有一个线程处于这个状态。\n\n当前线程时间片用完了，调用当前线程的yield()方法，当前线程进入 `就绪状态`。\n\n> 只有处于就绪状态的线程才有机会转到运行状态。\n\n### 4. 阻塞(BLOCKED)\n\n阻塞状态是指线程因为某些原因放弃CPU，暂时停止运行。当线程处于阻塞状态时，Java虚拟机不会给线程分配CPU，直到线程重新进入就绪状态，它才会有机会获得运行状态。\n\n阻塞状态分为三种:\n\n1. 等待阻塞:运行的线程执行wait（）方法，JVM会把该线程放入等待池中。\n\n2. 同步阻塞:运行的线程在获取对象同步锁时，若该同步锁被别的线程占用，则JVM会把线程放入锁池中。\n\n3. 其他阻塞:运行的线程执行Sleep（）方法，或者发出I/O请求时，JVM会把线程设为阻塞状态。当Sleep（）状态超时、或者I/O处理完毕时，线程重新转入就绪状态。\n\n### 5. 死亡(DEAD)\n\n当线程执行完run()方法中的代码，或者遇到了未捕获的异常，就会退出run()方法，此时就进入死亡状态，该线程结束生命周期，等待JVM GC 回收内存。\n","tags":["Concurrency"]},{"title":"重新认识 Java 8 HashMap","url":"%2F2018%2F03%2F12%2Fhashmap%2F","content":"\n## 摘要\n\nHashMap是Java程序员使用频率最高的用于映射(键值对)处理的数据类型。随着JDK（Java Developmet Kit）版本的更新，JDK1.8对HashMap底层的实现进行了优化，例如引入红黑树的数据结构和扩容的优化等。本文结合JDK1.7和JDK1.8的区别，深入探讨HashMap的结构实现和功能原理。\n\n## 简介\n\nJava为数据结构中的映射定义了一个接口java.util.Map，此接口主要有四个常用的实现类，分别是HashMap、Hashtable、LinkedHashMap和TreeMap，类继承关系如下图所示：\n\n![类继承关系](/images/hashmap/java.util.map类图.png)\n\n下面针对各个实现类的特点做一些说明：\n\n1. HashMap：它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。\n\n2. Hashtable：Hashtable是遗留类，很多映射的常用功能与HashMap类似，不同的是它承自Dictionary类，并且是线程安全的，任一时间只有一个线程能写Hashtable，并发性不如ConcurrentHashMap，因为ConcurrentHashMap引入了分段锁。Hashtable不建议在新代码中使用，不需要线程安全的场合可以用HashMap替换，需要线程安全的场合可以用ConcurrentHashMap替换。\n\n3. LinkedHashMap：LinkedHashMap是HashMap和双向链表合二为一的HashMap一个子类，保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。\n\n4. TreeMap：TreeMap实现SortedMap接口，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator遍历TreeMap时，得到的记录是排过序的。如果使用排序的映射，建议使用TreeMap。在使用TreeMap时，key必须实现Comparable接口或者在构造TreeMap传入自定义的Comparator，否则会在运行时抛出java.lang.ClassCastException类型的异常。\n\n对于上述四种Map类型的类，要求映射中的key是不可变对象。不可变对象是该对象在创建后它的哈希值不会被改变。如果对象的哈希值发生变化，Map对象很可能就定位不到映射的位置了。\n\n通过上面的比较，我们知道了HashMap是Java的Map家族中一个普通成员，鉴于它可以满足大多数场景的使用条件，所以是使用频度最高的一个。下文我们主要结合源码，从存储结构、常用方法分析、扩容以及安全性等方面深入讲解HashMap的工作原理。\n\n<!-- more -->\n\n## 内部实现\n\n搞清楚HashMap，首先需要知道HashMap是什么，即它的存储结构-字段；其次弄明白它能干什么，即它的功能实现-方法。下面我们针对这两个方面详细展开讲解。\n\n### 存储结构-字段\n\n从结构实现来讲，HashMap是数组+链表+红黑树（JDK1.8增加了红黑树部分）实现的，如下如所示。\n\n![存储结构-字段](/images/hashmap/hashMap内存结构图.png)\n\n这里需要讲明白两个问题：数据底层具体存储的是什么？\b这样的存储方式有什么\b优点呢？\n\n1. 从源码可知，HashMap类中有一个非常重要的字段，就是 Node[] table，即哈希桶数组，明显它是一个Node的数组。我们来看Node[JDK1.8]是何物。\n\n```java\nstatic class Node<K,V> implements Map.Entry<K,V> {\n    final int hash;    //用来定位数组索引位置\n    final K key;\n    V value;\n    Node<K,V> next;   //链表的下一个node\n    Node(int hash, K key, V value, Node<K,V> next) { ... }\n    public final K getKey(){ ... }\n    public final V getValue() { ... }\n    public final String toString() { ... }\n    public final int hashCode() { ... }\n    public final V setValue(V newValue) { ... }\n    public final boolean equals(Object o) { ... }\n}\n```\n\nNode是HashMap的一个内部类，实现了Map.Entry接口，本质是就是一个映射(键值对)。上图中的每个黑色圆点就是一个Node对象。\n2. HashMap就是使用哈希表来存储的。哈希表为解决冲突，可以采用开放地址法和链地址法等来解决问题，Java中HashMap采用了链地址法。链地址法，简单来说，就是数组加链表的结合。在每个数组元素上都一个链表结构，当数据被Hash后，得到数组下标，把数据放在对应下标元素的链表上。例如程序执行下面代码：\n\n```java\nmap.put(\"key\", \"value\");\n```\n\n系统将调用\"key\"这个key的hashCode()方法得到其hashCode 值（该方法适用于每个Java对象），然后再通过Hash算法的后两步运算（高位运算和取模运算，下文有介绍）来定位该键值对的存储位置，有时两个key会定位到相同的位置，表示发生了Hash碰撞。当然Hash算法计算结果越分散均匀，Hash碰撞的概率就越小，map的存取效率就会越高。\n\n如果哈希桶数组很大，即使较差的Hash算法也会比较分散，如果哈希桶数组数组很小，即使好的Hash算法也会出现较多碰撞，所以就需要在空间成本和时间成本之间权衡，其实就是在根据实际情况确定哈希桶数组的大小，并在此基础上设计好的hash算法减少Hash碰撞。那么通过什么方式来控制map使得Hash碰撞的概率又小，哈希桶数组（Node[] table）占用空间又少呢？答案就是好的Hash算法和扩容机制。\n\n在理解Hash和扩容流程之前，我们得先了解下HashMap的几个字段。从HashMap的默认构造函数源码可知，构造函数就是对下面几个字段进行初始化，源码如下：\n\n```java\n  int threshold;             // 所能容纳的key-value对极限\n  final float loadFactor;    // 负载因子\n  int modCount;\n  int size;\n```\n\n首先，Node[] table的初始化长度length(默认值是16)，Load factor为负载因子(默认值是0.75)，threshold是HashMap所能容纳的最大数据量的Node(键值对)个数。threshold = length * Load factor。也就是说，在数组定义好长度之后，负载因子越大，所能容纳的键值对个数越多。\n\n结合负载因子的定义公式可知，threshold就是在此Load factor和length(数组长度)对应下允许的最大元素数目，超过这个数目就重新resize(扩容)，扩容后的HashMap容量是之前容量的两倍。默认的负载因子0.75是对空间和时间效率的一个平衡选择，建议大家不要修改，除非在时间和空间比较特殊的情况下，如果内存空间很多而又对时间效率要求很高，可以降低负载因子Load factor的值；相反，如果内存空间紧张而对时间效率要求不高，可以增加负载因子loadFactor的值，这个值可以大于1。\n\nsize这个字段其实很好理解，就是HashMap中实际存在的键值对数量。注意和table的长度length、容纳最大键值对数量threshold的区别。而modCount字段主要用来记录HashMap内部结构发生变化的次数，主要用于迭代的快速失败。强调一点，内部结构发生变化指的是结构发生变化，例如put新键值对，但是某个key对应的value值被覆盖不属于结构变化。\n\n在HashMap中，哈希桶数组table的长度length大小必须为2的n次方(一定是合数)，这是一种非常规的设计，常规的设计是把桶的大小设计为素数。相对来说素数导致冲突的概率要小于合数，具体证明可以参考[http://blog.csdn.net/liuqiyao_01/article/details/14475159](http://blog.csdn.net/liuqiyao_01/article/details/14475159)，Hashtable初始化桶大小为11，就是桶大小设计为素数的应用（Hashtable扩容后不能保证还是素数）。HashMap采用这种非常规设计，主要是为了在取模和扩容时做优化，同时为了减少冲突，HashMap定位哈希桶索引位置时，也加入了高位参与运算的过程。\n\n这里存在一个问题，即使负载因子和Hash算法设计的再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响HashMap的性能。于是，在JDK1.8版本中，对数据结构做了进一步的优化，引入了红黑树。而当链表长度太长（默认超过8）时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高HashMap的性能，其中会用到红黑树的插入、删除、查找等算法。本文不再对红黑树展开讨论，想了解更多红黑树数据结构的工作原理可以参考[http://blog.csdn.net/v_july_v/article/details/6105630](http://blog.csdn.net/v_july_v/article/details/6105630)。\n\n### 功能实现-方法\n\nHashMap的内部功能实现很多，本文主要从根据key获取哈希桶数组索引位置、put方法的详细执行、扩容过程三个具有代表性的点深入展开讲解。\n\n#### 1. 确定哈希桶数组索引位置\n\n不管增加、删除、查找键值对，定位到哈希桶数组的位置都是很关键的第一步。前面说过HashMap的数据结构是数组和链表的结合，所以我们当然希望这个HashMap里面的元素位置尽量分布均匀些，尽量使得每个位置上的元素数量只有一个，那么当我们用hash算法求得这个位置的时候，马上就可以知道对应位置的元素就是我们要的，不用遍历链表，大大优化了查询的效率。HashMap定位数组索引位置，直接决定了hash方法的离散性能。先看看源码的实现(方法一+方法二):\n\n```java\n//方法一：\nstatic final int hash(Object key) {   //jdk1.8 & jdk1.7\n     int h;\n     // h = key.hashCode() 为第一步 取hashCode值\n     // h ^ (h >>> 16)  为第二步 高位参与运算\n     return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);\n}\n//方法二：\nstatic int indexFor(int h, int length) {  //jdk1.7的源码，jdk1.8没有这个方法，但是实现原理一样的\n     return h & (length-1);  //第三步 取模运算\n}\n```\n\n这里的Hash算法本质上就是三步：`取key的hashCode值`、`高位运算`、`取模运算`。\n\n对于任意给定的对象，只要它的hashCode()返回值相同，那么程序调用方法一所计算得到的Hash码值总是相同的。我们首先想到的就是把hash值对数组长度取模运算，这样一来，元素的分布相对来说是比较均匀的。但是，模运算的消耗还是比较大的，在HashMap中是这样做的：调用方法二来计算该对象应该保存在table数组的哪个索引处。\n\n这个方法非常巧妙，它通过h & (table.length -1)来得到该对象的保存位，而HashMap底层数组的长度总是2的n次方，这是HashMap在速度上的优化。当length总是2的n次方时，h& (length-1)运算等价于对length取模，也就是h%length，但是&比%具有更高的效率。\n\n在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h >>> 16)，主要是从速度、功效、质量来考虑的，这么做可以在\b数组table的length比较小的时候，也能保证考虑到高低Bit都参与到Hash的计算中，同时不会有太大的开销。\n\n下面举例说明下，n为table的长度。\n\n![hashMap哈希算法例图](/images/hashmap/hashMap哈希算法例图.png)\n\n#### 2. 分析HashMap的put方法\n\nHashMap的put方法执行过程可以通过下图来理解，自己有兴趣可以去对比源码更清楚地研究学习。\n\n![hashMap put方法执行流程图](/images/hashmap/hashMap-put方法执行流程图.png)\n\n1. 判断键值对数组table[i]是否为空或为null，否则执行resize()进行扩容；\n\n2. 根据键值key计算hash值得到插入的数组索引i，如果table[i]==null，直接新建节点添加，转向⑥，如果table[i]不为空，\b转向③；\n\n3. 判断\btable[i]的首个元素是否和key一样，如果相同直接覆盖value，否则转向④，这里的相同指的是hashCode以及equals；\n\n4. 判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向⑤；\n\n5. 遍历table[i]，判断链表长度是否大于8，大于8的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；\b遍历过程中若发现key已经存在直接覆盖value即可；\n\n6. 插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold，如果超过，进行扩容。\n\nJDK1.8HashMap的put方法源码如下:\n\n```java\n public V put(K key, V value) {\n     // 对key的hashCode()做hash\n     return putVal(hash(key), key, value, false, true);\n }\n\n final V putVal(int hash, K key, V value, boolean onlyIfAbsent,\n                boolean evict) {\n     Node<K,V>[] tab; Node<K,V> p; int n, i;\n     // 步骤①：tab为空则创建\n     if ((tab = table) == null || (n = tab.length) == 0)\n         n = (tab = resize()).length;\n     // 步骤②：计算index，并对null做处理\n     if ((p = tab[i = (n - 1) & hash]) == null)\n         tab[i] = newNode(hash, key, value, null);\n     else {\n         Node<K,V> e; K k;\n         // 步骤③：节点key存在，直接覆盖value\n         if (p.hash == hash &&\n             ((k = p.key) == key || (key != null && key.equals(k))))\n             e = p;\n         // 步骤④：判断该链为红黑树\n         else if (p instanceof TreeNode)\n             e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);\n         // 步骤⑤：该链为链表\n         else {\n             for (int binCount = 0; ; ++binCount) {\n                 if ((e = p.next) == null) {\n                     p.next = newNode(hash, key,value,null);\n                      //链表长度大于8转换为红黑树进行处理\n                     if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st\n                         treeifyBin(tab, hash);\n                     break;\n                 }\n                  // key已经存在直接覆盖value\n                 if (e.hash == hash &&\n                     ((k = e.key) == key || (key != null && key.equals(k))))\n                            break;\n                 p = e;\n             }\n         }\n         if (e != null) { // existing mapping for key\n             V oldValue = e.value;\n             if (!onlyIfAbsent || oldValue == null)\n                 e.value = value;\n             afterNodeAccess(e);\n             return oldValue;\n         }\n     }\n     ++modCount;\n     // 步骤⑥：超过最大容量 就扩容\n     if (++size > threshold)\n         resize();\n     afterNodeInsertion(evict);\n     return null;\n }\n```\n\n#### 3. 扩容机制\n\n扩容(resize)就是重新计算容量，向HashMap对象里不停的添加元素，而HashMap对象内部的数组无法装载更多的元素时，对象就需要扩大数组的长度，以便能装入更多的元素。当然Java里的数组是无法自动扩容的，方法是使用一个新的数组代替已有的容量小的数组，就像我们用一个小桶装水，如果想装更多的水，就得换大水桶。\n\n我们分析下resize的源码，鉴于JDK1.8融入了红黑树，较复杂，为了便于理解我们仍然使用JDK1.7的代码，好理解一些，本质上区别不大，具体区别后文再说。\n\n```java\nvoid resize(int newCapacity) {   //传入新的容量\n    Entry[] oldTable = table;    //引用扩容前的Entry数组\n    int oldCapacity = oldTable.length;\n    if (oldCapacity == MAXIMUM_CAPACITY) {  //扩容前的数组大小如果已经达到最大(2^30)了\n        threshold = Integer.MAX_VALUE; //修改阈值为int的最大值(2^31-1)，这样以后就不会扩容了\n        return;\n    }\n    Entry[] newTable = new Entry[newCapacity];  //初始化一个新的Entry数组\n    transfer(newTable);                         //！！将数据转移到新的Entry数组里\n    table = newTable;                           //HashMap的table属性引用新的Entry数组\n    threshold = (int)(newCapacity * loadFactor);//修改阈值\n}\n```\n\n这里就是使用一个容量更大的数组来代替已有的容量小的数组，transfer()方法将原有Entry数组的元素拷贝到新的Entry数组里。\n\n```java\nvoid transfer(Entry[] newTable) {\n    Entry[] src = table;                   //src引用了旧的Entry数组\n    int newCapacity = newTable.length;\n    for (int j = 0; j < src.length; j++) { //遍历旧的Entry数组\n        Entry<K,V> e = src[j];             //取得旧Entry数组的每个元素\n        if (e != null) {\n            src[j] = null;//释放旧Entry数组的对象引用（for循环后，旧的Entry数组不再引用任何对象）\n            do {\n                Entry<K,V> next = e.next;\n                int i = indexFor(e.hash, newCapacity); //！！重新计算每个元素在数组中的位置\n                e.next = newTable[i]; //标记[1]\n                newTable[i] = e;      //将元素放在数组上\n                e = next;             //访问下一个Entry链上的元素\n            } while (e != null);\n        }\n    }\n}\n```\n\nnewTable[i]的引用赋给了e.next，也就是使用了单链表的头插入方式，同一位置上新元素总会被放在链表的头部位置；这样先放在一个索引上的元素终会被放到Entry链的尾部(如果发生了hash冲突的话），这一点和Jdk1.8有区别，下文详解。在旧数组中同一条Entry链上的元素，通过重新计算索引位置后，有可能被放到了新数组的不同位置上。\n\n下面举个例子说明下扩容过程。假设了我们的hash算法就是简单的用key mod 一下表的大小（也就是数组的长度）。其中的\b哈希桶数组table的size=2， 所以key = 3、7、5，put顺序依次为 5、7、3。在mod 2以后都冲突在table[1]这里了。这里假设负载因子 loadFactor=1，即当键值对的实际大小size 大于 table的实际大小时进行扩容。接下来的三个步骤是哈希桶数组 resize成4，然后所有的Node重新rehash的过程。\n\n![jdk1.7扩容例图](/images/hashmap/jdk1.7扩容例图.png)\n\n下面我们讲解下JDK1.8做了哪些优化。经过观测可以发现，我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。看下图可以明白这句话的意思，n为table的长度，图（a）表示扩容前的key1和key2两种key确定索引位置的示例，图（b）表示扩容后key1和key2两种key确定索引位置的示例，其中hash1是key1对应的哈希与高位运算结果。\n\n![hashMap 1.8 哈希算法例图1](/images/hashmap/hashMap-1.8-哈希算法例图1.png)\n\n元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1bit(红色)，因此新的index就会发生这样的变化：\n\n![hashMap 1.8 哈希算法例图2](/images/hashmap/hashMap-1.8-哈希算法例图2.png)\n\n因此，我们在扩充HashMap的时候，不需要像JDK1.7的实现那样重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”，可以看看下图为16扩充为32的resize示意图：\n\n![jdk1.8 hashMap 扩容例图.png](/images/hashmap/jdk1.8-hashMap-扩容例图.png)\n\n这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。这一块就是JDK1.8新增的优化点。有一点注意区别，JDK1.7中rehash的时候，旧链表迁移新链表的时候，如果在新表的数组索引位置相同，则链表元素会倒置，但是从上图可以看出，JDK1.8不会倒置。有兴趣的同学可以研究下JDK1.8的resize源码，写的很赞，如下:\n\n```java\nfinal Node<K,V>[] resize() {\n    Node<K,V>[] oldTab = table;\n    int oldCap = (oldTab == null) ? 0 : oldTab.length;\n    int oldThr = threshold;\n    int newCap, newThr = 0;\n    if (oldCap > 0) {\n        // 超过最大值就不再扩充了，就只好随你碰撞去吧\n        if (oldCap >= MAXIMUM_CAPACITY) {\n            threshold = Integer.MAX_VALUE;\n            return oldTab;\n        }\n        // 没超过最大值，就扩充为原来的2倍\n        else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&\n                 oldCap >= DEFAULT_INITIAL_CAPACITY)\n            newThr = oldThr << 1; // double threshold\n    }\n    else if (oldThr > 0) // initial capacity was placed in threshold\n        newCap = oldThr;\n    else {               // zero initial threshold signifies using defaults\n        newCap = DEFAULT_INITIAL_CAPACITY;\n        newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);\n    }\n    // 计算新的resize上限\n    if (newThr == 0) {\n\n        float ft = (float)newCap * loadFactor;\n        newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ?\n                  (int)ft : Integer.MAX_VALUE);\n    }\n    threshold = newThr;\n    @SuppressWarnings({\"rawtypes\"，\"unchecked\"})\n        Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];\n    table = newTab;\n    if (oldTab != null) {\n        // 把每个bucket都移动到新的buckets中\n        for (int j = 0; j < oldCap; ++j) {\n            Node<K,V> e;\n            if ((e = oldTab[j]) != null) {\n                oldTab[j] = null;\n                if (e.next == null)\n                    newTab[e.hash & (newCap - 1)] = e;\n                else if (e instanceof TreeNode)\n                    ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);\n                else { // 链表优化重hash的代码块\n                    Node<K,V> loHead = null, loTail = null;\n                    Node<K,V> hiHead = null, hiTail = null;\n                    Node<K,V> next;\n                    do {\n                        next = e.next;\n                        // 原索引\n                        if ((e.hash & oldCap) == 0) {\n                            if (loTail == null)\n                                loHead = e;\n                            else\n                                loTail.next = e;\n                            loTail = e;\n                        }\n                        // 原索引+oldCap\n                        else {\n                            if (hiTail == null)\n                                hiHead = e;\n                            else\n                                hiTail.next = e;\n                            hiTail = e;\n                        }\n                    } while ((e = next) != null);\n                    // 原索引放到bucket里\n                    if (loTail != null) {\n                        loTail.next = null;\n                        newTab[j] = loHead;\n                    }\n                    // 原索引+oldCap放到bucket里\n                    if (hiTail != null) {\n                        hiTail.next = null;\n                        newTab[j + oldCap] = hiHead;\n                    }\n                }\n            }\n        }\n    }\n    return newTab;\n}\n```\n\n## 线程安全性\n\n在多线程使用场景中，应该尽量避免使用线程不安全的HashMap，而使用线程安全的ConcurrentHashMap。那么为什么说HashMap是线程不安全的，下面举例子说明在并发的多线程使用场景中使用HashMap可能造成死循环。代码例子如下(便于理解，仍然使用JDK1.7的环境)：\n\n```java\npublic class HashMapInfiniteLoop {\n\n    private static HashMap<Integer,String> map = new HashMap<Integer,String>(2，0.75f);\n    public static void main(String[] args) {\n        map.put(5， \"C\");\n\n        new Thread(\"Thread1\") {\n            public void run() {\n                map.put(7, \"B\");\n                System.out.println(map);\n            };\n        }.start();\n        new Thread(\"Thread2\") {\n            public void run() {\n                map.put(3, \"A);\n                System.out.println(map);\n            };\n        }.start();\n    }\n}\n```\n\n其中，map初始化为一个长度为2的数组，loadFactor=0.75，threshold=2*0.75=1，也就是说当put第二个key的时候，map就需要进行resize。\n\n通过设置断点让线程1和线程2同时debug到transfer方法(3.3小节代码块)的首行。注意此时两个线程已经成功添加数据。放开thread1的断点至transfer方法的“Entry next = e.next;” 这一行；然后放开线程2的的断点，让线程2进行resize。结果如下图。\n\n![jdk1.7 hashMap死循环例图1](/images/hashmap/jdk1.7-hashMap死循环例图1.png)\n\n注意，Thread1的 e 指向了key(3)，而next指向了key(7)，其在线程二rehash后，指向了线程二重组后的链表。\n\n线程一被调度回来执行，先是执行 newTalbe[i] = e， 然后是e = next，导致了e指向了key(7)，而下一次循环的next = e.next导致了next指向了key(3)。\n\n![jdk1.7 hashMap死循环例图2](/images/hashmap/jdk1.7-hashMap死循环例图2.png)\n\n![jdk1.7 hashMap死循环例图3](/images/hashmap/jdk1.7-hashMap死循环例图3.png)\n\ne.next = newTable[i] 导致 key(3).next 指向了 key(7)。注意：此时的key(7).next 已经指向了key(3)， 环形链表就这样出现了。\n\n![jdk1.7 hashMap死循环例图4](/images/hashmap/jdk1.7-hashMap死循环例图4.png)\n\n于是，当我们用线程一调用map.get(11)时，悲剧就出现了—— `Infinite Loop` 。\n\n## JDK1.8与JDK1.7的性能对比\n\nHashMap中，如果key经过hash算法得出的数组索引位置全部不相同，即Hash算法非常好，那样的话，getKey方法的时间复杂度就是O(1)，如果Hash算法技术的结果碰撞非常多，假如Hash算极其差，所有的Hash算法结果得出的索引位置一样，那样所有的键值对都集中到一个桶中，或者在一个链表中，或者在一个红黑树中，时间复杂度分别为O(n)和O(logn)。 鉴于JDK1.8做了多方面的优化，总体性能优于JDK1.7。\n\n## 小结\n\n1. 扩容是一个特别耗性能的操作，所以当程序员在使用HashMap的时候，估算map的大小，初始化的时候给一个大致的数值，避免map进行频繁的扩容。\n\n2. 负载因子是可以修改的，也可以大于1，但是建议不要轻易修改，除非情况非常特殊。\n\n3. HashMap是线程不安全的，不要在并发的环境中同时操作HashMap，建议使用ConcurrentHashMap。\n\n4. JDK1.8引入红黑树大程度优化了HashMap的性能。\n","tags":["Java"]},{"title":"Ubuntu apt 更新和清理","url":"%2F2018%2F02%2F08%2Fapt-shell%2F","content":"\n## `apt.sh`更新清理软件\n\n```sh\nsudo aptitude update\nsudo aptitude upgrade -y\nsudo aptitude autoclean -y\nsudo aptitude clean\n\necho \"\\033[31m aptitude end \\033[0m\"\n\nsudo apt-get update\nsudo apt-get upgrade -y\nsudo apt-get autoremove -y\nsudo apt-get autoclean\nsudo apt-get clean\n\necho \"\\033[31m apt-get end \\033[0m\"\n```\n\n## `ls.sh`查看deb包缓存\n\n```sh\ncd /var/cache/apt/archives\npwd\necho \"===================\"\nls | grep deb\n```\n","tags":["shell"]},{"title":"maven依赖非pom导入的额外Jar包","url":"%2F2018%2F02%2F08%2Fmaven-extdirs%2F","content":"\n非pom导入的额外Jar包, 在 `maven clean install` 的时候是会报错的, `程序包 *.*.* 不存在`, 需要配置 `pom.xml`。\n\n> 我们将额外Jar包放在一个统一的目录 `${dirs}` 下面。\n\n`dependency` 加上 `<scope>system</scope>` 以及 Jar包位置\n\n```xml\n<dependency>\n  <groupId>${groupId}</groupId>\n  <artifactId>${artifactId}</artifactId>\n  <version>${version}</version>\n  <scope>system</scope>\n  <systemPath>${project.basedir}/${dirs}/target1.jar</systemPath>\n</dependency>\n\n<dependency>\n  <groupId>${groupId}</groupId>\n  <artifactId>${artifactId}</artifactId>\n  <version>${version}</version>\n  <scope>system</scope>\n  <systemPath>${project.basedir}/${dirs}/target2.jar</systemPath>\n</dependency>\n```\n\n<!-- more -->\n\n`build` 时, 把Jar包加入 `manifestEntries` - `Class-Path` (我的配置文件将配置文件, lib, webApp等都打包在Jar包外)\n\n```xml\n<build>\n  <plugins>\n\n    <plugin>\n      <groupId>org.apache.maven.plugins</groupId>\n      <artifactId>maven-dependency-plugin</artifactId>\n      <executions>\n        <execution>\n          <id>copy-dependencies</id>\n          <phase>package</phase>\n          <goals>\n            <goal>copy-dependencies</goal>\n          </goals>\n          <configuration>\n            <outputDirectory>${project.build.directory}/lib</outputDirectory>\n            <overWriteReleases>false</overWriteReleases>\n            <overWriteSnapshots>true</overWriteSnapshots>\n            <excludeScope>provided</excludeScope>\n          </configuration>\n        </execution>\n      </executions>\n    </plugin>\n\n    <plugin>\n      <groupId>org.apache.maven.plugins</groupId>\n      <artifactId>maven-compiler-plugin</artifactId>\n      <configuration>\n        <source>${java.version}</source>\n        <target>${java.version}</target>\n        <compilerArgument>-proc:none</compilerArgument>\n        <encoding>UTF-8</encoding>\n      </configuration>\n    </plugin>\n\n    <plugin>\n      <groupId>org.apache.maven.plugins</groupId>\n      <artifactId>maven-resources-plugin</artifactId>\n      <version>2.6</version>\n      <executions>\n\n        <execution>\n          <id>copy-web</id>\n          <phase>package</phase>\n          <goals>\n            <goal>copy-resources</goal>\n          </goals>\n          <configuration>\n            <encoding>UTF-8</encoding>\n            <outputDirectory>${project.build.directory}/webApp</outputDirectory>\n            <resources>\n              <resource>\n                <directory>webApp</directory>\n              </resource>\n            </resources>\n          </configuration>\n        </execution>\n\n        <execution>\n          <id>copy-conf</id>\n          <phase>package</phase>\n          <goals>\n            <goal>copy-resources</goal>\n          </goals>\n          <configuration>\n            <encoding>UTF-8</encoding>\n            <outputDirectory>${project.build.directory}</outputDirectory>\n            <resources>\n              <resource>\n                <directory>src/main/resources</directory>\n              </resource>\n            </resources>\n          </configuration>\n        </execution>\n\n        <execution>\n          <id>copy-authFile</id>\n          <phase>package</phase>\n          <goals>\n            <goal>copy-resources</goal>\n          </goals>\n          <configuration>\n            <encoding>UTF-8</encoding>\n            <outputDirectory>${project.build.directory}</outputDirectory>\n            <resources>\n              <resource>\n                <directory>.</directory>\n                <include>settings.txt</include>\n              </resource>\n            </resources>\n          </configuration>\n        </execution>\n\n        <execution>\n          <id>copy-lib</id>\n          <phase>package</phase>\n          <goals>\n            <goal>copy-resources</goal>\n          </goals>\n          <configuration>\n            <outputDirectory>${project.build.directory}/lib</outputDirectory>\n            <resources>\n              <resource>\n                <directory>${dirs}</directory>\n              </resource>\n            </resources>\n          </configuration>\n        </execution>\n\n      </executions>\n    </plugin>\n\n    <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-jar-plugin</artifactId>\n        <version>2.4</version>\n        <configuration>\n          <classesDirectory>target/classes/</classesDirectory>\n          <archive>\n            <addMavenDescriptor>false</addMavenDescriptor>\n            <manifest>\n              <addClasspath>true</addClasspath>\n              <classpathPrefix>lib/</classpathPrefix>\n              <mainClass>${mainClass}</mainClass>\n            </manifest>\n            <manifestEntries>\n              <Class-Path>\n                lib/target1.jar lib/target2.jar\n              </Class-Path>\n            </manifestEntries>\n          </archive>\n        </configuration>\n      </plugin>\n  </plugins>\n</build>\n```\n","tags":["maven"]},{"title":"nodejs","url":"%2F2017%2F09%2F08%2Fnodejs%2F","content":"\n## npm使用淘宝源\n\n```sh\nnpm config set registry https://registry.npmmirror.com/\n```\n\n## 修改源地址为官方源\n\n```sh\nnpm config set registry https://registry.npmjs.org/\n```\n\n## 使用cnpm\n\n```sh\nnpm install -g cnpm --registry=https://registry.npmmirror.com/\n```\n\n## Fixing npm permissions\n\n```sh\nsudo chown -R $(whoami) $(npm config get prefix)/{lib/node_modules,bin,share}\n```\n\n*This changes the permissions of the sub-folders used by npm and some other tools (lib/node_modules, bin, and share).*\n\n## 升级 node.js\n\n```sh\nnpm install -g n\n\nn stable\n```\n\n## 升级 npm\n\n```sh\n(c)npm -g install (c)npm\n```\n","tags":["Node.js"]},{"title":"JAVA 中 CAS 原理详解","url":"%2F2017%2F09%2F04%2FCAS%2F","content":"\n## 什么是 CAS\n\n`CAS`, `Compare and Swap` 即比较并替换, CAS 有三个操作数：内存值 V、旧的预期值 A、要修改的值 B, 当且仅当预期值 A 和内存值 V 相同时, 将内存值修改为 B 并返回 true, 否则什么都不做并返回 false。\n\njava.util.concurrent.atomic 包下的原子操作类都是基于 CAS 实现的, 接下去我们通过 AtomicInteger 来看看是如何通过 CAS 实现原子操作的：\n\n```java\npublic class AtomicInteger extends Number implements java.io.Serializable {\n    // setup to use Unsafe.compareAndSwapInt for updates\n    private static final Unsafe unsafe = Unsafe.getUnsafe();\n    private static final long valueOffset;\n\n    static {\n        try {\n            valueOffset = unsafe.objectFieldOffset\n                (AtomicInteger.class.getDeclaredField(\"value\"));\n        } catch (Exception ex) { throw new Error(ex); }\n    }\n\n    private volatile int value;\n    public final int get() {return value;}\n}\n```\n\n1. Unsafe 是 CAS 的核心类, Java 无法直接访问底层操作系统, 而是通过本地（native）方法来访问。不过尽管如此, JVM 还是开了一个后门, JDK 中有一个类 Unsafe, 它提供了硬件级别的原子操作。\n\n2. valueOffset 表示的是变量值在内存中的偏移地址, 因为 Unsafe 就是根据内存偏移地址获取数据的原值的。\n\n3. value 是用 volatile 修饰的, 保证了多线程之间看到的 value 值是同一份。\n\n<!-- more -->\n\n接下去, 我们看看 AtomicInteger 是如何实现并发下的累加操作：\n\n```java\n//jdk1.8实现\npublic final int getAndAdd(int delta) {    \n    return unsafe.getAndAddInt(this, valueOffset, delta);\n}\n\npublic final int getAndAddInt(Object var1, long var2, int var4) {\n    int var5;\n    do {\n        var5 = this.getIntVolatile(var1, var2);\n    } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4));\n    return var5;\n}\n```\n\n假设现在线程 A 和线程 B 同时执行 getAndAdd 操作：\n\n1. AtomicInteger 里面的 value 原始值为 3, 即主内存中 AtomicInteger 的 value 为 3, 根据 Java 内存模型, 线程 A 和线程 B 各自持有一份 value 的副本, 值为 3。\n\n2. 线程 A 通过 getIntVolatile(var1, var2) 方法获取到 value 值 3, 线程切换, 线程 A 挂起。\n\n3. 线程 B 通过 getIntVolatile(var1, var2) 方法获取到 value 值 3, 并利用 compareAndSwapInt 方法比较内存值也为 3, 比较成功, 修改内存值为 2, 线程切换, 线程 B 挂起。\n\n4. 线程 A 恢复, 利用 compareAndSwapInt 方法比较, 发手里的值 3 和内存值 2 不一致, **此时 value 正在被另外一个线程修改, 线程 A 不能修改 value 值**。\n\n5. 线程的 compareAndSwapInt 实现, 循环判断, 重新获取 value 值, 因为 value 是 volatile 变量, 所以线程对它的修改, 线程 A 总是能够看到。线程 A 继续利用 \ncompareAndSwapInt 进行比较并替换, 直到 compareAndSwapInt 修改成功返回 true。\n\n整个过程中, 利用 CAS 保证了对于 value 的修改的线程安全性。\n\n## CAS 缺点\n\n### ABA 问题\n\n如果变量 V 初次读取的时候是 A, 并且在准备赋值的时候检查到它仍然是 A, 那能说明它的值没有被其他线程修改过了吗？如果在这段期间它的值曾经被改成了 B, 然后又改回 A, 那 CAS 操作就会误认为它从来没有被修改过。针对这种情况, java 并发包中提供了一个带有标记的原子引用类 \"AtomicStampedReference\", 它可以通过控制变量值的版本来保证 CAS 的正确性。\n\n### 循环时间长开销大\n\n自旋 CAS 如果长时间不成功, 会给 CPU 带来非常大的执行开销。如果 JVM 能支持处理器提供的 pause 指令那么效率会有一定的提升, pause 指令有两个作用, 第一它可以延迟流水线执行指令（de-pipeline）, 使 CPU 不会消耗过多的执行资源, 延迟的时间取决于具体实现的版本, 在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起 CPU 流水线被清空（CPU pipeline flush）, 从而提高 CPU 的执行效率。\n\n### 只能保证一个共享变量的原子操作。\n\n当对一个共享变量执行操作时, 我们可以使用循环 CAS 的方式来保证原子操作, 但是对多个共享变量操作时, 循环 CAS 就无法保证操作的原子性, 这个时候就可以用锁, 或者有一个取巧的办法, 就是把多个共享变量合并成一个共享变量来操作。比如有两个共享变量 i＝2,j=a, 合并一下 ij=2a, 然后用 CAS 来操作 ij。从 Java1.5 开始 JDK 提供了 AtomicReference 类来保证引用对象之间的原子性, 你可以把多个变量放在一个对象里来进行 CAS 操作。\n","tags":["Concurrency"]},{"title":"Synchronized 了解","url":"%2F2017%2F09%2F04%2FSynchronized%2F","content":"\n## 说明\nsychronized 一般用来修饰一个方法或者一个代码块。利用 sychronized 实现同步的基础：java 中每一个对象都可以作为锁。具体表现为以下 3 中形式。\n\n1. 修饰一个代码块, 被修饰的代码块称为同步语句块, 其作用的范围是大括号{}括起来的代码, 作用的对象是调用这个代码块的对象；\n\n2. 修饰一个方法, 被修饰的方法称为同步方法, 其作用的范围是整个方法, 作用的对象是调用这个方法的对象；\n\n3. 修改一个静态的方法, 其作用的范围是整个静态方法, 作用的对象是这个类的所有对象；\n\n4. 修改一个类, 其作用的范围是synchronized后面括号括起来的部分, 作用主的对象是这个类的所有对象。\n\n一个线程访问一个对象的synchronized代码块时, 别的线程可以访问该对象的非synchronized代码块而不受阻塞。\n\n## 总结\n\n1. 无论synchronized关键字加在方法上还是对象上, 如果它作用的对象是非静态的, 则它取得的锁是对象；如果synchronized作用的对象是一个静态方法或一个类, 则它取得的锁是对类, 该类所有的对象同一把锁。\n\n2. 每个对象只有一个锁（lock）与之相关联, 谁拿到这个锁谁就可以运行它所控制的那段代码。\n\n3. 实现同步是要很大的系统开销作为代价的, 甚至可能造成死锁, 所以尽量避免无谓的同步控制。\n","tags":["Concurrency"]},{"title":"ReentrantLock - 可重入互斥锁","url":"%2F2017%2F09%2F02%2FReentrantLock%2F","content":"\n## 锁的可重入性\n\n先举例来说明锁的可重入性：\n\n```java\npublic class UnReentrant{\n    Lock lock = new Lock();\n    public void outer(){\n        lock.lock();\n        inner();\n        lock.unlock();\n    }\n    public void inner(){\n        lock.lock();\n        //do something\n        lock.unlock();\n    }\n}\n```\n\nouter 中调用了 inner, outer 先锁住了 lock, 这样 inner 就不能再获取 lock。其实调用 outer 的线程已经获取了 lock 锁, 但是不能在 inner 中重复利用已经获取的锁资源, 这种锁即称之为 `不可重入` 。通常也称为 `自旋锁` 。相对来说, 可重入就意味着：线程可以进入任何一个它已经拥有的锁所同步着的代码块。\n\n## ReentrantLock 相比 synchronized 有如下几个优势\n\n1. 可以反复进入, 同一个线程获得几个锁, 在释放的时候也需要同等释放, 要不然会死锁的（其实 synchronized 也是可重入的）；\n\n2. 支持锁中断 lockInterruptiblity(), 防死锁, 优先响应中断；\n\n3. 支持限时获取锁, lock.tryLock(int,TimeUnit); 在给定的时间范围捏尝试获取锁；\n\n4. 支持尝试获取锁, tryLock 不带参数, 如果获取不到则立刻返回 false, 而不等待锁；\n\n5. 公平锁 ReentrantLock(boolean fair), 排队获取锁, 性能相对低下；\n\n6. 配合 Condition, 可以让线程在合适的时间等待, 得到通知后继续执行。\n","tags":["Concurrency"]},{"title":"类加载机制","url":"%2F2017%2F08%2F28%2Fclass-loader%2F","content":"\n## 类的加载\n\n 其中类加载的过程包括了加载、验证、准备、解析、初始化、使用、卸载七个阶段。验证、准备、解析三个阶段统称为连接。\n\n 在这七个阶段中, 加载、验证、准备和初始化和卸载这五个阶段发生的顺序是确定的, 而解析阶段则不一定, 它在某些情况下可以在初始化阶段之后开始, 这是为了支持 Java 语言的运行时绑定（也成为动态绑定或晚期绑定）。另外注意这里的几个阶段是按顺序开始, 而不是按顺序进行或完成, 因为这些阶段通常都是互相交叉地混合进行的, 通常在一个阶段执行的过程中调用或激活另一个阶段。\n\n### 加载\n\n加载时类加载过程的第一个阶段, 在加载阶段, 虚拟机需要完成以下三件事情：\n\n1. 通过一个类的全限定名来获取其定义的二进制字节流。\n2. 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。\n3. 在 Java 堆中生成一个代表这个类的 java.lang.Class 对象, 作为对方法区中这些数据的访问入口。\n\n在[类加载器](#类加载器)中将详细描述使用类加载器加载类的过程\n\n<!-- more -->\n\n### 连接\n\n- 验证：是否有正确的内部结构, 并和其他类协调一致\n- 准备：负责为类的静态成员分配内存, 并设置默认初始化值\n- 解析：将类的二进制数据中的符号引用替换为直接引用\n\n### 初始化\n\n类会在首次被 “主动使用” 时执行初始化, 为类（静态）变量赋予正确的初始值。在 Java 代码中, 一个正确的初始值是通过类变量初始化语句或者静态初始化块给出的。\n\n初始化一个类包括两个步骤：\n\n- 如果类存在直接父类的话, 且直接父类还没有被初始化, 则先初始化其直接父类\n- 如果类存在一个初始化方法, 就执行此方法\n\n注：初始化接口并不需要初始化它的父接口。\n\n## 类加载器\n\n基本上所有的类加载器都是 java.lang.ClassLoader类的一个实例。\n\nJava 中的类加载器大致可以分成两类, 一类是系统提供的, 另外一类则是由 Java 应用开发人员编写的。系统提供的类加载器主要有下面三个：\n- `引导类加载器（bootstrap class loader）`：负责加载 $JAVA_HOME 中 jre/lib/rt.jar 里所有的 class 或 Xbootclassoath 选项指定的 jar 包。由 C++ 实现, 不是 ClassLoader 子类。\n- `扩展类加载器（extensions class loader）`：负责加载 java 平台中扩展功能的一些 jar 包, 包括 $JAVA_HOME 中 jre/lib/ext/*.jar 或 -Djava.ext.dirs 指定目录下的 jar 包。\n- `应用类加载器（App class loader）`：它根据 Java 应用的类路径（CLASSPATH）来加载 Java 类。一般来说, Java 应用的类都是由它来完成加载的。\n\nJVM 中除了根类加载器之外的所有类的加载器都是 ClassLoader 子类的实例, 通过重写 ClassLoader 中的方法, 实现自定义的类加载器。\n\n## 双亲委派模型\n\n除了引导类加载器之外, 所有的类加载器都有一个父类加载器, 可以通过 getParent()方法得到。于系统提供的类加载器来说, 应用类加载器的父类加载器是扩展类加载器, 而扩展类加载器的父类加载器是引导类加载器；对于开发人员编写的类加载器来说, 其父类加载器是加载此类加载器 Java 类的类加载器。\n\n它们之间的层次关系被称为类加载器的`双亲委派模型`。\n\n### 双亲委派模型过程\n\n某个特定的类加载器在接到加载类的请求时, 首先将加载任务委托给父类加载器, 依次递归, 如果父类加载器可以完成类加载任务, 就成功返回；只有父类加载器无法完成此加载任务时, 才自己去加载。\n\n使用双亲委派模型的好处在于 `Java 类随着它的类加载器一起具备了一种带有优先级的层次关系`。例如类 java.lang.Object, 它存在在 rt.jar 中, 无论哪一个类加载器要加载这个类, 最终都是委派给处于模型最顶端的 Bootstrap ClassLoader 进行加载, 因此 Object 类在程序的各种类加载器环境中都是同一个类。相反, 如果没有双亲委派模型而是由各个类加载器自行加载的话, 如果用户编写了一个 java.lang.Object 的同名类并放在 ClassPath 中, 那系统中将会出现多个不同的 Object 类, 程序将混乱。因此, 如果开发者尝试编写一个与 rt.jar 类库中重名的 Java 类, 可以正常编译, 但是永远无法被加载运行。\n","tags":["类加载机制"]},{"title":"JDK 命令行工具及可视化工具","url":"%2F2017%2F08%2F28%2Fjdk-command%2F","content":"\n## JDK 命令行的工具\n\n### JPS ：虚拟机进程状况工具\n\n-q 只输出 LVMID, 省略主类的名称。\n-m 输出虚拟机进程启动时传递给主类 main() 函数的参数。\n-l 输出主类的全名, 如果进程执行的是 Jar 包, 输出 Jar 路径。\n-v 输出虚拟机进程启动 JVM 参数。\n\n### jstat：虚拟机统计信息监视工具\n可以显示本地或远程虚拟机进程中的类装载、内存、垃圾收集、JIT 编译等运行数据。\n命令格式 jstat [option vmid [interval[s|ms] [count]]]\n如果是远程虚拟机进程 那么 VMID 格式：[protocol:][//]lvmid[@hostname[:port]/servername]\n参数 interval 和 count 代表查询间隔和次数, 如果省略了这两个参数, 说明只查询一次。\n\n<!-- more -->\n\n### jinfo：Java 配置信息工具\njinfo 是作用是实时查看和调整虚拟机的各项参数。使用 jps -v 参数可以查看虚拟机启动时显示执行的参数列表。\njinfo -flag PretenureSizeThreshold 7832\n-XX:PretenureSizeThreshold=0\n\n### jmap：Java 内存映像工具\njmap（Memory Map for Java）命令用于生成堆转储快照（一般称为 heapdump 或 dump 文件）。\n-dump 生成 Java 堆转储快照。\n-finalizerinfo 显示在 F-Queue 中等待 Finalizer 线程执行 finalize 方法的对象。\n-heap 显示 Java 堆详细信息。\n-histo 显示堆中对象统计信息。\n-permstat 以 ClassLoader 为统计口径显示永久代内存状态。\n-F 当虚拟机进程对 - dump 选项没有响应时, 可使用这个选项强制生成 dump 快照。\n\n### jhat：虚拟机堆转储快照分析工具\n与 jmap 搭配使用, 来分析 jmap 生成的堆 转储快照。\n\n### jstack：Java 堆栈跟踪工具\njstack（Stack Trace for Java）命令用于生成虚拟机当前时刻的线程快照（一般称为 threaddump 或 javacore 文件）。\n-f 当正常输出的请求不被响应时, 强制输出线程堆栈。\n-l 除堆栈外, 显示锁的附件信息。\n-m 如果调用到本地方法的话, 可以显示 C/C++ 的堆栈。\n\n## JDK 的可视化工具\n\n### JConsole\nJConsole 工具在 JDK/bin 目录下, 启动 JConsole 后, 将自动搜索本机运行的 jvm 进程, 不需要 jps 命令来查询指定。双击其中一个 jvm 进程即可开始监控, 也可使用 “远程进程” 来连接远程服务器。\n\n### VisualVM\nVisualVM 是一个集成多个 JDK 命令行工具的可视化工具。VisualVM 基于 NetBeans 平台开发, 它具备了插件扩展功能的特性, 通过插件的扩展, 可用于显示虚拟机进程及进程的配置和环境信息 (jps, jinfo), 监视应用程序的 CPU、GC、堆、方法区及线程的信息 (jstat、jstack) 等。VisualVM 在 JDK/bin 目录下。\n","tags":["Java虚拟机"]},{"title":"垃圾收集器与内存分配策略","url":"%2F2017%2F08%2F28%2Fgc%2F","content":"\n## 垃圾收集器\n\n### 可达性分析算法\n\n通过一系列的称谓 `GC Roots` 的对象作为起始点, 从这些节点开始向下搜索, 搜索所有走过的路径为引用链, 当一个对象到 GC Roots 没有任何引用链项链时, 则证明此对象时不可用的。\n\nJava 语言中, 可作为 GC Roots 的对象包括下面几种：\n1. 虚拟机栈 (栈帧中的本地变量表) 中引用的对象\n2. 方法区中类静态属性引用的对象\n3. 方法区中常量引用的对象\n4. 本地方法栈中 JNI(即一般说的 Native 方法) 引用的对象\n\n### 引用类型\n\n从 JDK1.2 之后, Java 对引用的概念进行了扩充, 将引用分为强引用, 软引用, 弱引用, 虚引用, 这四种引用的强度一次逐渐减弱。\n1. 强引用就是指在程序代码之中普遍存在的, 类似 “Object obj = new Object()” 这类的引用, 只要强引用还存在, 垃圾回收器永远不会回收掉被引用的对象。\n2. 软引用是用来描述一些还有用但并非需要的对象, 对于软引用关联着的对象, 在系统将要发生内存异常之前, 将会把这些对象列进回收范围之中进行第二次回收, 如果这次回收还没有足够的内存, 才会抛出内存异常。\n3. 弱引用也是用来描述非必需对象的, 但是它的强度比软引用更弱一些, 被弱引用关联的对象只能生存岛下一次垃圾收集发生之前, 当垃圾收集器工作时, 无论当前内存释放足够, 都会回收掉只被弱引用关联的对象。\n4. 虚引用也称为幽灵引用或者幻影引用, 它是最弱的一种引用关系, 一个对象是否有虚引用的存在, 完全不会对其生存时间构成影响, 也无法通过虚引用来取得一个对象实例, 对一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知。\n\n<!-- more -->\n\n### 垃圾收集算法\n\n#### 标记—清除算法\n\n标记—清除算法是最基础的收集算法, 它分为 “标记” 和 “清除” 两个阶段：首先标记出所需回收的对象, 在标记完成后统一回收掉所有被标记的对象, 它的标记过程其实就是前面的根搜索算法中判定垃圾对象的标记过程。标记—清除算法的执行情况如下图所示：\n\n__回收前状态：__\n\n![回收前状态](/images/gc/return.png)\n\n__回收后状态：__\n\n![回收后状态](/images/gc/return1.png)\n\n之后的算法都是基于这种思路改进的, 他的主要不足有两个：\n1. 一个是效率问题, 标记和清楚的两个过程的效率都不高。\n2. 另一个是空间问题, 标记清除之后会产生大量内存碎片, 导致之后程序分配大对象内存时, 无法找到足够的内存而不得不提前触发另一次垃圾收集动作。\n\n#### 复制算法\n\n将可用内存按容量大小划分为大小相等的两块, 每次只使用其中的一块。当一块内存使用完了, 就将还存活着的对象复制到另一块上面, 然后再把已使用过的内存空间一次清理掉。这样使得每次都是对整个半区进行内存回收, 内存分配时也就不用考虑内存碎片等复杂情况。\n\n![复制算法](/images/gc/copy.png)\n\n目的：为了解决效率问题。\n缺点：将内存缩小为了原来的一半。\n\n现代的商业虚拟机都采用这种收集算法来回收新生代, IBM 公司的专门研究表明, 新生代中对象 98% 对象是 “朝生夕死” 的, 所以不需要按照 1：1 的比例来划分内存空间, 而是将内存分为较大的 Eden 空间和两块较小的 Survivor 空间, 每次使用 Eden 和其中一块 Survivor。HotSpot 虚拟机中默认 Eden 和 Survivor 的大小比例是 8：1。也就是说只有10%的内存是“浪费的”。\n\n#### 标记 - 整理算法\n\n复制收集算法在对象存活率较高时, 就要进行较多的复制操作, 效率就会变低。\n根据老年代的特点, 提出了” 标记 - 整理 “算法。\n标记过程仍然与” 标记 - 清除 “算法一样, 但后续步骤不是直接对可回收对象进行清理, 而是让所有存活的对象都向一端移动, 然后直接清理掉边界以外的内存。\n\n![标记 - 整理算法](/images/gc/mark-copy.png)\n\n#### 分代收集算法\n\n一般是把 Java 堆分为新生代和老年代, 这样就可以根据各个年代的特点采用最适当的收集算法。\n\n在新生代中, 每次垃圾收集时都发现有大批对象死去, 只有少量存活, 那就选用复制算法。\n\n在老年代中, 因为对象存活率高、没有额外空间对它进行分配担保, 就必须采用 “标记 - 清除” 或 “标记 - 整理” 算法来进行回收。\n\n### 垃圾回收机制的一些知识\n\n#### JVM 中的年代\n\nJVM 中分为年轻代（Young generation）和老年代 (Tenured generation)。\n\n一般情况下, 新创建的对象都会被分配到 Eden 区 (一些大对象特殊处理), 这些对象经过第一次 Minor GC 后, 如果仍然存活, 将会被移到 Survivor 区。对象在 Survivor 区中每熬过一次 Minor GC, 年龄就会增加 1 岁, 当它的年龄增加到一定程度时, 就会被移动到年老代中。\n\n使用复制算法GC之后, , Eden 区和 From 区已经被清空。这个时候, “From” 和 “To” 会交换他们的角色, 也就是新的 “To” 就是上次 GC 前的 “From”, 新的 “From” 就是上次 GC 前的 “To”。不管怎样, 都会保证名为 To 的 Survivor 区域是空的。Minor GC 会一直重复这样的过程, 直到 “To” 区被填满, “To” 区被填满之后, 会将所有对象移动到年老代中。\n\n#### Minor GC 和 Full GC 的区别\n\nMinor GC: 指发生在新生代的垃圾收集动作, 该动作非常频繁。\n\nFull GC/Major GC: 指发生在老年代的垃圾收集动作, 出现了 Major GC, 经常会伴随至少一次的 Minor GC。Major GC 的速度一般会比 Minor GC 慢 10 倍以上。\n\n#### 空间分配担保\n\n在发生 Minor GC 之前, 虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象的总空间, 如果这个条件成立, 那么 Minor GC 可以 确保是安全的。如果不成立, 则虚拟机会查看 HandlePromotionFailure 设置值是否允许担保失败。如果允许, 那会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小, 如果大于, 则将尝试进行一次 Minor GC, 尽管这个 Minor GC 是有风险的。如果小于, 或者 HandlePromotionFailure 设置不允许冒险, 那这时也要改为进行一次 Full GC。\n\n### 垃圾收集器\n\n__7种收集器：__\n\n![收集器](/images/gc/collector.png)\n\n#### Serial 收集器\n\n是最基本、发展历史最悠久的收集器。这是一个单线程收集器。但它的 “单线程” 的意义并不仅仅说明它只会使用一个 CPU 或一条收集线程去完成垃圾收集工作, 更重要的是它在进行垃圾收集时, 必须暂停其他所有的工作线程, 直到它收集结束。\n\n![Serial 收集器](/images/gc/serial.png)\n\n是虚拟机运行在 Client 模式下的默认新生代收集器。\n\n优势：简单而高效（与其他收集器的单线程比）, 对于限定单个 CPU 的环境来说, Serial 收集器由于没有线程交互的开销, 专心做垃圾收集自然可以获得最高的单线程效率。\n\n#### ParNew 收集器\n\nParNew 收集器其实就是 Serial 收集器的多线程版本。\n\n是许多运行在 Server 模式下的虚拟机中首选的新生代收集器, 其中一个与性能无关但很重要的原因是, 除了 Serial 收集器外, 目前只有它能与 CMS 收集器配合工作。\n\nParNew 收集器默认开启的收集线程数与 CPU 的数量相同。\n\n#### Parallel Scavenge 收集器\n\nParallel Scavenge 收集器是一个新生代收集器, 使用复制算法, 又是并行的多线程收集器。\n最大的特点是：Parallel Scavenge 收集器的目标是达到一个可控制的吞吐量。\n\n所谓吞吐量就是 CPU 用于运行用户代码的时间与 CPU 总消耗时间的比值, 即吞吐量 = 运行用户代码时间 /（运行用户代码时间 + 垃圾收集时间）。\n\n高吞吐量则可以高效率地利用 CPU 时间, 尽快完成程序的运算任务, 主要适合在后台运算而不需要太多交互的任务。\n\n#### Serial Old 收集器\n\nSerial Old 是 Serial 收集器的老年代版本, 同样是一个单线程收集器, 使用 “标记 - 整理” 算法。这个收集器的主要意义也是在于给 Client 模式下虚拟机使用。\n\n如果在 Server 模式下, 它主要还有两大用途：\n1. 与 Parallel Scavenge 收集器搭配使用\n\n2. 作为 CMS 收集器的后备预案, 在并发收集发生 Conurrent Mode Failure 使用。\n\n#### Parallel Old 收集器\n\nParallel Old 是 Parallel Scavenge 收集器的老年代版本, 使用多线程和 “标记 - 整理” 算法。\n\n在注重吞吐量以及 CPU 资源敏感的场合, 都可以优先考虑 Parallel Scavenge+Parallel Old 收集器\n\n#### CMS（Concurrent Mark Sweep）收集器\n\n是 HotSpot 虚拟机中第一款真正意义上的并发收集器, 它第一次实现了让垃圾收集线程与用户线程同时工作。\n\n关注点：尽可能地缩短垃圾收集时用户线程的停顿时间。\n\nCMS 收集器是基于 “标记 - 清除” 算法实现的, 整个过程分为 4 个步骤：\n1. 初始标记\n2. 并发标记\n3. 重新标记\n4. 并发清除\n\n其中, 初始标记, 重新标记这两个步骤仍然需要 “Stop The World”。初始标记仅仅只标记一下 GC Roots 能直接关联到的对象, 速度很快。并发标记阶段就是 进行 GC Roots Tracing 的过程。\n\n重新标记阶段则是为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记几率, 这个阶段的停顿时间一般会比初始标记阶段稍长, 但远比并发标记时间短。\n\n整个过程耗时最长的阶段是并发标记, 并发清除过程, 但这两个过程可以和用户线程一起工作。\n\n![CMS流程](/images/gc/CMS.jpeg)\n\n缺点：\n1. CMS 收集器对 CPU 资源非常敏感。在并发阶段, 它虽然不会导致用户线程停顿, 但是会因为占用了一部分线程（或者说 CPU 资源）而导致应用程序变慢, 总吞吐量会降低。\n\n2. CMS 收集器无法处理浮动垃圾, 可能出现 “Conurrent Mode Failure” 失败而导致另一次 Full GC 的产生。由于 CMS 并发清理阶段用户线程还在运行着, 伴随程序运行自然就还会产生新的垃圾, 这一部分垃圾出现在标记过程之后, CMS 无法在档次收集中处理掉它们, 只好留待下一次 GC 时再清理掉。这部分垃圾就称为 “浮动垃圾”。因此 CMS 收集器不能像其他收集器那样等到老年代几乎完全被填满了再进行收集, 需要预留一部分空间提供并发收集时程序运作使用。在 JDK1.5 的默认设置下, CMS 收集器当老年代使用了 68% 的空间后就会被激活。如果预留空间无法满足程序需要, 就会出现一次 “Concurrent Mode Failure” 失败, 这时虚拟机将启动后备预案 Serial Old。\n\n3. CMS 是一款基于 “标记 - 清除” 算法实现的收集器, 所以会有大量空间碎片问题。\n\n#### G1 收集器\n\n是当今收集器技术发展的最前沿成果之一。是一款面向服务端应用的垃圾收集器。\n特点：\n1. 并行与并发\n能充分利用多 CPU, 多核环境下的硬件优势, 缩短 Stop-The-World 停顿的时间, 同时可以通过并发的方式让 Java 程序继续执行。\n\n2. 分代收集\n可以不需要其他收集器的配合管理整个堆, 但是仍采用不同的方式去处理分代的对象。\n\n3. 空间整合\nG1 从整体上来看, 采用基于 “标记 - 整理” 算法实现收集器\nG1 从局部上来看, 采用基于 “复制” 算法实现。\n\n4. 可预测停顿\n使用 G1 收集器时, Java 堆内存布局与其他收集器有很大差别, 它将整个 Java 堆划分成为多个大小相等的独立区域。\n\nG1 跟踪各个 Region 里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值）, 在后台维护一个优先列表, 每次根据允许的收集时间, 优先回收价值最大的 Region。\n\n## 内存分配策略\n\n### 对象优先在 Eden 上分配\n\n大多数情况下, 对象优先在新生代 Eden 区域中分配。当 Eden 内存区域没有足够的空间进行分配时, 虚拟机将触发一次 Minor GC(新生代 GC)。Minor GC 期间虚拟机将 Eden 区域的对象移动到其中一块 Survivor 区域。虚拟机提供了 - XX：+PrintGCDetails 这个收集器日志参数, 告诉虚拟机在发生垃圾收集行为时打印内存回收日志, 并且在进程退出的时候输出当前的内存各区域分配情况。\n\n### 大对象直接进入老年代\n\n所谓大对象是指需要大量连续空间的对象。虚拟机提供了一个 XX:PretenureSizeThreshold 参数, 大于这个值的对象直接在老年代中分配。\n\n### 长期存活的对象将进入老年代\n\n虚拟机采用分代收集的思想管理内存, 那内存回收时就必须能识别那些对象该放到新生代, 那些该到老年代中。为了做到这点, 虚拟机为每个对象定义了一个对象年龄 Age, 每经过一次新生代 GC 后任然存活, 将对象的年龄 Age 增加 1 岁, 当年龄到一定程度（默认为 15）时, 将会被晋升到老年代中, 对象晋升老年代的年龄限定值, 可通过 - XX:MaxTenuringThreshold 来设置。为了能更好地适应不同程序的内存状况, 虚拟机并不是永远地要求对象的年龄必须达到了 MaxTenuringThreshold 才能晋升老年代, 如果在 Survivor 空间中相同年龄所有对象大小的总和大于 Survivor 空间的一半, 年龄大于或等于该年龄的对象就可以直接进入老年代, 无须等到 MaxTenuringThreshold 中要求的年龄。\n\n### 空间分配担保\n\n在发生 Minor GC 之前, 虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象总空间, 如果这个条件成立, 那么 Minor GC 可以确保是安全的。如果不成立, 则虚拟机会查看 HandlePromotionFailure 设置值是否允许担保失败。如果允许, 那么会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小, 如果大于, 将尝试着进行一次 Minor GC, 尽管这次 Minor GC 是有风险的；如果小于或者 HandlePromotionFailure 设置不允许冒险, 那这时也要改为进行一次 Full GC。\n\n“冒险” 是冒了什么风险, 新生代使用复制收集算法, 但为了内存利用率, 只使用其中一个 Survivor 空间来作为轮换备份, 因此当出现大量对象在 Minor GC 后仍然存活的情况（最极端的情况就是内存回收后新生代中所有对象都存活）, 就需要老年代进行分配担保, 把 Survivor 无法容纳的对象直接进入老年代。与生活中的贷款担保类似, 老年代要进行这样的担保, 前提是老年代本身还有容纳这些对象的剩余空间, 一共有多少对象会活下来在实际完成内存回收之前是无法明确知道的, 所以只好取之前每一次回收晋升到老年代对象容量的平均大小值作为经验值, 与老年代的剩余空间进行比较, 决定是否进行 Full GC 来让老年代腾出更多空间。取平均值进行比较其实仍然是一种动态概率的手段, 也就是说, 如果某次 Minor GC 存活后的对象突增, 远远高于平均值的话, 依然会导致担保失败（Handle Promotion Failure）。如果出现了 HandlePromotionFailure 失败, 那就只好在失败后重新发起一次 Full GC。虽然担保失败时绕的圈子是最大的, 但大部分情况下都还是会将 HandlePromotionFailure 开关打开, 避免 Full GC 过于频繁, 在 JDK 6 Update 24 之后, HandlePromotionFailure 参数不会再影响到虚拟机的空间分配担保策略, 观察 OpenJDK 中的源码变化, 虽然源码中还定义了 HandlePromotionFailure 参数, 但是在代码中已经不会再使用它。JDK 6 Update 24 之后的规则变为只要老年代的连续空间大于新生代对象总大小或者历次晋升的平均大小就会进行 Minor GC, 否则将进行 Full GC。\n","tags":["Java虚拟机"]},{"title":"Java内存区域","url":"%2F2017%2F08%2F28%2Fjvm%2F","content":"\n## 运行时数据区域\n\n![运行时数据区域](/images/jvm/jvmdata.png)\n\n### 程序计数器\n\n字节码解释器通过更改这个计数器的值来选取下一条需要执行的字节码指令。每条线程都需要一个独立的程序计数器, 线程之间, 互不影响。**Java虚拟机规范中唯一一个没有规定任何OutOfMemoryError情况的区域**。\n\n<!-- more -->\n\n### Java虚拟机栈\n\n线程私有, 生命周期与线程相同。局部变量表存放编译器可知的基本数据类型, 对象引用, 和returnAddress类型。\n\n局部变量表所需要的内存空间在编译期间完成分配, 当进入一个方法时, 在方法运行期间不会改变局部变量表的大小。\n\n这个区域有两种异常：\n\n1. 当线程请求的栈深度大于虚拟机所允许的深度, 将抛出Stackoverflow异常。\n2. 当虚拟机可以动态扩展, 但无法申请到足够的内存, 将抛出OutOfMemoryError异常。\n\n#### 局部变量表\n\n局部变量表是一组变量值存储空间, 用于存放方法参数和方法内部定义的局部变量, 其中存放的数据的类型是编译期可知的各种基本数据类型、对象引用（reference）和 returnAddress 类型（它指向了一条字节码指令的地址）。局部变量表所需的内存空间在编译期间完成分配, 即在 Java 程序被编译成 Class 文件时, 就确定了所需分配的最大局部变量表的容量。当进入一个方法时, 这个方法需要在栈中分配多大的局部变量空间是完全确定的, 在方法运行期间不会改变局部变量表的大小。\n\n局部变量表的容量以变量槽（Slot）为最小单位。在虚拟机规范中并没有明确指明一个 Slot 应占用的内存空间大小（允许其随着处理器、操作系统或虚拟机的不同而发生变化）, 一个 Slot 可以存放一个32位以内的数据类型：boolean、byte、char、short、int、float、reference 和 returnAddresss。reference 是对象的引用类型, returnAddress 是为字节指令服务的, 它执行了一条字节码指令的地址。对于 64 位的数据类型（long和double）, 虚拟机会以高位在前的方式为其分配两个连续的 Slot 空间。\n\n虚拟机通过索引定位的方式使用局部变量表, 索引值的范围是从 0 开始到局部变量表最大的 Slot 数量, 对于 32 位数据类型的变量, 索引 n 代表第 n 个 Slot, 对于 64 位的, 索引 n 代表第 n 和第 n+1 两个 Slot。\n\n在方法执行时, 虚拟机是使用局部变量表来完成参数值到参数变量列表的传递过程的, 如果是实例方法（非static）, 则局部变量表中的第 0 位索引的 Slot 默认是用于传递方法所属对象实例的引用, 在方法中可以通过关键字“this”来访问这个隐含的参数。其余参数则按照参数表的顺序来排列, 占用从1开始的局部变量 Slot, 参数表分配完毕后, 再根据方法体内部定义的变量顺序和作用域分配其余的 Slot。\n\n局部变量表中的 Slot 是可重用的, 方法体中定义的变量, 作用域并不一定会覆盖整个方法体, 如果当前字节码PC计数器的值已经超过了某个变量的作用域, 那么这个变量对应的 Slot 就可以交给其他变量使用。这样的设计不仅仅是为了节省空间, 在某些情况下 Slot 的复用会直接影响到系统的而垃圾收集行为。\n\n#### 操作数栈\n\n操作数栈又常被称为操作栈, 操作数栈的最大深度也是在编译的时候就确定了。32 位数据类型所占的栈容量为 1,64 位数据类型所占的栈容量为 2。当一个方法开始执行时, 它的操作栈是空的, 在方法的执行过程中, 会有各种字节码指令（比如：加操作、赋值元算等）向操作栈中写入和提取内容, 也就是入栈和出栈操作。\n\nJava 虚拟机的解释执行引擎称为“基于栈的执行引擎”, 其中所指的“栈”就是操作数栈。因此我们也称 Java 虚拟机是基于栈的, 这点不同于 Android 虚拟机, Android 虚拟机是基于寄存器的。\n\n基于栈的指令集最主要的优点是可移植性强, 主要的缺点是执行速度相对会慢些；而由于寄存器由硬件直接提供, 所以基于寄存器指令集最主要的优点是执行速度快, 主要的缺点是可移植性差。\n\n#### 动态连接\n\n每个栈帧都包含一个指向运行时常量池（在方法区中, 后面介绍）中该栈帧所属方法的引用, 持有这个引用是为了支持方法调用过程中的动态连接。Class 文件的常量池中存在有大量的符号引用, 字节码中的方法调用指令就以常量池中指向方法的符号引用为参数。这些符号引用, 一部分会在类加载阶段或第一次使用的时候转化为直接引用（如 final、static 域等）, 称为静态解析, 另一部分将在每一次的运行期间转化为直接引用, 这部分称为动态连接。\n\n#### 方法返回地址\n\n当一个方法被执行后, 有两种方式退出该方法：执行引擎遇到了任意一个方法返回的字节码指令或遇到了异常, 并且该异常没有在方法体内得到处理。无论采用何种退出方式, 在方法退出之后, 都需要返回到方法被调用的位置, 程序才能继续执行。方法返回时可能需要在栈帧中保存一些信息, 用来帮助恢复它的上层方法的执行状态。一般来说, 方法正常退出时, 调用者的 PC 计数器的值就可以作为返回地址, 栈帧中很可能保存了这个计数器值, 而方法异常退出时, 返回地址是要通过异常处理器来确定的, 栈帧中一般不会保存这部分信息。\n\n方法退出的过程实际上等同于把当前栈帧出站, 因此退出时可能执行的操作有：恢复上层方法的局部变量表和操作数栈, 如果有返回值, 则把它压入调用者栈帧的操作数栈中, 调整 PC 计数器的值以指向方法调用指令后面的一条指令。\n\n### 本地方法栈\n\n和Java虚拟机栈类似, 不过Java虚拟机栈为字节码服务, 而本地方法栈为虚拟机使用的Native方法服务。\n这个区域也有可能抛出Stackoverflow异常和OutOfMemoryError异常。\n\n### Java堆\n\nJava堆是Java虚拟机管理内存中最大的一块, 被所有线程共享, 在虚拟机启动时创建, 用于存储对象实例。\n\nJava堆是垃圾回收的主要区域。从内存回收角度, Java堆可以细分为：新生代和老年代；新生代再细致一点分为：Eden空间, From Survivor空间, To Survivor空间。从内存分配角度, Java堆可能划分出多个线程私有的分配缓冲区（TLAB）。\n\n可以通过 jvm 选项设定堆容量：\n\n1. -Xms20M\n\n表示设置堆容量的最小值为 20M, 必须以 M 为单位\n\n2. -Xmx20M\n\n表示设置堆容量的最大值为 20M, 必须以 M 为单位\n\n### 方法区\n\n线程共享, 用于存储已被虚拟机加载的类信息, 常量, 静态变量, 编译后的代码等。有可能抛出OutOfMemoryError异常。\n\n#### 运行时常量池\n\n方法区的一部分, 用于存放编译器生成的Class文件中各种字面量和符号引用, 运行期间也可能将新的常量放入池中。\n\n### 直接内存\n\n直接内存并不是虚拟机运行时区域的一部分, 也不是Java虚拟机规范定义的内存区域。JDK 1.4中新加入的NIO类, 引入了一种基于通道和缓冲区的I/O方式, 它可以使用native函数库直接分配堆外内存, 然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作。这样做是为了能在一些场景中显著提高性能, 因为避免了Java堆和native堆来回复制数据。  \n\n本机直接内存的分配不受到Java堆的大小限制, 但会受到物理内存和操作系统的限制。\n\n## 对象\n\n### 对象的创建\n\n对内存分配情况分析最常见的示例便是对象实例化:\n\n```\nObject obj = new Object();\n```\nobj 会作为引用类型（reference）的数据保存在 Java 栈的本地变量表中, 而在 Java 堆中保存该引用的实例化对象, Java 堆中还必须包含能查找到此对象类型数据的地址信息（如对象类型、父类、实现的接口、方法等）, 这些类型数据则保存在方法区中。\n\n虚拟机遇到一条new指令时, 首先将去检查这个指令的参数是否能在常氮池中定位到一 个类的符号引用, 并且检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有, 那必须先执行相应的类加载过程。\n\n在类加载检查通过后, 接下来虚拟机将为新生对象分配内存。对象所需内存的大小在类加载完成后便可完全确定, 为对象分配空间的任务等同于把一块确定大小的内存从Java堆中划分出来。\n\n如果Java堆中内存是绝对规整的, 那么分配内存就只需要把指针移动一个对象大小的距离, 这种分配方式叫`指针碰撞`；如果已使用的内存和空闲内存相互交错, 虚拟机就需要维护一个空闲内存的列表, 在分配的时候从列表中找到一个足够大的内存分配给对象, 并更新列表, 这种分配方式叫`空闲列表`。\n","tags":["Java虚拟机"]},{"title":"ES6 入门学习","url":"%2F2017%2F08%2F22%2FES6-note%2F","content":"\n## 数组的扩展\n\n### Array.from()\n\nArray.from 方法用于将两类对象转为真正的数组：\n\n- 类似数组的对象（ array-like object） \n- 可遍历（ iterable） 的对象（ 包括ES6新增的数据结构Set和Map）\n\nArray.from 还可以接受第二个参数, 作用类似于数组的 map 方法, 用来对每个\n元素进行处理, 将处理后的值放入返回的数组。\n\n```javascript\nArray.from(arrayLike, x => x * x);\n```\n\n<!-- more -->\n\n### Array.of()\n\nArray.of 方法用于将一组值, 转换为数组\n\n### 数组实例的copyWithin()\n\n数组实例的 copyWithin 方法, 在当前数组内部, 将指定位置的成员复制到其他\n位置（ 会覆盖原有成员） , 然后返回当前数组。也就是说, 使用这个方法, 会修改\n当前数组, 它接受三个参数:\n\n- target（必需） ：从该位置开始替换数据\n- start（可选） ：从该位置开始读取数据, 默认为0。如果为负值, 表示倒数\n- end（可选） ：到该位置前停止读取数据, 默认等于数组长度。如果为负值, \n表示倒数\n\n这三个参数都应该是数值, 如果不是, 会自动转为数值\n\n```javascript\n[1, 2, 3, 4, 5].copyWithin(0, 3)\n// [4, 5, 3, 4, 5]\n```\n\n上面代码表示将从3号位直到数组结束的成员（ 4和5） , 复制到从0号位开始的位\n置, 结果覆盖了原来的1和2\n\n### 数组实例的find()和findIndex()\n\n数组实例的 find 方法, 用于找出第一个符合条件的数组成员。它的参数是一个回\n调函数, 所有数组成员依次执行该回调函数, 直到找出第一个返回值为 true 的成\n员, 然后返回该成员。如果没有符合条件的成员, 则返回 undefined 。\n\n```javascript\n[1, 5, 10, 15].find(function(value, index, arr) {\n  return value > 9;\n}) // 10\n\n[1, 5, 10, 15].find((n) => n > 9)//10\n```\n\nfind 方法的回调函数可以接受三个参数, 依次为当前的值、当前的\n位置和原数组\n\n数组实例的 findIndex 方法的用法与 find 方法非常类似, 返回第一个符合条件\n的数组成员的位置, 如果所有成员都不符合条件, 则返回 -1\n\n### 数组实例的fill()\n\nfill 方法使用给定值, 填充一个数组。fill 方法还可以接受第二个和第三个参数, 用于指定填充的起始位置和结束位置。\n\n```javascript\n['a', 'b', 'c'].fill(7, 1, 2)\n// ['a', 7, 'c']\n```\n\n### 数组实例的entries(), keys()和values()\n\nES6提供三个新的方法—— entries(),  keys() 和 values() ——用于遍历数\n组。它们都返回一个遍历器对象, 可以用 for...of 循环进行遍历, 唯一的区别是 keys() 是对键名的遍历、 values() 是对键值的遍历,  entries() 是对键值对的遍历。\n\n```javascript\nfor (let [index, elem] of ['a', 'b'].entries()) {\n  console.log(index, elem);\n} \n// 0 \"a\"\n// 1 \"b\"\n```\n\n### 数组实例的includes()\n\nincludes 方法返回一个布尔值, 表示某个数组是否包含给定\n的值, 与字符串的 includes 方法类似。该方法属于ES7, 但Babel转码器已经支\n持。\n\n该方法的第二个参数表示搜索的起始位置, 默认为0。如果第二个参数为负数, 则\n表示倒数的位置, 如果这时它大于数组长度（ 比如第二个参数为-4, 但数组长度为\n3） , 则会重置为从0开始。\n\n```javascript\n[1, 2, 3].includes(3, 3); // false\n[1, 2, 3].includes(3, -1); // true\n```\n\n## 函数的扩展\n\n### 扩展运算符\n\n扩展运算符（ spread） 是三个点（... ）。它好比rest参数的逆运算, 将一个数组转为用逗号分隔的参数序列。\n\n```javascript\nconsole.log(...[1, 2, 3])\n// 1 2 3\n```\n\n#### 扩展运算符的应用\n\n1.  合并数组\n\n```javascript\nvar arr1 = ['a', 'b'];\nvar arr2 = ['c'];\nvar arr3 = ['d', 'e'];\n// ES5的合并数组\narr1.concat(arr2, arr3);\n// [ 'a', 'b', 'c', 'd', 'e' ]\n// ES6的合并数组\n[...arr1, ...arr2, ...arr3]\n// [ 'a', 'b', 'c', 'd', 'e' ]\n```\n\n2.  与解构赋值结合\n\n```javascript\n// ES5\na = list[0], rest = list.slice(1)\n// ES6\n[a, ...rest] = list\n\n\nconst [first, ...rest] = [1, 2, 3, 4, 5];\nfirst // 1\nrest // [2, 3, 4, 5]\n```\n\n如果将扩展运算符用于数组赋值, 只能放在参数的最后一位, 否则会报错。\n\n3.  字符串\n\n扩展运算符还可以将字符串转为真正的数组。\n\n```javascript\n[...'hello']\n// [ \"h\", \"e\", \"l\", \"l\", \"o\" ]\n```\n\n4.  Map和Set结构, Generator函数\n\n扩展运算符内部调用的是数据结构的Iterator接口, 因此只要具有Iterator接口的对\n象, 都可以使用扩展运算符, 比如Map结构。\n\n```javascript\nlet map = new Map([\n  [1, 'one'],\n  [2, 'two'],\n  [3, 'three'],\n]);\n\nlet arr = [...map.keys()]; // [1, 2, 3]\n```\n\n### 函数绑定\n\n函数绑定运算符是并排的两个双冒号（::） , 双冒号左边是一个对象, 右边是一个\n函数。该运算符会自动将左边的对象, 作为上下文环境（ 即this对象） , 绑定到右\n边的函数上面。\n\n```javascript\nfoo::bar;\n// 等同于\nbar.bind(foo);\n\nfoo::bar(...arguments);\n// 等同于\nbar.apply(foo, arguments);\n```\n\n如果双冒号左边为空, 右边是一个对象的方法, 则等于将该方法绑定在该对象上\n面。\n\n```javascript\nvar method = obj::obj.foo;\n// 等同于\nvar method = ::obj.foo;\n\nlet log = ::console.log;\n// 等同于\nvar log = console.log.bind(console);\n```\n\n由于双冒号运算符返回的还是原对象, 因此可以采用链式写法。\n\n```javascript\nlet { find, html } = jake;\ndocument.querySelectorAll(\"div.myClass\")\n::find(\"p\")\n::html(\"hahaha\");\n```\n\n## 对象的扩展\n\n### 属性的简洁表示法\n\nES6允许直接写入变量和函数, 作为对象的属性和方法。这样的书写更加简洁。\n\n```javascript\nvar foo = 'bar';\nvar baz = {foo};\nbaz // {foo: \"bar\"}\n// 等同于\nvar baz = {foo: foo};\n\nfunction f(x, y) {\n  return {x, y};\n} \n// 等同于\nfunction f(x, y) {\nreturn {x: x, y: y};\n} f\n(1, 2) // Object {x: 1, y: 2}\n\nvar o = {\n  method() {\n    return \"Hello!\";\n  }\n};\n// 等同于\nvar o = {\n  method: function() {\n    return \"Hello!\";\n  }\n};\n```\n\n如果某个方法的值是一个Generator函数, 前面需要加上星号。\n\n```javascript\nvar obj = {\n  * m(){\n    yield 'hello world';\n  }\n};\n```\n\n### Object.is()\n\nES5比较两个值是否相等, 只有两个运算符：相等运算符（== ） 和严格相等运算\n符（=== ） 。它们都有缺点, 前者会自动转换数据类型, 后者的 NaN 不等于自\n身, 以及 +0 等于 -0 。JavaScript缺乏一种运算, 在所有环境中, 只要两个值是\n一样的, 它们就应该相等。\n\nObject.is 用来比较两个值是否严格相等, 与严格比较运算符（ ===） 的行为基本一致。不同之处只有两个：一是 +0 不等于 -0 , 二是 NaN 等于自身。\n\n```javascript\n+0 === -0 //true\nNaN === NaN // false\n\nObject.is(+0, -0) // false\nObject.is(NaN, NaN) // true\n```\n\n### Object.assign()\n\nObject.assign 方法用于对象的合并, 将源对象（ source） 的所有可枚举属性, \n复制到目标对象（ target） 。Object.assign 方法的第一个参数是目标对象, 后面的参数都是源对象。\n\n```javascript\nvar target = { a: 1 };\n\nvar source1 = { b: 2 };\nvar source2 = { c: 3 };\n\nObject.assign(target, source1, source2);\ntarget // {a:1, b:2, c:3}\n```\n\nObject.assign 方法实行的是浅拷贝, 而不是深拷贝。也就是说, 如果源对象某\n个属性的值是对象, 那么目标对象拷贝得到的是这个对象的引用。\n\n## Generator 函数\n\n形式上, Generator函数是一个普通函数, 但是有两个特征。一是,  function 关\n键字与函数名之间有一个星号；二是, 函数体内部使用 yield 语句, 定义不同的\n内部状态（ yield语句在英语里的意思就是“产出”）。\n\n### yield语句\n\n由于Generator函数返回的遍历器对象, 只有调用 next 方法才会遍历下一个内部\n状态, 所以其实提供了一种可以暂停执行的函数。 yield 语句就是暂停标志。\n遍历器对象的 next 方法的运行逻辑如下。\n\n（1） 遇到 yield 语句, 就暂停执行后面的操作, 并将紧跟在 yield 后面的那个\n表达式的值, 作为返回的对象的 value 属性值。\n\n（2） 下一次调用 next 方法时, 再继续往下执行, 直到遇到下一个 yield 语\n句。\n\n（3） 如果没有再遇到新的 yield 语句, 就一直运行到函数结束, 直\n到 return 语句为止, 并将 return 语句后面的表达式的值, 作为返回的对象\n的 value 属性值。\n\n（4） 如果该函数没有 return 语句, 则返回的对象的 value 属性值\n为 undefined 。\n","tags":["JavaScript"]},{"title":"java-foreach","url":"%2F2017%2F07%2F25%2Fjava-foreach%2F","content":"\n## 查看反编译代码\n\n对以下代码进行反编译：\n\n```java\n\nfor (Integer i : list) {\n  System.out.println(i);\n}\n\n```\n\n反编译后：\n\n```java\n\nInteger i;\nfor(Iterator iterator = list.iterator(); iterator.hasNext(); System.out.println(i)){\n  i = (Integer)iterator.next();\n}\n\n```\n\n<!-- more -->\n\n我们按照执行顺序拆解一下：\n```\n\nInteger i; 定义一个临时变量 i\nIterator iterator = list.iterator(); 获取 List 的迭代器\niterator.hasNext(); 判断迭代器中是否有未遍历过的元素\ni = (Integer)iterator.next(); 获取第一个未遍历的元素, 赋值给临时变量 i\nSystem.out.println(i) 输出临时变量 i 的值\n\n```\n如此循环往复, 直到遍历完 List 中的所有元素。\n\n通过反编译, 我们看到, 其实 JAVA 中的增强 for 循环底层是通过迭代器模式来实现的。\n\n## 坑\n\n既然增强for循环通过迭代器实现, 那么必然有迭代器的特性, 在使用迭代器遍历元素的时候, 在对集合进行删除的时候一定要注意, 使用不当有可能发生`ConcurrentModificationException`, 如以下代码, 会抛出`ConcurrentModificationException`异常:\n\n```java\nfor (Student stu : students) {\n  if (stu.getId() == 2)\n    students.remove(stu);\n}\n```\n\n> Iterator 是工作在一个独立的线程中, 并且拥有一个 mutex 锁。 Iterator 被创建之后会建立一个指向原来对象的单链索引表, 当原来的对象数量发生变化时, 这个索引表的内容不会同步改变, 所以当索引指针往后移动的时候就找不到要迭代的对象, 所以按照 fail-fast 原则 Iterator 会马上抛出java.util.ConcurrentModificationException异常。\n\n所以 Iterator 在工作的时候是不允许被迭代的对象被改变的。但你可以使用 Iterator 本身的方法 remove() 来删除对象, Iterator.remove() 方法会在删除当前迭代对象的同时维护索引的一致性。\n\n正确的在遍历的同时删除元素的方式：\n\n```java\nIterator<Student> stuIter = students.iterator();\nwhile (stuIter.hasNext()) {\n  Student student = stuIter.next();\n  if (student.getId() == 2)\n    stuIter.remove();//这里要使用Iterator的remove方法移除当前对象, 如果使用List的remove方法, 则同样会出现ConcurrentModificationException\n}\n```\n","tags":["Java"]},{"title":"谷歌Chrome首页设置","url":"%2F2017%2F07%2F24%2Fgoogle-home-page%2F","content":"\n使用香港Google\n```\nhttps://www.google.com.hk/ncr\n```\n使用自动识别语言Google\n```\nhttps://www.google.com/ncr\n```\n","tags":["chrome"]},{"title":"Ubuntu 18.04 清华源","url":"%2F2017%2F07%2F22%2Fubuntu-apt-sources%2F","content":"\n## 先备份sources.list\n\n```shell\nsudo cp /etc/apt/sources.list /etc/apt/sources.list.bk\nsudo vim /etc/apt/sources.list \n```\n\n<!-- more -->\n\n## 再修改为如下:  \n\n清华源：\n\n```shell\n# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse\n\n# 预发布软件源，不建议启用\n# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse\n```\n","tags":["Ubuntu"]},{"title":"Java Enum","url":"%2F2017%2F07%2F22%2Fjava-enum%2F","content":"\nJava 中enum的书写方法\n\n```java\npublic enum EnumColumn {\n  key1(valve1), key2(valve2), key3(valve2);\n\n  private String type;\n\n  EnumColumn(String type) {\n    this.type = type;\n  }\n\n  @Override\n  public String toString() {\n    return String.valueOf(this.type);\n  }\n}\n  \npublic static void main(String[] args) {\n\n  EnumColumn enumColumn1 = EnumColumn.valueOf(key1);\n  EnumColumn enumColumn2 = EnumColumn.key2;\n    \n  System.out.println(groupColumn1.name());//key1\n  System.out.println(groupColumn1.toString());//valve1\n  System.out.println(groupColumn1.type);//valve1\n}\n```\n","tags":["Java"]},{"title":"利用序列化实现对象的拷贝","url":"%2F2017%2F07%2F22%2Fdeep-copy%2F","content":"\n> 如何利用序列化来完成对象的拷贝呢？在内存中通过字节流的拷贝是比较容易实现的。把母对象写入到一个字节流中, 再从字节流中将其读出来, 这样就可以创建一个新的对象了, 并且该新对象与母对象之间并不存在引用共享的问题, 真正实现对象的深拷贝。\n\n## 样例\n\n```java\npublic class CloneUtils {\n  @SuppressWarnings(\"unchecked\")\n  public static <T extends Serializable> T clone(T   obj){\n    T cloneObj = null;\n      try {\n        //写入字节流\n        ByteArrayOutputStream out = new ByteArrayOutputStream();\n        ObjectOutputStream obs = new ObjectOutputStream(out);\n        obs.writeObject(obj);\n        obs.close();\n        //分配内存, 写入原始对象, 生成新对象\n        ByteArrayInputStream ios = new  ByteArrayInputStream(out.toByteArray());\n        ObjectInputStream ois = new ObjectInputStream(ios);\n        //返回生成的新对象\n        cloneObj = (T) ois.readObject();\n        ois.close();\n      } catch (Exception e) {\n        e.printStackTrace();\n      }\n      return cloneObj;\n  }\n}\n```\n\n<!-- more -->\n\n使用该工具类的对象必须要实现 Serializable 接口, 否则是没有办法实现克隆的。\n\n```java\npublic class Person implements Serializable{\n  private static final long serialVersionUID = 2631590509760908280L;\n\n    ..................\n    //去除clone()方法\n\n    }\n\n    public class Email implements Serializable{\n        private static final long serialVersionUID = 1267293988171991494L;\n\n        ....................\n    }\n\n```\n\n所以使用该工具类的对象只要实现 Serializable 接口就可实现对象的克隆, 无须继承 Cloneable 接口实现 clone() 方法。\n\n```java\npublic class Client {\n    public static void main(String[] args) {\n        //写封邮件\n        Email email = new Email(\"请参加会议\",\"请与今天12:30到二会议室参加会议...\");\n\n        Person person1 =  new Person(\"张三\",email);\n\n        Person person2 =  CloneUtils.clone(person1);\n        person2.setName(\"李四\");\n        Person person3 =  CloneUtils.clone(person1);\n        person3.setName(\"王五\");\n        person1.getEmail().setContent(\"请与今天12:00到二会议室参加会议...\");\n\n        System.out.println(person1.getName() + \"的邮件内容是：\" + person1.getEmail().getContent());\n        System.out.println(person2.getName() + \"的邮件内容是：\" + person2.getEmail().getContent());\n        System.out.println(person3.getName() + \"的邮件内容是：\" + person3.getEmail().getContent());\n    }\n}\n-------------------\nOutput:\n张三的邮件内容是：请与今天12:00到二会议室参加会议...\n李四的邮件内容是：请与今天12:30到二会议室参加会议...\n王五的邮件内容是：请与今天12:30到二会议室参加会议...\n```\n\n在 JavaScript 中, 使用 `JSON.parse/JSON.stringify` 的方式实现:\n\n```js\nlet cloned = JSON.parse(JSON.stringify(objectToClone));\n```\n","tags":["JavaScript"]},{"title":"Hexo 常用命令","url":"%2F2017%2F07%2F22%2Fhexo-command%2F","content":"\n```sh\nhexo n \"title\"  => hexo new \"title\"\nhexo clean # 清除生成的public目录\nhexo g  => hexo generate  #生成\nhexo s  => hexo server  #启动服务预览\nhexo d  => hexo deploy  #部署, 部署之前需要先 generate\n```\n","tags":["hexo"]}]